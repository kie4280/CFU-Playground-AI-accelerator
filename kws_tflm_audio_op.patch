diff --git a/third_party/tflite-micro/tensorflow/lite/core/api/op_resolver.cc b/third_party/tflite-micro/tensorflow/lite/core/api/op_resolver.cc
index ce5ae4f4..3d770220 100644
--- a/third_party/tflite-micro/tensorflow/lite/core/api/op_resolver.cc
+++ b/third_party/tflite-micro/tensorflow/lite/core/api/op_resolver.cc
@@ -19,6 +19,7 @@ limitations under the License.
 #include "tensorflow/lite/core/api/error_reporter.h"
 #include "tensorflow/lite/core/c/common.h"
 #include "tensorflow/lite/schema/schema_utils.h"
+#include "tensorflow/lite/micro/micro_log.h" //
 
 namespace tflite {
 
@@ -60,6 +61,7 @@ TfLiteStatus GetRegistrationFromOpCode(
       // Do not report error for unresolved custom op, we do the final check
       // while preparing ops.
       status = kTfLiteError;
+      MicroPrintf("builtin_code: %s, name: %s, version: %d",EnumNameBuiltinOperator(builtin_code), name, version);
     }
   }
   return status;
diff --git a/third_party/tflite-micro/tensorflow/lite/kernels/internal/mfcc.h b/third_party/tflite-micro/tensorflow/lite/kernels/internal/mfcc.h
new file mode 100644
index 00000000..95b0bcfe
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/kernels/internal/mfcc.h
@@ -0,0 +1,81 @@
+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+// Basic class for computing MFCCs from spectrogram slices.
+
+#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_MFCC_H_
+#define TENSORFLOW_LITE_KERNELS_INTERNAL_MFCC_H_
+
+#include <cstdio> // for size_t declaration
+
+
+#include "tensorflow/lite/kernels/internal/mfcc_dct.h"
+#include "tensorflow/lite/kernels/internal/mfcc_mel_filterbank.h"
+
+namespace tflite {
+namespace internal {
+
+class Mfcc {
+ public:
+  Mfcc();
+  bool Initialize(int input_length, double input_sample_rate);
+
+  // Input is a single squared-magnitude spectrogram frame. The input spectrum
+  // is converted to linear magnitude and weighted into bands using a
+  // triangular mel filterbank, and a discrete cosine transform (DCT) of the
+  // values is taken. Output is populated with the lowest dct_coefficient_count
+  // of these values.
+  void Compute(const float* spectrogram_frame, size_t size,
+               float* output) const;
+
+  void set_upper_frequency_limit(double upper_frequency_limit) {
+    // CHECK(!initialized_) << "Set frequency limits before calling
+    // Initialize.";
+    upper_frequency_limit_ = upper_frequency_limit;
+  }
+
+  void set_lower_frequency_limit(double lower_frequency_limit) {
+    // CHECK(!initialized_) << "Set frequency limits before calling
+    // Initialize.";
+    lower_frequency_limit_ = lower_frequency_limit;
+  }
+
+  void set_filterbank_channel_count(int filterbank_channel_count) {
+    /// CHECK(!initialized_) << "Set channel count before calling Initialize.";
+    filterbank_channel_count_ = filterbank_channel_count;
+  }
+
+  void set_dct_coefficient_count(int dct_coefficient_count) {
+    // CHECK(!initialized_) << "Set coefficient count before calling
+    // Initialize.";
+    dct_coefficient_count_ = dct_coefficient_count;
+  }
+
+  int get_dct_coefficient_count(){ return dct_coefficient_count_; }
+
+ private:
+  MfccMelFilterbank mel_filterbank_;
+  MfccDct dct_;
+  bool initialized_;
+  double lower_frequency_limit_;
+  double upper_frequency_limit_;
+  int filterbank_channel_count_;
+  int dct_coefficient_count_;
+};
+
+}  // namespace internal
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_MFCC_H_
diff --git a/third_party/tflite-micro/tensorflow/lite/kernels/internal/mfcc_dct.cc b/third_party/tflite-micro/tensorflow/lite/kernels/internal/mfcc_dct.cc
new file mode 100644
index 00000000..0447aa31
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/kernels/internal/mfcc_dct.cc
@@ -0,0 +1,87 @@
+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/kernels/internal/mfcc_dct.h"
+
+#include <math.h>
+#include <cstdio>
+
+
+namespace tflite {
+namespace internal {
+
+//double cosines_[30][80];
+
+MfccDct::MfccDct() : initialized_(false) {}
+
+bool MfccDct::Initialize(int input_length, int coefficient_count) {
+  coefficient_count_ = coefficient_count; // mfcc output size, 30
+  input_length_ = input_length; // mel_filterbank_.Compute() outpit size, 80
+
+  if (coefficient_count_ < 1) {
+    return false;
+  }
+
+  if (input_length < 1) {
+    return false;
+  }
+
+  if (coefficient_count_ > input_length_) {
+    return false;
+  }
+
+  // cosines_.resize(coefficient_count_); // r x c = coefficient_count_ x input_length_ = 13 x 40, type = double
+  double fnorm = sqrt(2.0 / input_length_);
+  // Some platforms don't have M_PI, so define a local constant here.
+  const double pi = atan(1.0) * 4.0;
+  double arg = pi / input_length_;
+  for (int i = 0; i < coefficient_count_; ++i) {
+    // cosines_[i].resize(input_length_);
+    for (int j = 0; j < input_length_; ++j) {
+      cosines_[i][j] = fnorm * cos(i * arg * (j + 0.5));
+    }
+  }
+
+  initialized_ = true;
+  return true;
+}
+
+void MfccDct::Compute(const double* input, size_t size,
+                      float* output) const { //arr[80], 80, arr[30]
+  //printf("in MfccDct:: Compute\n");
+
+  if (!initialized_) {
+    return;
+  }
+
+  /*
+  output->resize(coefficient_count_);
+    int length = input.size();
+    if (length > input_length_) {
+       length = input_length_;
+  }
+  */
+
+  for (int i = 0; i < coefficient_count_; ++i) { // 30
+    double sum = 0.0;
+    for (unsigned int j = 0; j < size; ++j) { // 80
+      sum += cosines_[i][j] * input[j];
+    }
+    output[i] = sum;
+  }
+}
+
+}  // namespace internal
+}  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/kernels/internal/mfcc_dct.h b/third_party/tflite-micro/tensorflow/lite/kernels/internal/mfcc_dct.h
new file mode 100644
index 00000000..426f81cf
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/kernels/internal/mfcc_dct.h
@@ -0,0 +1,45 @@
+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+// Basic minimal DCT class for MFCC speech processing.
+
+#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_MFCC_DCT_H_
+#define TENSORFLOW_LITE_KERNELS_INTERNAL_MFCC_DCT_H_
+
+#include <cstdio> // for size_t declaration
+
+namespace tflite {
+namespace internal {
+
+
+class MfccDct {
+ public:
+  MfccDct();
+  bool Initialize(int input_length, int coefficient_count);
+  void Compute(const double* input, size_t size,
+               float* output) const;
+
+ private:
+  bool initialized_;
+  int coefficient_count_;
+  int input_length_;
+  double cosines_[30][80];
+  // std::vector<std::vector<double>> vec_cosines_;
+};
+
+}  // namespace internal
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_MFCC_DCT_H_
diff --git a/third_party/tflite-micro/tensorflow/lite/kernels/internal/mfcc_function.cc b/third_party/tflite-micro/tensorflow/lite/kernels/internal/mfcc_function.cc
new file mode 100644
index 00000000..57d0ff10
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/kernels/internal/mfcc_function.cc
@@ -0,0 +1,72 @@
+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <math.h>
+#include <cstdio>
+
+#include "tensorflow/lite/kernels/internal/mfcc.h"
+
+double working[80]; // save stack memory
+
+namespace tflite {
+namespace internal {
+
+const double kDefaultUpperFrequencyLimit = 4000;
+const double kDefaultLowerFrequencyLimit = 20;
+const double kFilterbankFloor = 1e-12;
+const int kDefaultFilterbankChannelCount = 40; //typically
+const int kDefaultDCTCoefficientCount = 13;
+
+Mfcc::Mfcc()
+    : initialized_(false),
+      lower_frequency_limit_(kDefaultLowerFrequencyLimit),
+      upper_frequency_limit_(kDefaultUpperFrequencyLimit),
+      filterbank_channel_count_(kDefaultFilterbankChannelCount),
+      dct_coefficient_count_(kDefaultDCTCoefficientCount) {}
+
+bool Mfcc::Initialize(int input_length, double input_sample_rate) {
+  bool initialized = mel_filterbank_.Initialize(
+      input_length, input_sample_rate, filterbank_channel_count_,
+      lower_frequency_limit_, upper_frequency_limit_);
+
+
+  initialized &=
+      dct_.Initialize(filterbank_channel_count_, dct_coefficient_count_); // 40, 13
+  initialized_ = initialized;
+
+  return initialized;
+}
+
+void Mfcc::Compute(const float* spectrogram_frame, size_t size, float* output) const { //arr[513], 513, arr[30]
+
+  if (!initialized_) {
+    // LOG(ERROR) << "Mfcc not initialized.";
+    return;
+  }
+
+  size_t working_size = mel_filterbank_.Compute(spectrogram_frame, size, working); //arr[513], 513, arr[80]
+  // size = 80, change the following 80 to size will output wrong answer
+  for (unsigned int i = 0; i < working_size; ++i) { // working array only has size of 80
+    double val = working[i];
+    if (val < kFilterbankFloor) {
+      val = kFilterbankFloor;
+    }
+    working[i] = log(val);
+  }
+  dct_.Compute(working, working_size, output); //arr[80], 80, arr[30]
+}
+
+}  // namespace internal
+}  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/kernels/internal/mfcc_mel_filterbank.cc b/third_party/tflite-micro/tensorflow/lite/kernels/internal/mfcc_mel_filterbank.cc
new file mode 100644
index 00000000..8fb1d912
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/kernels/internal/mfcc_mel_filterbank.cc
@@ -0,0 +1,254 @@
+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+// This code resamples the FFT bins, and smooths then with triangle-shaped
+// weights to create a mel-frequency filter bank. For filter i centered at f_i,
+// there is a triangular weighting of the FFT bins that extends from
+// filter f_i-1 (with a value of zero at the left edge of the triangle) to f_i
+// (where the filter value is 1) to f_i+1 (where the filter values returns to
+// zero).
+
+// Note: this code fails if you ask for too many channels.  The algorithm used
+// here assumes that each FFT bin contributes to at most two channels: the
+// right side of a triangle for channel i, and the left side of the triangle
+// for channel i+1.  If you ask for so many channels that some of the
+// resulting mel triangle filters are smaller than a single FFT bin, these
+// channels may end up with no contributing FFT bins.  The resulting mel
+// spectrum output will have some channels that are always zero.
+
+#include "tensorflow/lite/kernels/internal/mfcc_mel_filterbank.h"
+
+#include <cmath>
+#include <cstdio>
+#include <cstring>
+
+/*
+char* uint32_to_dec_cstring(char buf[11], uint32_t n) {
+  for (int i{9}; i >= 0; --i) {
+    // printf("iterating\n");
+    buf[i] = '0' + n % 10;
+    n /= 10;
+  }
+  buf[10] = '\0';
+  return buf;
+}
+*/
+/*
+  printf("[DEBUG] Before\n"); // replace printf(); to know the size of output array
+  fflush(stdout);
+  char num[11];
+  uint32_to_dec_cstring(num, num_channels_); // 80
+  char buf[50] = "num_channels_=";
+  strcat(strcat(buf, num), "\n");
+  fwrite(buf, sizeof(char), sizeof(buf), stderr);
+  printf("[DEBUG] After\n");
+  fflush(stdout);
+*/
+
+namespace tflite {
+namespace internal {
+
+MfccMelFilterbank::MfccMelFilterbank() : initialized_(false) {}
+
+bool MfccMelFilterbank::Initialize(int input_length, double input_sample_rate,
+                                   int output_channel_count,
+                                   double lower_frequency_limit,
+                                   double upper_frequency_limit) {
+  
+  num_channels_ = output_channel_count;
+  sample_rate_ = input_sample_rate;
+  input_length_ = input_length;
+
+  // printf("num_channels + 1 = %d\n", num_channels_ + 1); //81
+  // printf("input_length_ = %d\n", input_length_); // 513
+
+  if (num_channels_ < 1) {
+    // LOG(ERROR) << "Number of filterbank channels must be positive.";
+    return false;
+  }
+
+  if (sample_rate_ <= 0) {
+    // LOG(ERROR) << "Sample rate must be positive.";
+    return false;
+  }
+
+  if (input_length < 2) {
+    // LOG(ERROR) << "Input length must greater than 1.";
+    return false;
+  }
+
+  if (lower_frequency_limit < 0) {
+    // LOG(ERROR) << "Lower frequency limit must be nonnegative.";
+    return false;
+  }
+
+  if (upper_frequency_limit <= lower_frequency_limit) {
+    /// LOG(ERROR) << "Upper frequency limit must be greater than "
+    //           << "lower frequency limit.";
+    return false;
+  }
+  
+
+  // An extra center frequency is computed at the top to get the upper
+  // limit on the high side of the final triangular filter.
+  //center_frequencies_.resize(num_channels_ + 1); 
+  const double mel_low = FreqToMel(lower_frequency_limit);
+  const double mel_hi = FreqToMel(upper_frequency_limit);
+  const double mel_span = mel_hi - mel_low;
+  const double mel_spacing = mel_span / static_cast<double>(num_channels_ + 1);
+  for (int i = 0; i < num_channels_ + 1; ++i) {
+    center_frequencies_[i] = mel_low + (mel_spacing * (i + 1));
+  }
+
+  // Always exclude DC; emulate HTK.
+  const double hz_per_sbin =
+      0.5 * sample_rate_ / static_cast<double>(input_length_ - 1);
+  start_index_ = static_cast<int>(1.5 + (lower_frequency_limit / hz_per_sbin));
+  end_index_ = static_cast<int>(upper_frequency_limit / hz_per_sbin);
+
+
+  // Maps the input spectrum bin indices to filter bank channels/indices. For
+  // each FFT bin, band_mapper tells us which channel this bin contributes to
+  // on the right side of the triangle.  Thus this bin also contributes to the
+  // left side of the next channel's triangle response.
+  //band_mapper_.resize(input_length_);
+  int channel = 0;
+  for (int i = 0; i < input_length_; ++i) {
+    double melf = FreqToMel(i * hz_per_sbin);
+    if ((i < start_index_) || (i > end_index_)) {
+      band_mapper_[i] = -2;  // Indicate an unused Fourier coefficient.
+    } else {
+      while ((channel < num_channels_) &&
+             (center_frequencies_[channel] < melf)) {
+        ++channel;
+      }
+      band_mapper_[i] = channel - 1;  // Can be == -1
+    }
+  }
+
+  // Create the weighting functions to taper the band edges.  The contribution
+  // of any one FFT bin is based on its distance along the continuum between two
+  // mel-channel center frequencies.  This bin contributes weights_[i] to the
+  // current channel and 1-weights_[i] to the next channel.
+  //weights_.resize(input_length_)y;
+  for (int i = 0; i < input_length_; ++i) {
+    channel = band_mapper_[i];
+    if ((i < start_index_) || (i > end_index_)) {
+      weights_[i] = 0.0;
+    } else {
+      if (channel >= 0) {
+        weights_[i] =
+            (center_frequencies_[channel + 1] - FreqToMel(i * hz_per_sbin)) /
+            (center_frequencies_[channel + 1] - center_frequencies_[channel]);
+      } else {
+        weights_[i] = (center_frequencies_[0] - FreqToMel(i * hz_per_sbin)) /
+                      (center_frequencies_[0] - mel_low);
+      }
+    }
+  }
+
+  // Check the sum of FFT bin weights for every mel band to identify
+  // situations where the mel bands are so narrow that they don't get
+  // significant weight on enough (or any) FFT bins -- i.e., too many
+  // mel bands have been requested for the given FFT size.
+#if 0
+  std::vector<int> bad_channels;
+  for (int c = 0; c < num_channels_; ++c) {
+    float band_weights_sum = 0.0;
+    for (int i = 0; i < input_length_; ++i) {
+      if (band_mapper_[i] == c - 1) {
+        band_weights_sum += (1.0 - weights_[i]);
+      } else if (band_mapper_[i] == c) {
+        band_weights_sum += weights_[i];
+      }
+    }
+    // The lowest mel channels have the fewest FFT bins and the lowest
+    // weights sum.  But given that the target gain at the center frequency
+    // is 1.0, if the total sum of weights is 0.5, we're in bad shape.
+    // printf("band_weights_sum = %f\n", band_weights_sum);
+    if (band_weights_sum < 0.5) {
+      bad_channels.push_back(c);
+    }
+  }
+
+  if (!bad_channels.empty()) {
+    // The following are commented out so that "bad_channels" vector might not necessary
+    LOG(ERROR) << "Missing " << bad_channels.size() << " bands "
+               << " starting at " << bad_channels[0]
+               << " in mel-frequency design. "
+               << "Perhaps too many channels or "
+               << "not enough frequency resolution in spectrum. ("
+               << "input_length: " << input_length
+               << " input_sample_rate: " << input_sample_rate
+               << " output_channel_count: " << output_channel_count
+               << " lower_frequency_limit: " << lower_frequency_limit
+               << " upper_frequency_limit: " << upper_frequency_limit;
+  }
+#endif
+
+  initialized_ = true;
+  return true;
+}
+
+
+
+// Compute the mel spectrum from the squared-magnitude FFT input by taking the
+// square root, then summing FFT magnitudes under triangular integration windows
+// whose widths increase with frequency.
+size_t MfccMelFilterbank::Compute(const float* input, size_t size,
+                                double* output) const { // arr[513], 513, arr[80]
+
+  if (!initialized_) {
+    // LOG(ERROR) << "Mel Filterbank not initialized.";
+    return -1;
+  }
+
+  if ((int)size <= end_index_) {
+    // LOG(ERROR) << "Input too short to compute filterbank";
+    return -1;
+  }
+
+  // Ensure output is right length and reset all values.
+  //output->assign(num_channels_, 0.0);
+  for (int i{0}; i < num_channels_; ++i) output[i] = 0.0;
+
+  int channel;
+  for (int i = start_index_; i < end_index_; i++) { // For each FFT bin
+
+    double spec_val = sqrt(input[i]);
+    double weighted = spec_val * weights_[i];
+    channel = band_mapper_[i];
+    if (channel >= 0)
+      output[channel] += weighted;
+      //(*output)[channel] += weighted;  // Right side of triangle, downward slope
+    channel++;
+    if (channel < num_channels_)
+      output[channel] += spec_val - weighted;
+      //(*output)[channel] += spec_val - weighted;  // Left side of triangle
+  }
+  /*
+  if (channel < num_channels_)
+    return num_channels_;
+  return channel;
+  */
+  return num_channels_;
+}
+
+double MfccMelFilterbank::FreqToMel(double freq) const {
+  return 1127.0 * log1p(freq / 700.0);
+}
+
+}  // namespace internal
+}  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/kernels/internal/mfcc_mel_filterbank.h b/third_party/tflite-micro/tensorflow/lite/kernels/internal/mfcc_mel_filterbank.h
new file mode 100644
index 00000000..88ede782
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/kernels/internal/mfcc_mel_filterbank.h
@@ -0,0 +1,66 @@
+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+// Basic class for applying a mel-scale mapping to a power spectrum.
+
+#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_MFCC_MEL_FILTERBANK_H_
+#define TENSORFLOW_LITE_KERNELS_INTERNAL_MFCC_MEL_FILTERBANK_H_
+
+#include <cstdio> // for size_t declaration
+
+namespace tflite {
+namespace internal {
+
+class MfccMelFilterbank {
+ public:
+  MfccMelFilterbank();
+  bool Initialize(int input_length,  // Number of unique FFT bins fftsize/2+1.
+                  double input_sample_rate, int output_channel_count,
+                  double lower_frequency_limit, double upper_frequency_limit);
+
+  // Takes a squared-magnitude spectrogram slice as input, computes a
+  // triangular-mel-weighted linear-magnitude filterbank, and places the result
+  // in output.
+  size_t Compute(const float* input, size_t size,
+               double* output) const;
+
+ private:
+  double FreqToMel(double freq) const;
+  bool initialized_;
+  int num_channels_;
+  double sample_rate_;
+  int input_length_;
+  //std::vector<double> center_frequencies_;  // In mel, for each mel channel.
+  double center_frequencies_[81];
+  // Each FFT bin b contributes to two triangular mel channels, with
+  // proportion weights_[b] going into mel channel band_mapper_[b], and
+  // proportion (1 - weights_[b]) going into channel band_mapper_[b] + 1.
+  // Thus, weights_ contains the weighting applied to each FFT bin for the
+  // upper-half of the triangular band.
+  //std::vector<double> weights_;  // Right-side weight for this fft  bin.
+  double weights_[513];
+
+  // FFT bin i contributes to the upper side of mel channel band_mapper_[i]
+  //std::vector<int> band_mapper_;
+  int band_mapper_[513];
+
+  int start_index_;  // Lowest FFT bin used to calculate mel spectrum.
+  int end_index_;    // Highest FFT bin used to calculate mel spectrum.
+};
+
+}  // namespace internal
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_MFCC_MEL_FILTERBANK_H_
diff --git a/third_party/tflite-micro/tensorflow/lite/kernels/internal/spectrogram.cc b/third_party/tflite-micro/tensorflow/lite/kernels/internal/spectrogram.cc
new file mode 100644
index 00000000..bb2a124f
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/kernels/internal/spectrogram.cc
@@ -0,0 +1,316 @@
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/kernels/internal/spectrogram.h"
+
+#include <assert.h>
+#include <math.h>
+#include <cstdio>
+//#include <string.h> // memset
+#include "third_party/fft2d/fft.h"
+#include "tensorflow/lite/micro/kernels/kernel_util.h"
+#include "tensorflow/lite/micro/memory_helpers.h"
+#include "tensorflow/lite/kernels/op_macros.h"
+#include "tensorflow/lite/kernels/kernel_util.h"
+
+
+namespace tflite {
+namespace internal {
+
+using std::complex;
+
+/*
+namespace {
+  
+// Returns the default Hann window function for the spectrogram.
+void GetPeriodicHann(int window_length, double* window_) {
+  // Some platforms don't have M_PI, so define a local constant here.
+  const double pi = std::atan(1.0) * 4.0;
+ 
+  //window->resize(window_length);  
+  for (int i = 0; i < window_length; ++i) {
+    window_[i] = 0.5 - 0.5 * cos((2.0 * pi * i) / window_length);
+  }
+}
+
+}  // namespace
+
+
+
+inline int Log2Floor(uint32_t n) {
+  if (n == 0) return -1;
+  int log = 0;
+  uint32_t value = n;
+  for (int i = 4; i >= 0; --i) {
+    int shift = (1 << i);
+    uint32_t x = value >> shift;
+    if (x != 0) {
+      value = x;
+      log += shift;
+    }
+  }
+  return log;
+}
+
+inline int Log2Ceiling(uint32_t n) {
+  int floor = Log2Floor(n);
+  if (n == (n & ~(n - 1)))  // zero or a power of two
+    return floor;
+  else
+    return floor + 1;
+}
+
+inline uint32_t NextPowerOfTwo(uint32_t value) {
+  int exponent = Log2Ceiling(value);
+  // DCHECK_LT(exponent, std::numeric_limits<uint32>::digits);
+  return 1 << exponent;
+}
+*/
+
+bool Spectrogram::Initialize(int window_length, int step_length, int input_length, int fft_length, int output_frequency_channels) {
+  window_length_ = window_length;
+  //GetPeriodicHann(window_length_, window_);
+  // Some platforms don't have M_PI, so define a local constant here.
+  const double pi = std::atan(1.0) * 4.0;
+ 
+  //window->resize(window_length);  
+  for (int i = 0; i < window_length_; ++i) {
+    window_[i] = 0.5 - 0.5 * cos((2.0 * pi * i) / window_length_);
+  }
+
+  if (window_length_ < 2) {
+    // LOG(ERROR) << "Window length too short.";
+    initialized_ = false;
+    return false;
+  }
+
+  step_length_ = step_length;
+  if (step_length_ < 1) {
+    // LOG(ERROR) << "Step length must be positive.";
+    initialized_ = false;
+    return false;
+  }
+
+
+  //fft_length_ = NextPowerOfTwo(window_length_); // 1024
+  // CHECK(fft_length_ >= window_length_);
+
+  // output_frequency_channels_ = 1 + fft_length_ / 2; // 513
+  // int half_fft_length = fft_length_ / 2; // 512
+  fft_length_ = fft_length;
+  output_frequency_channels_ = output_frequency_channels;
+  
+  // Allocate 2 more than what rdft needs, so we can rationalize the layout.
+  //fft_input_output_.assign(fft_length_ + 2, 0.0); // 1026
+  //fft_double_working_area_.assign(half_fft_length, 0.0);
+  //fft_integer_working_area_.assign(2 + static_cast<int>(sqrt(half_fft_length)),0);
+  
+  //printf("fft_length = %d\n", fft_length_); // 1024
+  //printf("half_fft_length = %d\n", half_fft_length); // 512
+  //printf("fft_integer_working_area_ = %d\n", 2 + static_cast<int>(sqrt(half_fft_length))); // 24
+
+  
+  // Set flag element to ensure that the working areas are initialized
+  // on the first call to cdft.  It's redundant given the assign above,
+  // but keep it as a reminder.
+  
+  // test w & w/o initializing above array with 0, the output is still the same
+  //fft_integer_working_area_[0] = 0;
+  samples_to_next_step_ = step_length_;
+  input_length_ = input_length;
+  initialized_ = true;
+  return true;
+}
+/*
+template <class InputSample, class OutputSample>
+bool Spectrogram::ComputeComplexSpectrogram(
+    const InputSample* input,
+    std::complex<OutputSample> *output) {
+  if (!initialized_) { return false; }
+
+  //output->clear();
+  cur_output = 0;
+  int input_start = 0;
+  fft_integer_working_area_[0] = 0;
+  
+  while (GetNextWindowOfSamples(input, &input_start)) {
+    // DCHECK_EQ(input_queue_.size(), window_length_);
+    ProcessCoreFFT();  // Processes input_queue_ to fft_input_output_.
+    
+    cur_output += 1;
+    // Get a reference to the newly added slice to fill in.
+    auto* spectrogram_slice = output + cur_output*output_frequency_channels_;
+
+    for (int i = 0; i < output_frequency_channels_; ++i) {
+      // This will convert double to float if it needs to.
+      spectrogram_slice[i] = complex<OutputSample>(
+          fft_input_output_[2 * i], fft_input_output_[2 * i + 1]);
+    }
+  }
+  return true;
+}
+
+// Instantiate it four ways:
+template bool Spectrogram::ComputeComplexSpectrogram(
+    const float* input, 
+    std::complex<float>*);
+template bool Spectrogram::ComputeComplexSpectrogram(
+    const double* input,
+    std::complex<float>*);
+template bool Spectrogram::ComputeComplexSpectrogram(
+    const float* input,
+    std::complex<double>*);
+template bool Spectrogram::ComputeComplexSpectrogram(
+    const double* input,
+    std::complex<double>*);
+*/
+template <class InputSample, class OutputSample>
+bool Spectrogram::ComputeSquaredMagnitudeSpectrogram(
+    const InputSample* input,
+    OutputSample *output){
+  if (!initialized_) { return false; }
+
+  //output->clear();
+  cur_output = -1;
+  int input_start = 0;
+  
+  while (GetNextWindowOfSamples(input, &input_start)) {
+    // DCHECK_EQ(input_queue_.size(), window_length_);
+    ProcessCoreFFT();  // Processes input_queue_ to fft_input_output_.
+    // Add a new slice vector onto the output, to save new result to.
+    //output->resize(output->size() + 1);
+    cur_output += 1;
+    
+    // Get a reference to the newly added slice to fill in.
+    //auto& spectrogram_slice = output->back();
+    auto* spectrogram_slice = output + cur_output * output_frequency_channels_;
+
+    //spectrogram_slice.resize(output_frequency_channels_);
+    
+    for (int i = 0; i < output_frequency_channels_; ++i) {
+      // Similar to the Complex case, except storing the norm.
+      // But the norm function is known to be a performance killer,
+      // so do it this way with explicit real and imaginary temps.
+      const double re = fft_input_output_[2 * i];
+      const double im = fft_input_output_[2 * i + 1];
+      // Which finally converts double to float if it needs to.
+      spectrogram_slice[i] = re * re + im * im;
+
+    }
+  }
+  return true;
+}
+
+// Instantiate it four ways:
+template bool Spectrogram::ComputeSquaredMagnitudeSpectrogram(
+    const float* input, float*);
+template bool Spectrogram::ComputeSquaredMagnitudeSpectrogram(
+    const double* input, float*);
+template bool Spectrogram::ComputeSquaredMagnitudeSpectrogram(
+    const float* input, double*);
+template bool Spectrogram::ComputeSquaredMagnitudeSpectrogram(
+    const double* input, double*);
+
+// Return true if a full window of samples is prepared; manage the queue.
+template <class InputSample>
+bool Spectrogram::GetNextWindowOfSamples(
+    const InputSample* input,
+    int* input_start) {
+
+  auto* input_it = input + *input_start;
+  int input_remaining = input + input_length_ - input_it; //stream_non_stream_change : non stream 16000, stream : 640
+
+  if(input_remaining >= window_length_)
+  {
+    memcpy(input_queue_, input_it, window_length_ * sizeof(float));
+    *input_start += samples_to_next_step_;
+    // DCHECK_EQ(window_length_, input_queue_.size());
+    samples_to_next_step_ = step_length_;  // Be ready for next time.
+    return true;  // Yes, input_queue_ now contains exactly a window-full.
+  }
+  return false;
+  /*
+  if(input_remaining < window_length_){
+    // Copy in as many samples are left and return false, no full window.
+    for(int i = 0; i < input_remaining; i++)
+    {
+        input_queue_[i] = *(input_it + i);
+    }
+
+    *input_start += input_remaining;  // Increases it to input.size().
+    samples_to_next_step_ -= input_remaining;
+    return false;  // Not enough for a full window.
+  } 
+  else 
+  {
+    // Copy just enough into queue to make a new window, then trim the
+    // front off the queue to make it window-sized.
+    for(int i = 0; i < window_length_; i++)
+    {
+        input_queue_[i] = input_it[i];
+    }
+    *input_start += samples_to_next_step_;
+    // DCHECK_EQ(window_length_, input_queue_.size());
+    samples_to_next_step_ = step_length_;  // Be ready for next time.
+    return true;  // Yes, input_queue_ now contains exactly a window-full.
+  }
+  */
+}
+
+void Spectrogram::ProcessCoreFFT() {
+
+  for (int j = 0; j < window_length_; ++j) {
+    fft_input_output_[j] = (double)input_queue_[j] * (double)window_[j];
+  }
+  
+  // Zero-pad the rest of the input buffer.
+  for (int j = window_length_; j < fft_length_; ++j) {
+    fft_input_output_[j] = 0.0;
+  }
+  
+  const int kForwardFFT = 1;  // 1 means forward; -1 reverse.
+  // This real FFT is a fair amount faster than using cdft here.
+  rdft(fft_length_, kForwardFFT, fft_input_output_, fft_integer_working_area_, fft_double_working_area_);
+  
+  // Make rdft result look like cdft result;
+  // unpack the last real value from the first position's imag slot.
+  fft_input_output_[fft_length_] = fft_input_output_[1];
+  fft_input_output_[fft_length_ + 1] = 0;
+  fft_input_output_[1] = 0;
+}
+
+}  // namespace internal
+}  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/kernels/internal/spectrogram.h b/third_party/tflite-micro/tensorflow/lite/kernels/internal/spectrogram.h
new file mode 100644
index 00000000..f39d5baa
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/kernels/internal/spectrogram.h
@@ -0,0 +1,155 @@
+
+
+
+
+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+// Class for generating spectrogram slices from a waveform.
+// Initialize() should be called before calls to other functions.  Once
+// Initialize() has been called and returned true, The Compute*() functions can
+// be called repeatedly with sequential input data (ie. the first element of the
+// next input vector directly follows the last element of the previous input
+// vector). Whenever enough audio samples are buffered to produce a
+// new frame, it will be placed in output. Output is cleared on each
+// call to Compute*(). This class is thread-unsafe, and should only be
+// called from one thread at a time.
+// With the default parameters, the output of this class should be very
+// close to the results of the following MATLAB code:
+// overlap_samples = window_length_samples - step_samples;
+// window = hann(window_length_samples, 'periodic');
+// S = abs(spectrogram(audio, window, overlap_samples)).^2;
+
+#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_SPECTROGRAM_H_
+#define TENSORFLOW_LITE_KERNELS_INTERNAL_SPECTROGRAM_H_
+
+#include <complex>
+
+#include "third_party/fft2d/fft.h"
+
+namespace tflite {
+namespace internal {
+
+class Spectrogram {
+ public:
+  Spectrogram() : initialized_(false) {}
+  ~Spectrogram() {}
+
+  // Initializes the class with a given window length and step length
+  // (both in samples). Internally a Hann window is used as the window
+  // function. Returns true on success, after which calls to Process()
+  // are possible. window_length must be greater than 1 and step
+  // length must be greater than 0.
+  // Initialize with an explicit window instead of a length.
+    bool Initialize(int window_length, int step_length, int input_length, int fft_length, int output_frequency_channels);
+
+
+  // Processes an arbitrary amount of audio data (contained in input)
+  // to yield complex spectrogram frames. After a successful call to
+  // Initialize(), Process() may be called repeatedly with new input data
+  // each time.  The audio input is buffered internally, and the output
+  // vector is populated with as many temporally-ordered spectral slices
+  // as it is possible to generate from the input.  The output is cleared
+  // on each call before the new frames (if any) are added.
+  // The template parameters can be float or double.
+  /*
+  template <class InputSample, class OutputSample>
+  bool ComputeComplexSpectrogram(
+      //const std::vector<InputSample>& input,
+      const InputSample* input,
+      //std::vector<std::vector<std::complex<OutputSample>>>* output);
+      std::complex<OutputSample> *output );
+  */
+
+  // This function works as the one above, but returns the power
+  // (the L2 norm, or the squared magnitude) of each complex value.
+  template <class InputSample, class OutputSample>
+  bool ComputeSquaredMagnitudeSpectrogram(
+      //const std::vector<InputSample>& input,
+      const InputSample* input,
+      //std::vector<std::vector<OutputSample>>* output
+      OutputSample* output );
+
+  // Return reference to the window function used internally.
+  //const double* GetWindow() const { return window_; }
+
+  // Return the number of frequency channels in the spectrogram.
+  int output_frequency_channels() const { return output_frequency_channels_; }
+  //float * get_input_for_channel_()  { return input_for_channel_; }
+  //float * get_spectrogram_output_()  { return reinterpret_cast<float *>(spectrogram_output_); }
+
+ private:
+  template <class InputSample>
+  bool GetNextWindowOfSamples(
+    //const std::vector<InputSample>& input,
+    const InputSample* input,
+    int* input_start);
+  void ProcessCoreFFT();
+
+  int fft_length_;
+  int output_frequency_channels_;
+  int window_length_;
+  int step_length_;
+  bool initialized_;
+  int samples_to_next_step_;
+
+  
+  //std::vector<double> window_;
+  //std::vector<double> fft_input_output_;
+  //std::deque<double> input_queue_;
+  double window_[640];
+  double fft_input_output_[1026];
+  float input_queue_[16000];
+
+  // Working data areas for the FFT routines.
+  //std::vector<int> fft_integer_working_area_;
+  //std::vector<double> fft_double_working_area_;
+  int fft_integer_working_area_[24];
+  double fft_double_working_area_[512];
+
+  int cur_output;
+  int input_length_;
+};
+
+/*
+// Explicit instantiations in spectrogram.cc.
+extern template bool Spectrogram::ComputeComplexSpectrogram(
+    const float* input,
+    std::complex<float> *);
+extern template bool Spectrogram::ComputeComplexSpectrogram(
+    const double* input,
+    std::complex<float> *);
+extern template bool Spectrogram::ComputeComplexSpectrogram(
+    const float* input,
+    std::complex<double> *);
+extern template bool Spectrogram::ComputeComplexSpectrogram(
+    const double* input,
+    std::complex<double> *);
+*/
+
+extern template bool Spectrogram::ComputeSquaredMagnitudeSpectrogram(
+    const float* input, float *);
+extern template bool Spectrogram::ComputeSquaredMagnitudeSpectrogram(
+    const double* input, float *);
+extern template bool Spectrogram::ComputeSquaredMagnitudeSpectrogram(
+    const float* input, double *);
+extern template bool Spectrogram::ComputeSquaredMagnitudeSpectrogram(
+    const double* input, double *);
+
+
+}  // namespace internal
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_SPECTROGRAM_H_
diff --git a/third_party/tflite-micro/tensorflow/lite/kernels/internal/tensor.h b/third_party/tflite-micro/tensorflow/lite/kernels/internal/tensor.h
new file mode 100644
index 00000000..84de43ca
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/kernels/internal/tensor.h
@@ -0,0 +1,47 @@
+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_TENSOR_H_
+#define TENSORFLOW_LITE_KERNELS_INTERNAL_TENSOR_H_
+
+// Most functionality has been moved into a version of this file that doesn't
+// rely on std::string, so that it can be used in TFL Micro.
+#include "tensorflow/lite/kernels/internal/portable_tensor.h"
+#include "tensorflow/lite/string_util.h"
+
+namespace tflite {
+
+template <>
+class SequentialTensorWriter<string> {
+ public:
+  SequentialTensorWriter(const TfLiteTensor* input, TfLiteTensor* output)
+      : input_(input), output_(output) {}
+  ~SequentialTensorWriter() { buffer_.WriteToTensor(output_, nullptr); }
+
+  void Write(int position) { this->WriteN(position, 1); }
+  void WriteN(int position, int len) {
+    for (int i = 0; i < len; i++) {
+      buffer_.AddString(GetString(input_, position + i));
+    }
+  }
+
+ private:
+  const TfLiteTensor* input_;
+  TfLiteTensor* output_;
+  DynamicBuffer buffer_;
+};
+
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_TENSOR_H_
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/all_ops_resolver.cc b/third_party/tflite-micro/tensorflow/lite/micro/all_ops_resolver.cc
index 150f4525..5faf8042 100644
--- a/third_party/tflite-micro/tensorflow/lite/micro/all_ops_resolver.cc
+++ b/third_party/tflite-micro/tensorflow/lite/micro/all_ops_resolver.cc
@@ -1,3 +1,4 @@
+
 /* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
 
 Licensed under the Apache License, Version 2.0 (the "License");
@@ -27,6 +28,8 @@ AllOpsResolver::AllOpsResolver() {
   AddArgMax();
   AddArgMin();
   AddAssignVariable();
+  // add new
+  AddAudio_Spectrogram();
   AddAveragePool2D();
   AddBatchToSpaceNd();
   AddBroadcastArgs();
@@ -74,6 +77,8 @@ AllOpsResolver::AllOpsResolver() {
   AddMaxPool2D();
   AddMaximum();
   AddMean();
+  // add new
+  AddMFCC();
   AddMinimum();
   AddMirrorPad();
   AddMul();
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/arena_allocator/non_persistent_arena_buffer_allocator.cc b/third_party/tflite-micro/tensorflow/lite/micro/arena_allocator/non_persistent_arena_buffer_allocator.cc
index a8f00ead..7a82f95c 100644
--- a/third_party/tflite-micro/tensorflow/lite/micro/arena_allocator/non_persistent_arena_buffer_allocator.cc
+++ b/third_party/tflite-micro/tensorflow/lite/micro/arena_allocator/non_persistent_arena_buffer_allocator.cc
@@ -72,7 +72,7 @@ TfLiteStatus NonPersistentArenaBufferAllocator::ResetTempAllocations() {
   if (!IsAllTempDeallocated()) {
     MicroPrintf(
         "All temp buffers must be freed before calling ResetTempAllocations()");
-    return kTfLiteError;
+    //return kTfLiteError;
   }
   next_temp_ = head_temp_;
   return kTfLiteOk;
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/README.md b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/README.md
new file mode 100644
index 00000000..fc77f264
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/README.md
@@ -0,0 +1,106 @@
+# EmbARC MLI Library Based Optimizations of TensorFlow Lite Micro Kernels for ARC Platforms.
+
+## Maintainers
+
+*   [dzakhar](https://github.com/dzakhar)
+*   [JaccovG](https://github.com/JaccovG)
+*   [gerbauz](https://github.com/gerbauz)
+
+## Introduction
+
+This folder contains kernel implementations which use optimized
+[embARC MLI Library](https://github.com/foss-for-synopsys-dwc-arc-processors/embarc_mli).
+It allows acceleration of inference operations which use int8 (asymmetric
+quantization).
+
+## Usage
+
+embARC MLI Library is used to speed up execution of some kernels for 
+asymmetrically quantized layers and can be applied with the option `OPTIMIZED_KERNEL_DIR=arc_mli`.
+This means that usual library generation for
+ARC specific target implies usage of embARC MLI.
+
+For example:
+
+```
+make -f tensorflow/lite/micro/tools/make/Makefile clean
+make -f tensorflow/lite/micro/tools/make/Makefile TARGET=arc_emsdp \
+OPTIMIZED_KERNEL_DIR=arc_mli TARGET_ARCH=arc \
+microlite
+```
+
+In case MLI implementation canÃ¢â‚¬â„¢t be used, kernels in this folder fallback to
+TFLM reference implementations. For applications which may not benefit from MLI
+library, TF Lite Micro library can be generated without these implementations **removing** `OPTIMIZED_KERNEL_DIR=arc_mli` in the command line, which can reduce overall code size:
+
+```
+make -f tensorflow/lite/micro/tools/make/Makefile clean
+make -f tensorflow/lite/micro/tools/make/Makefile TARGET=arc_emsdp \
+TARGET_ARCH=arc \
+microlite
+```
+---
+### Optional (experimental features):
+
+TFLM can be built using [embARC MLI Library 2.0](https://github.com/foss-for-synopsys-dwc-arc-processors/embarc_mli/tree/Release_2.0_EA) as an experimental feature.
+To build TFLM using the embARC MLI Library 2.0, add the following tag to the command:
+```
+ARC_TAGS=mli20_experimental
+```
+In this case, generated projectes will be in <tcf_file_basename>_mli20_arc_default folder.
+
+Some of configurations may require a custom run-time library specified using the BUILD_LIB_DIR option. Please, check MLI Library 2.0 [documentation](https://github.com/foss-for-synopsys-dwc-arc-processors/embarc_mli/tree/Release_2.0_EA#build-configuration-options) for more details. The following option can be added:
+```
+BUILD_LIB_DIR=<path_to_buildlib>
+```
+## Limitations
+
+Currently, the MLI Library provides optimized implementation only for int8
+(asymmetric) versions of the following kernels: 
+1. Convolution 2D Ã¢â‚¬â€œ Per axis
+quantization only, `dilation_ratio==1` 
+2. Depthwise Convolution 2D Ã¢â‚¬â€œ Per axis
+quantization only, `dilation_ratio==1` 
+3. Average Pooling 
+4. Max Pooling 
+5. Fully Connected
+
+## Scratch Buffers and Slicing
+
+The following information applies only for ARC EM SDP, VPX and other targets with XY or VCCM
+memory. embARC MLI uses specific optimizations which assumes node operands are
+in XY, VCCM memory and/or DCCM (Data Closely Coupled Memory). As operands might be
+quite big and may not fit in available XY or VCCM memory, special slicing logic is
+applied which allows kernel calculations to be split into multiple parts. For
+this reason, internal static buffers are allocated in these X, Y, VCCM and DCCM memory
+banks and used to execute sub-calculations.
+
+All this is performed automatically and invisible to the user. Half of the DCCM
+memory bank and the full XY banks or 3/4 of VCCM bank are occupied for MLI specific needs.
+If the user needs space in XY or VCCM memory for other tasks, these arrays can be reduced by
+setting specific sizes. For this, add the following option to build command
+replacing **<size[a|b|c]>** with required values:
+
+**For EM:**
+```
+EXT_CFLAGS="-DSCRATCH_MEM_Z_SIZE=<size_a> -DSCRATCH_MEM_X_SIZE=<size_b> -DSCRATCH_MEM_Y_SIZE=<size_c>"
+```
+**For VPX:**
+```
+EXT_CFLAGS="-DSCRATCH_MEM_VEC_SIZE=<size_a>"
+```
+
+For example, to reduce sizes of arrays placed in DCCM and XCCM to 32k and 8k
+respectively, use next command:
+
+```
+make -f tensorflow/lite/micro/tools/make/Makefile <...> \
+EXT_CFLAGS="-DSCRATCH_MEM_Z_SIZE=32*1024 -DSCRATCH_MEM_X_SIZE=8*1024" \
+microlite
+```
+
+## License
+
+TensorFlow's code is covered by the Apache2 License included in the repository,
+and third party dependencies are covered by their respective licenses, in the
+third_party folder of this package.
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/add.cc b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/add.cc
new file mode 100644
index 00000000..d6cbddd6
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/add.cc
@@ -0,0 +1,424 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/kernels/internal/reference/add.h"
+
+#include <algorithm>
+#include <limits>
+
+#include "mli_api.h"  // NOLINT
+#include "tensorflow/lite/c/builtin_op_data.h"
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/kernels/internal/quantization_util.h"
+#include "tensorflow/lite/kernels/internal/reference/integer_ops/add.h"
+#include "tensorflow/lite/kernels/internal/reference/process_broadcast_shapes.h"
+#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
+#include "tensorflow/lite/kernels/kernel_util.h"
+#include "tensorflow/lite/kernels/op_macros.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/mli_slicers.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/mli_tf_utils.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/scratch_buf_mgr.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.h"
+#include "tensorflow/lite/micro/kernels/kernel_util.h"
+#include "tensorflow/lite/micro/memory_helpers.h"
+#include "tensorflow/lite/micro/micro_log.h"
+
+namespace tflite {
+
+constexpr int kInputTensor1 = 0;
+constexpr int kInputTensor2 = 1;
+constexpr int kOutputTensor = 0;
+
+struct OpData {
+  bool requires_broadcast;
+
+  // These fields are used in both the general 8-bit -> 8bit quantized path,
+  // and the special 16-bit -> 16bit quantized path
+  int input1_shift;
+  int input2_shift;
+  int32_t output_activation_min;
+  int32_t output_activation_max;
+
+  // These fields are used only in the general 8-bit -> 8bit quantized path
+  int32_t input1_multiplier;
+  int32_t input2_multiplier;
+  int32_t output_multiplier;
+  int output_shift;
+  int left_shift;
+  int32_t input1_offset;
+  int32_t input2_offset;
+  int32_t output_offset;
+
+  // Used only for float evals:
+  float output_activation_min_f32;
+  float output_activation_max_f32;
+
+  // The result of checking if MLI optimized version of tensors can be used.
+  bool is_mli_applicable;
+
+  // Tensors in MLI format.
+  mutable ops::micro::MliTensorInterface mli_input1;
+  mutable ops::micro::MliTensorInterface mli_input2;
+  mutable ops::micro::MliTensorInterface mli_out;
+};
+
+TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteAddParams* params,
+                             const TfLiteTensor* input1,
+                             const TfLiteTensor* input2, TfLiteTensor* output,
+                             OpData* data) {
+  data->requires_broadcast = !HaveSameShapes(input1, input2);
+
+  if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt8) {
+    TF_LITE_ENSURE_STATUS(CalculateActivationRangeQuantized(
+        context, params->activation, output, &data->output_activation_min,
+        &data->output_activation_max));
+
+    // MLI 2.0 optimized version only supports int8_t datatype and min/max
+    // within container range. Broadcasting isn't supported on the primitive
+    // level (but might be implemented as part of slicing in future)
+#ifdef MLI_2_0  //
+    data->is_mli_applicable =
+        (input1->type == kTfLiteInt8) && (input2->type == kTfLiteInt8) &&
+        (output->type == kTfLiteInt8) && !data->requires_broadcast &&
+        data->output_activation_min == std::numeric_limits<int8_t>::min() &&
+        data->output_activation_max == std::numeric_limits<int8_t>::max();
+#else
+    data->is_mli_applicable = false;
+#endif
+
+    if (data->is_mli_applicable) {
+      data->mli_input1 =
+          ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+              context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+      data->mli_input2 =
+          ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+              context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+      data->mli_out = ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+          context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+
+      ops::micro::ConvertToMliTensor(input1, &data->mli_input1);
+      ops::micro::ConvertToMliTensor(input2, &data->mli_input2);
+      ops::micro::ConvertToMliTensor(output, &data->mli_out);
+      /* Flatten tensors to simplify the process (as we don't support
+       * broadcasting). */
+      data->mli_input1.Shape()[0] =
+          mli_hlp_count_elem_num(data->mli_input1.MliTensor(), 0);
+      data->mli_input2.Shape()[0] =
+          mli_hlp_count_elem_num(data->mli_input2.MliTensor(), 0);
+      data->mli_out.Shape()[0] =
+          mli_hlp_count_elem_num(data->mli_out.MliTensor(), 0);
+      data->mli_input1.MemStride()[0] = data->mli_input2.MemStride()[0] = 1;
+      data->mli_out.MemStride()[0] = 1;
+      *data->mli_input1.Rank() = *data->mli_input2.Rank() = 1;
+      *data->mli_out.Rank() = 1;
+    }
+  } else {
+    data->is_mli_applicable = false;
+  }
+
+#if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+  if (output->type == kTfLiteInt8 || output->type == kTfLiteInt16) {
+    // 8bit -> 8bit general quantized path, with general rescalings
+    data->input1_offset = -input1->params.zero_point;
+    data->input2_offset = -input2->params.zero_point;
+    data->output_offset = output->params.zero_point;
+    data->left_shift = (output->type == kTfLiteInt16) ? 15 : 20;
+    const double twice_max_input_scale =
+        2 * static_cast<double>(
+                std::max(input1->params.scale, input2->params.scale));
+    const double real_input1_multiplier =
+        static_cast<double>(input1->params.scale) / twice_max_input_scale;
+    const double real_input2_multiplier =
+        static_cast<double>(input2->params.scale) / twice_max_input_scale;
+    const double real_output_multiplier =
+        twice_max_input_scale /
+        ((1 << data->left_shift) * static_cast<double>(output->params.scale));
+
+    QuantizeMultiplierSmallerThanOneExp(
+        real_input1_multiplier, &data->input1_multiplier, &data->input1_shift);
+
+    QuantizeMultiplierSmallerThanOneExp(
+        real_input2_multiplier, &data->input2_multiplier, &data->input2_shift);
+
+    QuantizeMultiplierSmallerThanOneExp(
+        real_output_multiplier, &data->output_multiplier, &data->output_shift);
+
+    TF_LITE_ENSURE_STATUS(CalculateActivationRangeQuantized(
+        context, params->activation, output, &data->output_activation_min,
+        &data->output_activation_max));
+  } else if (output->type == kTfLiteFloat32) {
+    CalculateActivationRange(params->activation,
+                             &data->output_activation_min_f32,
+                             &data->output_activation_max_f32);
+#endif  // !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+  }
+
+  return kTfLiteOk;
+}
+
+TfLiteStatus EvalAdd(TfLiteContext* context, TfLiteNode* node,
+                     TfLiteAddParams* params, const OpData* data,
+                     const TfLiteEvalTensor* input1,
+                     const TfLiteEvalTensor* input2, TfLiteEvalTensor* output) {
+#if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+  tflite::ArithmeticParams op_params;
+  SetActivationParams(data->output_activation_min_f32,
+                      data->output_activation_max_f32, &op_params);
+  if (data->requires_broadcast) {
+    reference_ops::BroadcastAdd4DSlow(
+        op_params, tflite::micro::GetTensorShape(input1),
+        tflite::micro::GetTensorData<float>(input1),
+        tflite::micro::GetTensorShape(input2),
+        tflite::micro::GetTensorData<float>(input2),
+        tflite::micro::GetTensorShape(output),
+        tflite::micro::GetTensorData<float>(output));
+  } else {
+    reference_ops::Add(op_params, tflite::micro::GetTensorShape(input1),
+                       tflite::micro::GetTensorData<float>(input1),
+                       tflite::micro::GetTensorShape(input2),
+                       tflite::micro::GetTensorData<float>(input2),
+                       tflite::micro::GetTensorShape(output),
+                       tflite::micro::GetTensorData<float>(output));
+  }
+  return kTfLiteOk;
+#else
+  MicroPrintf("Node configuration is not supported by ARC MLI Library.");
+  return kTfLiteError;
+#endif
+}
+
+TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
+                              TfLiteAddParams* params, const OpData* data,
+                              const TfLiteEvalTensor* input1,
+                              const TfLiteEvalTensor* input2,
+                              TfLiteEvalTensor* output) {
+#if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+  tflite::ArithmeticParams op_params;
+  op_params.left_shift = data->left_shift;
+  op_params.input1_offset = data->input1_offset;
+  op_params.input1_multiplier = data->input1_multiplier;
+  op_params.input1_shift = data->input1_shift;
+  op_params.input2_offset = data->input2_offset;
+  op_params.input2_multiplier = data->input2_multiplier;
+  op_params.input2_shift = data->input2_shift;
+  op_params.output_offset = data->output_offset;
+  op_params.output_multiplier = data->output_multiplier;
+  op_params.output_shift = data->output_shift;
+  SetActivationParams(data->output_activation_min, data->output_activation_max,
+                      &op_params);
+  bool need_broadcast = reference_ops::ProcessBroadcastShapes(
+      tflite::micro::GetTensorShape(input1),
+      tflite::micro::GetTensorShape(input2), &op_params);
+
+  switch (output->type) {
+    case kTfLiteInt8: {
+      if (need_broadcast) {
+        reference_integer_ops::BroadcastAdd4DSlow(
+            op_params, tflite::micro::GetTensorShape(input1),
+            tflite::micro::GetTensorData<int8_t>(input1),
+            tflite::micro::GetTensorShape(input2),
+            tflite::micro::GetTensorData<int8_t>(input2),
+            tflite::micro::GetTensorShape(output),
+            tflite::micro::GetTensorData<int8_t>(output));
+      } else {
+        reference_integer_ops::Add(
+            op_params, tflite::micro::GetTensorShape(input1),
+            tflite::micro::GetTensorData<int8_t>(input1),
+            tflite::micro::GetTensorShape(input2),
+            tflite::micro::GetTensorData<int8_t>(input2),
+            tflite::micro::GetTensorShape(output),
+            tflite::micro::GetTensorData<int8_t>(output));
+      }
+      break;
+    }
+    case kTfLiteInt16: {
+      if (need_broadcast) {
+        reference_ops::BroadcastAdd4DSlow(
+            op_params, tflite::micro::GetTensorShape(input1),
+            tflite::micro::GetTensorData<int16_t>(input1),
+            tflite::micro::GetTensorShape(input2),
+            tflite::micro::GetTensorData<int16_t>(input2),
+            tflite::micro::GetTensorShape(output),
+            tflite::micro::GetTensorData<int16_t>(output));
+      } else {
+        reference_ops::Add(op_params, tflite::micro::GetTensorShape(input1),
+                           tflite::micro::GetTensorData<int16_t>(input1),
+                           tflite::micro::GetTensorShape(input2),
+                           tflite::micro::GetTensorData<int16_t>(input2),
+                           tflite::micro::GetTensorShape(output),
+                           tflite::micro::GetTensorData<int16_t>(output),
+                           false);
+      }
+      break;
+    }
+    default:
+      MicroPrintf("Type %s (%d) not supported.",
+                  TfLiteTypeGetName(output->type), output->type);
+      return kTfLiteError;
+  }
+
+  return kTfLiteOk;
+#else
+  MicroPrintf("Node configuration is not supported by ARC MLI Library.");
+  return kTfLiteError;
+#endif
+}
+
+TfLiteStatus EvalMLIAddInt8(TfLiteContext* context, TfLiteNode* node,
+                            TfLiteAddParams* params, const OpData* data,
+                            const TfLiteEvalTensor* input1,
+                            const TfLiteEvalTensor* input2,
+                            TfLiteEvalTensor* output) {
+#ifdef MLI_2_0
+  TF_LITE_ENSURE(context, data->is_mli_applicable == true);
+  TF_LITE_ENSURE(context, input1->type == kTfLiteInt8);
+  TF_LITE_ENSURE(context, input2->type == kTfLiteInt8);
+  TF_LITE_ENSURE(context, output->type == kTfLiteInt8);
+
+  ops::micro::MliTensorAttachBuffer<int8_t>(input1, &data->mli_input1);
+  ops::micro::MliTensorAttachBuffer<int8_t>(input2, &data->mli_input2);
+  ops::micro::MliTensorAttachBuffer<int8_t>(output, &data->mli_out);
+
+  // mli_mov config and tensors for data in fast (local) memory with interface
+  mli_mov_cfg_t copy_config;
+  mli_mov_cfg_for_copy(&copy_config);
+  mli_tensor input1_local_tsr = *data->mli_input1.MliTensor();
+  mli_tensor input2_local_tsr = *data->mli_input2.MliTensor();
+  mli_tensor out_local_tsr = *data->mli_out.MliTensor();
+  ops::micro::MliTensorInterface input1_local(&input1_local_tsr);
+  ops::micro::MliTensorInterface input2_local(&input2_local_tsr);
+  ops::micro::MliTensorInterface out_local(&out_local_tsr);
+
+  /* allocate the local buffers, and compute the slice size */
+  TF_LITE_ENSURE_STATUS(ops::micro::get_arc_scratch_buffer_for_eltwise_tensors(
+      context, &input1_local, &input2_local, &out_local));
+  TF_LITE_ENSURE(context, *input1_local.Rank() == 1 &&
+                              *input2_local.Rank() == 1 &&
+                              *out_local.Rank() == 1);
+  uint32_t min_capacity = *input1_local.DataCapacity();
+  min_capacity = std::min(min_capacity, *input2_local.DataCapacity());
+  min_capacity = std::min(min_capacity, *out_local.DataCapacity());
+  const int slice_dim = 0;
+  const int slice_size =
+      min_capacity / mli_hlp_tensor_element_size(out_local.MliTensor());
+
+  /* is_local indicates that the tensor is already in local memory,
+     so in that case the original tensor can be used,
+     and there is no need to copy it to the local tensor*/
+  const bool input1_is_local =
+      input1_local.Data<int8_t>() == data->mli_input1.Data<int8_t>();
+  const bool input2_is_local =
+      input2_local.Data<int8_t>() == data->mli_input2.Data<int8_t>();
+  const bool out_is_local =
+      out_local.Data<int8_t>() == data->mli_out.Data<int8_t>();
+
+  ops::micro::TensorSlicer input1_slice(data->mli_input1.MliTensor(), slice_dim,
+                                        slice_size);
+  ops::micro::TensorSlicer input2_slice(data->mli_input2.MliTensor(), slice_dim,
+                                        slice_size);
+  ops::micro::TensorSlicer out_slice(data->mli_out.MliTensor(), slice_dim,
+                                     slice_size);
+
+  mli_tensor* input1_tsr =
+      input1_is_local ? input1_slice.Sub() : input1_local.MliTensor();
+  mli_tensor* input2_tsr =
+      input2_is_local ? input2_slice.Sub() : input2_local.MliTensor();
+  mli_tensor* out_tsr = out_is_local ? out_slice.Sub() : out_local.MliTensor();
+
+  while (!out_slice.Done()) {
+    mli_mov_tensor_sync(input1_slice.Sub(), &copy_config, input1_tsr);
+    mli_mov_tensor_sync(input2_slice.Sub(), &copy_config, input2_tsr);
+
+    mli_krn_eltwise_add_sa8(input1_tsr, input2_tsr, out_tsr);
+
+    mli_mov_tensor_sync(out_tsr, &copy_config, out_slice.Sub());
+    input1_slice.Next();
+    input2_slice.Next();
+    out_slice.Next();
+  }
+  return kTfLiteOk;
+#else
+  return kTfLiteError;
+#endif
+}
+
+void* AddInit(TfLiteContext* context, const char* buffer, size_t length) {
+  TFLITE_DCHECK(context->AllocatePersistentBuffer != nullptr);
+  return context->AllocatePersistentBuffer(context, sizeof(OpData));
+}
+
+TfLiteStatus AddPrepare(TfLiteContext* context, TfLiteNode* node) {
+  TFLITE_DCHECK(node->user_data != nullptr);
+  TFLITE_DCHECK(node->builtin_data != nullptr);
+
+  MicroContext* micro_context = GetMicroContext(context);
+
+  TfLiteTensor* input1 =
+      micro_context->AllocateTempInputTensor(node, kInputTensor1);
+  TF_LITE_ENSURE(context, input1 != nullptr);
+  TfLiteTensor* input2 =
+      micro_context->AllocateTempInputTensor(node, kInputTensor2);
+  TF_LITE_ENSURE(context, input2 != nullptr);
+  TfLiteTensor* output = AllocateTempOutputTensor(node, kOutputTensor);
+  TF_LITE_ENSURE(context, output != nullptr);
+
+  OpData* data = static_cast<OpData*>(node->user_data);
+  auto* params = reinterpret_cast<TfLiteAddParams*>(node->builtin_data);
+
+  TF_LITE_ENSURE_STATUS(
+      CalculateOpData(context, params, input1, input2, output, data));
+
+  micro_context->DeallocateTempTfLiteTensor(input1);
+  micro_context->DeallocateTempTfLiteTensor(input2);
+  micro_context->DeallocateTempTfLiteTensor(output);
+
+  return kTfLiteOk;
+}
+
+TfLiteStatus AddEval(TfLiteContext* context, TfLiteNode* node) {
+  TfLiteStatus ret_val = kTfLiteOk;
+  auto* params = reinterpret_cast<TfLiteAddParams*>(node->builtin_data);
+
+  TFLITE_DCHECK(node->user_data != nullptr);
+  const OpData* data = static_cast<const OpData*>(node->user_data);
+
+  const TfLiteEvalTensor* input1 =
+      tflite::micro::GetEvalInput(context, node, kInputTensor1);
+  const TfLiteEvalTensor* input2 =
+      tflite::micro::GetEvalInput(context, node, kInputTensor2);
+  TfLiteEvalTensor* output =
+      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+  if (data->is_mli_applicable) {
+    ret_val =
+        EvalMLIAddInt8(context, node, params, data, input1, input2, output);
+  } else if (output->type == kTfLiteFloat32) {
+    ret_val = EvalAdd(context, node, params, data, input1, input2, output);
+  } else if (output->type == kTfLiteInt8 || output->type == kTfLiteInt16) {
+    ret_val =
+        EvalAddQuantized(context, node, params, data, input1, input2, output);
+  } else {
+    MicroPrintf("Type %s (%d) not supported.", TfLiteTypeGetName(output->type),
+                output->type);
+    ret_val = kTfLiteError;
+  }
+
+  return ret_val;
+}
+
+TFLMRegistration Register_ADD() {
+  return tflite::micro::RegisterOp(AddInit, AddPrepare, AddEval);
+}
+
+}  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/conv.cc b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/conv.cc
new file mode 100644
index 00000000..41d2c535
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/conv.cc
@@ -0,0 +1,711 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/kernels/internal/reference/conv.h"
+
+#include "mli_api.h"  // NOLINT
+#include "tensorflow/lite/c/builtin_op_data.h"
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/kernels/internal/common.h"
+#include "tensorflow/lite/kernels/internal/quantization_util.h"
+#include "tensorflow/lite/kernels/internal/reference/integer_ops/conv.h"
+#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
+#include "tensorflow/lite/kernels/kernel_util.h"
+#include "tensorflow/lite/kernels/padding.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/mli_function_specializations.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/mli_slicers.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/mli_tf_utils.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/scratch_buf_mgr.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.h"
+#include "tensorflow/lite/micro/kernels/kernel_util.h"
+#include "tensorflow/lite/micro/micro_log.h"
+
+namespace tflite {
+namespace {
+
+constexpr int kInputTensor = 0;
+constexpr int kFilterTensor = 1;
+constexpr int kBiasTensor = 2;
+constexpr int kOutputTensor = 0;
+
+// Conv is quantized along dimension 0:
+// https://www.tensorflow.org/lite/performance/quantization_spec
+#if defined(MLI_2_0) && !defined(MLI_2_0_KRNL_TEST)
+constexpr int kConvQuantizedDimension = 3;
+#else
+constexpr int kConvQuantizedDimension = 0;
+#endif
+
+// This file has 2 implementation of Conv.
+
+struct OpData {
+  TfLitePaddingValues padding;
+
+  // Cached tensor zero point values for quantized operations.
+  int32_t input_zero_point;
+  int32_t filter_zero_point;
+  int32_t output_zero_point;
+
+  // The scaling factor from input to output (aka the 'real multiplier') can
+  // be represented as a fixed point multiplier plus a left shift.
+  int32_t output_multiplier;
+  int output_shift;
+
+  // Per channel output multiplier and shift.
+  int32_t* per_channel_output_multiplier;
+  int32_t* per_channel_output_shift;
+#ifdef MLI_2_0
+  int8_t* per_channel_scale_frac_bits;
+#endif
+
+  // The range of the fused activation layer. For example for kNone and
+  // uint8_t these would be 0 and 255.
+  int32_t output_activation_min;
+  int32_t output_activation_max;
+
+  // The result of checking if MLI optimized version of tensors can be used.
+  bool is_mli_applicable;
+
+  // Tensors in MLI format.
+  mutable ops::micro::MliTensorInterface mli_in;
+  mutable ops::micro::MliTensorInterface mli_weights;
+  mutable ops::micro::MliTensorInterface mli_bias;
+  mutable ops::micro::MliTensorInterface mli_out;
+  mli_conv2d_cfg* cfg;
+
+  // Pointer to the mli convolution function.
+  conv_func_ptr p_mli_krn_conv2d_sa8_sa8_sa32;
+};
+
+#if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+inline PaddingType RuntimePaddingType(TfLitePadding padding) {
+  switch (padding) {
+    case TfLitePadding::kTfLitePaddingSame:
+      return PaddingType::kSame;
+    case TfLitePadding::kTfLitePaddingValid:
+      return PaddingType::kValid;
+    case TfLitePadding::kTfLitePaddingUnknown:
+    default:
+      return PaddingType::kNone;
+  }
+}
+#endif
+
+bool IsMliApplicable(TfLiteContext* context, const TfLiteTensor* input,
+                     const TfLiteTensor* filter, const TfLiteTensor* bias,
+                     const TfLiteConvParams* params) {
+  const auto* affine_quantization =
+      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);
+  // MLI optimized version only supports int8_t datatype, dilation factor of 1
+  // and per-axis quantization of weights (no broadcasting/per-tensor)
+  bool ret_val = (filter->type == kTfLiteInt8) &&
+                 (input->type == kTfLiteInt8) && (bias->type == kTfLiteInt32) &&
+                 (params->dilation_width_factor == 1) &&
+                 (params->dilation_height_factor == 1) &&
+                 (affine_quantization->scale->size ==
+                  filter->dims->data[kConvQuantizedDimension]);
+  return ret_val;
+}
+
+TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,
+                             const TfLiteConvParams* params, int width,
+                             int height, int filter_width, int filter_height,
+                             int out_width, int out_height,
+                             const TfLiteType data_type, OpData* data) {
+  bool has_bias = node->inputs->size == 3;
+  // Check number of inputs/outputs
+  TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
+  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);
+
+  // Matching GetWindowedOutputSize in TensorFlow.
+  auto padding = params->padding;
+  data->padding = ComputePaddingHeightWidth(
+      params->stride_height, params->stride_width,
+      params->dilation_height_factor, params->dilation_width_factor, height,
+      width, filter_height, filter_width, padding, &out_height, &out_width);
+  // Note that quantized inference requires that all tensors have their
+  // parameters set. This is usually done during quantized training.
+#if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+  MicroContext* micro_context = GetMicroContext(context);
+  TfLiteTensor* input =
+      micro_context->AllocateTempInputTensor(node, kInputTensor);
+  TfLiteTensor* filter =
+      micro_context->AllocateTempInputTensor(node, kFilterTensor);
+  TfLiteTensor* bias =
+      micro_context->AllocateTempInputTensor(context, node, kBiasTensor);
+  TfLiteTensor* output =
+      micro_context->AllocateTempOutputTensor(node, kOutputTensor);
+
+  if (data_type != kTfLiteFloat32 && !data->is_mli_applicable) {
+    int output_channels = filter->dims->data[kConvQuantizedDimension];
+
+    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
+        context, input, filter, bias, output, params->activation,
+        &data->output_multiplier, &data->output_shift,
+        &data->output_activation_min, &data->output_activation_max,
+        data->per_channel_output_multiplier,
+        reinterpret_cast<int*>(data->per_channel_output_shift),
+        output_channels));
+  }
+
+  micro_context->DeallocateTempTfLiteTensor(input);
+  micro_context->DeallocateTempTfLiteTensor(filter);
+  micro_context->DeallocateTempTfLiteTensor(bias);
+  micro_context->DeallocateTempTfLiteTensor(output);
+#endif
+  return kTfLiteOk;
+}
+void* Init(TfLiteContext* context, const char* buffer, size_t length) {
+  TFLITE_DCHECK(context->AllocatePersistentBuffer != nullptr);
+  return context->AllocatePersistentBuffer(context, sizeof(OpData));
+}
+
+TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
+  TFLITE_DCHECK(node->user_data != nullptr);
+  TFLITE_DCHECK(node->builtin_data != nullptr);
+
+  OpData* data = static_cast<OpData*>(node->user_data);
+  const auto params = static_cast<const TfLiteConvParams*>(node->builtin_data);
+
+  MicroContext* micro_context = GetMicroContext(context);
+
+  TfLiteTensor* output =
+      micro_context->AllocateTempOutputTensor(node, kOutputTensor);
+  TfLiteTensor* input =
+      micro_context->AllocateTempInputTensor(node, kInputTensor);
+  TfLiteTensor* filter =
+      micro_context->AllocateTempInputTensor(node, kFilterTensor);
+  TfLiteTensor* bias =
+      micro_context->AllocateTempInputTensor(context, node, kBiasTensor);
+
+  int input_width = input->dims->data[2];
+  int input_height = input->dims->data[1];
+#if defined(MLI_2_0) && !defined(MLI_2_0_KRNL_TEST)
+  int filter_width = filter->dims->data[1];
+  int filter_height = filter->dims->data[0];
+#else
+  int filter_width = filter->dims->data[2];
+  int filter_height = filter->dims->data[1];
+#endif
+  int output_width = output->dims->data[2];
+  int output_height = output->dims->data[1];
+
+  // Dynamically allocate per-channel quantization parameters.
+  const int num_channels = filter->dims->data[kConvQuantizedDimension];
+  data->per_channel_output_multiplier =
+      reinterpret_cast<int32_t*>(context->AllocatePersistentBuffer(
+          context, num_channels * sizeof(int32_t)));
+  data->per_channel_output_shift =
+      reinterpret_cast<int32_t*>(context->AllocatePersistentBuffer(
+          context, num_channels * sizeof(int32_t)));
+
+  data->is_mli_applicable =
+      IsMliApplicable(context, input, filter, bias, params);
+
+  // All per-channel quantized tensors need valid zero point and scale arrays.
+  if (input->type == kTfLiteInt8) {
+    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
+                      kTfLiteAffineQuantization);
+
+    const auto* affine_quantization =
+        static_cast<TfLiteAffineQuantization*>(filter->quantization.params);
+    TF_LITE_ENSURE(context, affine_quantization);
+    TF_LITE_ENSURE(context, affine_quantization->scale);
+    TF_LITE_ENSURE(context, affine_quantization->zero_point);
+
+    TF_LITE_ENSURE(context,
+                   affine_quantization->scale->size == 1 ||
+                       affine_quantization->scale->size ==
+                           filter->dims->data[kConvQuantizedDimension]);
+    TF_LITE_ENSURE_EQ(context, affine_quantization->scale->size,
+                      affine_quantization->zero_point->size);
+  }
+
+  TF_LITE_ENSURE_STATUS(CalculateOpData(
+      context, node, params, input_width, input_height, filter_width,
+      filter_height, output_width, output_height, input->type, data));
+
+  data->input_zero_point = input->params.zero_point;
+  data->filter_zero_point = filter->params.zero_point;
+  data->output_zero_point = output->params.zero_point;
+
+  if (data->is_mli_applicable) {
+    data->mli_in = ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+    data->mli_weights = ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+    data->mli_bias = ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+    data->mli_out = ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+    data->cfg = static_cast<mli_conv2d_cfg*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_conv2d_cfg)));
+
+#ifdef MLI_2_0
+    data->per_channel_scale_frac_bits =
+        static_cast<int8_t*>(context->AllocatePersistentBuffer(
+            context, 2 * num_channels * sizeof(int16_t)));
+#endif
+
+    // Reuse space allocated for OpData parameters.
+#ifdef MLI_2_0
+    *data->mli_weights.Scale<int16_t**>() =
+        reinterpret_cast<int16_t*>(data->per_channel_output_multiplier);
+    *data->mli_bias.Scale<int16_t**>() =
+        reinterpret_cast<int16_t*>(data->per_channel_output_multiplier) +
+        num_channels;
+#else
+    *data->mli_weights.Scale<int32_t**>() =
+        static_cast<int32_t*>(data->per_channel_output_multiplier);
+    *data->mli_bias.Scale<int32_t**>() =
+        static_cast<int32_t*>(data->per_channel_output_shift);
+#endif
+
+#ifdef MLI_2_0
+    *data->mli_weights.ZeroPoint<int16_t**>() =
+        reinterpret_cast<int16_t*>(data->per_channel_output_shift);
+    *data->mli_bias.ZeroPoint<int16_t**>() =
+        reinterpret_cast<int16_t*>(data->per_channel_output_shift) +
+        num_channels;
+#else
+    *data->mli_weights.ZeroPoint<int16_t**>() =
+        reinterpret_cast<int16_t*>(&data->filter_zero_point);
+    *data->mli_bias.ZeroPoint<int16_t**>() =
+        reinterpret_cast<int16_t*>(&data->filter_zero_point) + sizeof(int16_t);
+#endif
+
+#ifdef MLI_2_0
+    *data->mli_weights.ScaleFracBits<int8_t**>() =
+        reinterpret_cast<int8_t*>(data->per_channel_scale_frac_bits);
+    *data->mli_bias.ScaleFracBits<int8_t**>() =
+        reinterpret_cast<int8_t*>(data->per_channel_scale_frac_bits) +
+        num_channels;
+#endif
+
+    ops::micro::ConvertToMliTensor(input, &data->mli_in);
+    ops::micro::ConvertToMliTensorPerChannel(filter, &data->mli_weights,
+                                             /* is_bias_tensor = */ false);
+    ops::micro::ConvertToMliTensorPerChannel(bias, &data->mli_bias,
+                                             /* is_bias_tensor = */ true);
+#ifdef MLI_2_0
+    ops::micro::AdjustBiasTensor(&data->mli_bias, &data->mli_in,
+                                 &data->mli_weights);
+#endif
+    ops::micro::ConvertToMliTensor(output, &data->mli_out);
+
+#ifdef MLI_2_0
+    // Choose convolution mli specialized function.
+    data->p_mli_krn_conv2d_sa8_sa8_sa32 =
+        mli_krn_conv2d_hwcn(data->mli_weights.MliTensor());
+#else
+    data->p_mli_krn_conv2d_sa8_sa8_sa32 =
+        mli_krn_conv2d_hwcn(data->mli_weights.MliTensor(), data->cfg);
+#endif
+
+#ifdef MLI_2_0
+    data->cfg->dilation_width = 1;
+    data->cfg->dilation_height = 1;
+#endif
+
+    if (data->output_activation_min == -128 &&
+        data->output_activation_max == 127) {
+      data->cfg->relu.type = MLI_RELU_NONE;
+    } else if (params->activation == kTfLiteActRelu) {
+      data->cfg->relu.type = MLI_RELU_GEN;
+    } else if (params->activation == kTfLiteActRelu6) {
+      data->cfg->relu.type = MLI_RELU_6;
+    } else if (params->activation == kTfLiteActReluN1To1) {
+      data->cfg->relu.type = MLI_RELU_1;
+    } else {
+      data->cfg->relu.type = MLI_RELU_NONE;
+    }
+    data->cfg->stride_width = params->stride_width;
+    data->cfg->stride_height = params->stride_height;
+    if (params->padding == kTfLitePaddingValid) {
+      data->cfg->padding_left = 0;
+      data->cfg->padding_right = 0;
+      data->cfg->padding_top = 0;
+      data->cfg->padding_bottom = 0;
+    } else {
+      data->cfg->padding_left = data->padding.width;
+      data->cfg->padding_right =
+          data->padding.width + data->padding.width_offset;
+      data->cfg->padding_top = data->padding.height;
+      data->cfg->padding_bottom =
+          data->padding.height + data->padding.height_offset;
+    }
+  }
+
+  micro_context->DeallocateTempTfLiteTensor(output);
+  micro_context->DeallocateTempTfLiteTensor(input);
+  micro_context->DeallocateTempTfLiteTensor(filter);
+  micro_context->DeallocateTempTfLiteTensor(bias);
+  return kTfLiteOk;
+}
+
+TfLiteStatus EvalMliQuantizedPerChannel(
+    TfLiteContext* context, TfLiteNode* node, TfLiteConvParams* params,
+    const OpData& data, const TfLiteEvalTensor* input,
+    const TfLiteEvalTensor* filter, const TfLiteEvalTensor* bias,
+    TfLiteEvalTensor* output) {
+  // Run Conv MLI kernel
+  // MLI optimized version only supports int8_t dataype and dilation factor of 1
+  if (data.is_mli_applicable) {
+    // Copy configuration data from external to local memory
+    mli_conv2d_cfg cfg_local = *data.cfg;
+
+    ops::micro::MliTensorAttachBuffer<int8_t>(input, &data.mli_in);
+    ops::micro::MliTensorAttachBuffer<int8_t>(filter, &data.mli_weights);
+    ops::micro::MliTensorAttachBuffer<int32_t>(bias, &data.mli_bias);
+    ops::micro::MliTensorAttachBuffer<int8_t>(output, &data.mli_out);
+
+    // for height slicing
+    const int height_dimension = 1;
+    int in_slice_height = 0;
+    int out_slice_height = 0;
+    const int kernel_height =
+        static_cast<int>(data.mli_weights.Shape()[KRNL_H_DIM_HWC]);
+    const int overlap = kernel_height - cfg_local.stride_height;
+
+// for weight slicing (on output channels)
+#if defined(MLI_2_0) && !defined(MLI_2_0_KRNL_TEST)
+    // HWCN layout for weights, output channel dimension is the first dimension.
+    const int weight_out_ch_dimension = 3;
+#else
+    // NHWC layout for weights, output channel dimension is the first dimension.
+    const int weight_out_ch_dimension = 0;
+#endif
+    // bias has only 1 dimension
+    const int bias_out_ch_dimension = 0;
+    int slice_channels =
+        static_cast<int>(data.mli_weights.Shape()[weight_out_ch_dimension]);
+    // Batch-Height-Width-Channel layout means last dimension is output
+    // channels.
+    const int out_tensor_ch_dimension = 3;
+
+    // Tensors for data in fast (local) memory and config to copy data from
+    // external to local memory
+    mli_tensor weights_local = *data.mli_weights.MliTensor();
+    mli_tensor bias_local = *data.mli_bias.MliTensor();
+    mli_tensor in_local = *data.mli_in.MliTensor();
+    mli_tensor out_local = *data.mli_out.MliTensor();
+
+    ops::micro::MliTensorInterface weights_local_interface(&weights_local);
+    ops::micro::MliTensorInterface bias_local_interface(&bias_local);
+    ops::micro::MliTensorInterface in_local_interface(&in_local);
+    ops::micro::MliTensorInterface out_local_interface(&out_local);
+
+    mli_mov_cfg_t copy_config;
+    mli_mov_cfg_for_copy(&copy_config);
+
+    TF_LITE_ENSURE_STATUS(ops::micro::get_arc_scratch_buffer_for_conv_tensors(
+        context, &in_local_interface, &weights_local_interface,
+        &bias_local_interface, &out_local_interface));
+    TF_LITE_ENSURE_STATUS(ops::micro::arc_scratch_buffer_calc_slice_size_io(
+        &in_local_interface, &out_local_interface, kernel_height,
+        cfg_local.stride_height, cfg_local.padding_top,
+        cfg_local.padding_bottom, &in_slice_height, &out_slice_height));
+    TF_LITE_ENSURE_STATUS(
+        ops::micro::arc_scratch_buffer_calc_slice_size_weights(
+            &weights_local_interface, &bias_local_interface,
+            weight_out_ch_dimension, &slice_channels));
+
+    /* is_local indicates that the tensor is already in local memory,
+       so in that case the original tensor can be used,
+       and there is no need to copy it to the local tensor*/
+    const bool in_is_local =
+        in_local_interface.Data<int8_t>() == data.mli_in.Data<int8_t>();
+    const bool out_is_local =
+        out_local_interface.Data<int8_t>() == data.mli_out.Data<int8_t>();
+    const bool b_is_local =
+        bias_local_interface.Data<int32_t>() == data.mli_bias.Data<int32_t>();
+#ifndef MLI_2_0_KRNL_TEST
+    const bool w_is_local = weights_local_interface.Data<int8_t>() ==
+                            data.mli_weights.Data<int8_t>();
+#endif
+
+#if defined(MLI_2_0) && !defined(MLI_2_0_KRNL_TEST)
+    ops::micro::TensorSlicer w_slice(data.mli_weights.MliTensor(),
+                                     weight_out_ch_dimension, slice_channels, 0,
+                                     0, 0, true);
+#else
+    ops::micro::TensorSlicer w_slice(data.mli_weights.MliTensor(),
+                                     weight_out_ch_dimension, slice_channels);
+#endif
+    ops::micro::TensorSlicer b_slice(data.mli_bias.MliTensor(),
+                                     bias_out_ch_dimension, slice_channels);
+    ops::micro::TensorSlicer out_ch_slice(data.mli_out.MliTensor(),
+                                          out_tensor_ch_dimension,
+                                          slice_channels, 0, 0, 0, true);
+
+#ifdef MLI_2_0_KRNL_TEST
+    mli_tensor* w_ptr = &weights_local;
+#else
+    mli_tensor* w_ptr = w_is_local ? w_slice.Sub() : &weights_local;
+#endif
+    mli_tensor* b_ptr = b_is_local ? b_slice.Sub() : &bias_local;
+
+    void* input_buffer_ptr = NULL;
+    uint32_t input_buffer_size = 0;
+
+    while (!w_slice.Done()) {
+#ifndef MLI_2_0_KRNL_TEST
+      mli_mov_tensor_sync(w_slice.Sub(), &copy_config, w_ptr);
+#endif
+      mli_mov_tensor_sync(b_slice.Sub(), &copy_config, b_ptr);
+
+      /* mli_in tensor contains batches of HWC tensors. so it is a 4 dimensional
+      tensor. because the mli kernel will process one HWC tensor at a time, the
+      4 dimensional tensor needs to be sliced into nBatch 3 dimensional tensors.
+      on top of that there could be a need to also slice in the Height
+      dimension. for that the sliceHeight has been calculated. The tensor slicer
+      is configured that it will completely slice the nBatch dimension (0) and
+      slice the height dimension (1) in chunks of 'sliceHeight' */
+      ops::micro::TensorSlicer in_slice(
+          data.mli_in.MliTensor(), height_dimension, in_slice_height,
+          cfg_local.padding_top, cfg_local.padding_bottom, overlap);
+
+      /* output tensor is already sliced in the output channel dimension.
+      out_ch_slice.Sub() is the tensor for the amount of output channels of this
+      iteration of the weight slice loop. This tensor needs to be further
+      sliced over the batch and height dimension. */
+      ops::micro::TensorSlicer out_slice(out_ch_slice.Sub(), height_dimension,
+                                         out_slice_height);
+
+      /* setup the pointers to the local or remote tensor to make the code
+       * inside the loop easier. */
+      mli_tensor* in_ptr = in_is_local ? in_slice.Sub() : &in_local;
+      mli_tensor* out_ptr = out_is_local ? out_slice.Sub() : &out_local;
+
+#ifdef MLI_2_0_KRNL_TEST
+      /* Permute weights tensor to the HWCN layout */
+      // Checking conditions here to prevent usage non-contiguous buffer memory.
+      if (data.mli_out.Shape()[out_tensor_ch_dimension] !=
+              out_slice.Sub()->shape[FMAP_C_DIM_HWC] ||
+          data.mli_out.Shape()[height_dimension] !=
+              out_slice.Sub()->shape[FMAP_H_DIM_HWC]) {
+        MicroPrintf("Slicing is not supported with real-time permutation.");
+        return kTfLiteError;
+      }
+      mli_permute_cfg permute_cfg = {{1, 2, 3, 0}};
+      ops::micro::permute_weights(data.mli_weights.MliTensor(), &permute_cfg,
+                                  w_ptr, &out_ptr->data);
+#endif
+
+      while (!out_slice.Done()) {
+        if (!out_is_local) {
+          ops::micro::PrepareLocalTensor(out_slice.Sub(), &out_local);
+          ops::micro::PrepareLocalTensor(in_slice.Sub(), &in_local);
+        }
+
+        TF_LITE_ENSURE(context, !in_slice.Done());
+        cfg_local.padding_top = in_slice.GetPaddingPre();
+        cfg_local.padding_bottom = in_slice.GetPaddingPost();
+
+        // if same input copy as previous iteration, skip the copy of input
+#ifdef MLI_2_0
+        if ((in_slice.Sub()->data.mem.pi8 != input_buffer_ptr) ||
+            (mli_hlp_count_elem_num(in_slice.Sub(), 0) != input_buffer_size)) {
+          mli_mov_tensor_sync(in_slice.Sub(), &copy_config, in_ptr);
+          input_buffer_ptr = in_slice.Sub()->data.mem.pi8;
+          input_buffer_size = mli_hlp_count_elem_num(in_slice.Sub(), 0);
+        }
+
+        data.p_mli_krn_conv2d_sa8_sa8_sa32(in_ptr, w_ptr, b_ptr, &cfg_local,
+                                           out_ptr);
+#else
+        if ((in_slice.Sub()->data != input_buffer_ptr) ||
+            (mli_hlp_count_elem_num(in_slice.Sub(), 0) != input_buffer_size)) {
+          mli_mov_tensor_sync(in_slice.Sub(), &copy_config, in_ptr);
+          input_buffer_ptr = in_slice.Sub()->data;
+          input_buffer_size = mli_hlp_count_elem_num(in_slice.Sub(), 0);
+        }
+        data.p_mli_krn_conv2d_sa8_sa8_sa32(in_ptr, w_ptr, b_ptr, &cfg_local,
+                                           out_ptr);
+#endif
+        mli_mov_tensor_sync(out_ptr, &copy_config, out_slice.Sub());
+
+        in_slice.Next();
+        out_slice.Next();
+      }
+      w_slice.Next();
+      b_slice.Next();
+      out_ch_slice.Next();
+      TF_LITE_ENSURE(context, in_slice.Done());
+    }
+  }
+  return kTfLiteOk;
+}
+
+void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
+                             TfLiteConvParams* params, const OpData& data,
+                             const TfLiteEvalTensor* input,
+                             const TfLiteEvalTensor* filter,
+                             const TfLiteEvalTensor* bias,
+                             TfLiteEvalTensor* output,
+                             TfLiteEvalTensor* im2col) {
+#if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+  ConvParams op_params;
+  op_params.input_offset = -data.input_zero_point;
+  op_params.output_offset = data.output_zero_point;
+  op_params.stride_height = params->stride_height;
+  op_params.stride_width = params->stride_width;
+  op_params.dilation_height_factor = params->dilation_height_factor;
+  op_params.dilation_width_factor = params->dilation_width_factor;
+  op_params.padding_values.height = data.padding.height;
+  op_params.padding_values.width = data.padding.width;
+  op_params.quantized_activation_min = data.output_activation_min;
+  op_params.quantized_activation_max = data.output_activation_max;
+
+  reference_integer_ops::ConvPerChannel(
+      op_params, data.per_channel_output_multiplier,
+      data.per_channel_output_shift, tflite::micro::GetTensorShape(input),
+      tflite::micro::GetTensorData<int8_t>(input),
+      tflite::micro::GetTensorShape(filter),
+      tflite::micro::GetTensorData<int8_t>(filter),
+      tflite::micro::GetTensorShape(bias),
+      tflite::micro::GetTensorData<int32_t>(bias),
+      tflite::micro::GetTensorShape(output),
+      tflite::micro::GetTensorData<int8_t>(output));
+#else
+  MicroPrintf("Node configuration is not supported by ARC MLI Library.");
+#endif
+}
+
+void EvalQuantizedPerChannelInt16(TfLiteContext* context, TfLiteNode* node,
+                                  TfLiteConvParams* params, const OpData& data,
+                                  const TfLiteEvalTensor* input,
+                                  const TfLiteEvalTensor* filter,
+                                  const TfLiteEvalTensor* bias,
+                                  TfLiteEvalTensor* output) {
+#if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+  ConvParams op_params;
+  op_params.input_offset = -data.input_zero_point;
+  op_params.output_offset = data.output_zero_point;
+  op_params.stride_height = params->stride_height;
+  op_params.stride_width = params->stride_width;
+  op_params.dilation_height_factor = params->dilation_height_factor;
+  op_params.dilation_width_factor = params->dilation_width_factor;
+  op_params.padding_values.height = data.padding.height;
+  op_params.padding_values.width = data.padding.width;
+  op_params.quantized_activation_min = data.output_activation_min;
+  op_params.quantized_activation_max = data.output_activation_max;
+
+  reference_integer_ops::ConvPerChannel(
+      op_params, data.per_channel_output_multiplier,
+      data.per_channel_output_shift, tflite::micro::GetTensorShape(input),
+      tflite::micro::GetTensorData<int16_t>(input),
+      tflite::micro::GetTensorShape(filter),
+      tflite::micro::GetTensorData<int8_t>(filter),
+      tflite::micro::GetTensorShape(bias),
+      tflite::micro::GetTensorData<std::int64_t>(bias),
+      tflite::micro::GetTensorShape(output),
+      tflite::micro::GetTensorData<int16_t>(output));
+#else
+  MicroPrintf("Node configuration is not supported by ARC MLI Library.");
+#endif
+}
+
+void EvalFloat(TfLiteContext* context, TfLiteNode* node,
+               TfLiteConvParams* params, const OpData& data,
+               const TfLiteEvalTensor* input, const TfLiteEvalTensor* filter,
+               const TfLiteEvalTensor* bias, TfLiteEvalTensor* im2col,
+               TfLiteEvalTensor* hwcn_weights, TfLiteEvalTensor* output) {
+#if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+  float output_activation_min, output_activation_max;
+  CalculateActivationRange(params->activation, &output_activation_min,
+                           &output_activation_max);
+  ConvParams op_params;
+  op_params.padding_type = RuntimePaddingType(params->padding);
+  op_params.padding_values.width = data.padding.width;
+  op_params.padding_values.height = data.padding.height;
+  op_params.stride_width = params->stride_width;
+  op_params.stride_height = params->stride_height;
+  op_params.dilation_width_factor = params->dilation_width_factor;
+  op_params.dilation_height_factor = params->dilation_height_factor;
+  op_params.float_activation_min = output_activation_min;
+  op_params.float_activation_max = output_activation_max;
+
+  reference_ops::Conv(op_params, tflite::micro::GetTensorShape(input),
+                      tflite::micro::GetTensorData<float>(input),
+                      tflite::micro::GetTensorShape(filter),
+                      tflite::micro::GetTensorData<float>(filter),
+                      tflite::micro::GetTensorShape(bias),
+                      tflite::micro::GetTensorData<float>(bias),
+                      tflite::micro::GetTensorShape(output),
+                      tflite::micro::GetTensorData<float>(output),
+                      tflite::micro::GetTensorShape(im2col),
+                      tflite::micro::GetTensorData<float>(im2col));
+#else
+  MicroPrintf("Type %s (%d) is not supported by ARC MLI Library.",
+              TfLiteTypeGetName(input->type), input->type);
+#endif
+}
+
+TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
+  auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
+
+  TfLiteEvalTensor* output =
+      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+  const TfLiteEvalTensor* input =
+      tflite::micro::GetEvalInput(context, node, kInputTensor);
+  const TfLiteEvalTensor* filter =
+      tflite::micro::GetEvalInput(context, node, kFilterTensor);
+  const TfLiteEvalTensor* bias =
+      tflite::micro::GetEvalInput(context, node, kBiasTensor);
+
+  TFLITE_DCHECK(node->user_data != nullptr);
+  const OpData& data = *(static_cast<const OpData*>(node->user_data));
+
+  TF_LITE_ENSURE_EQ(context, input->type, output->type);
+  TF_LITE_ENSURE_MSG(
+      context,
+      input->type == filter->type ||
+          (input->type == kTfLiteInt16 && filter->type == kTfLiteInt8),
+      "Hybrid models are not supported on TFLite Micro.");
+
+  switch (input->type) {  // Already know in/out types are same.
+    case kTfLiteFloat32:
+      EvalFloat(context, node, params, data, input, filter, bias, nullptr,
+                nullptr, output);
+      break;
+    case kTfLiteInt8:
+      if (data.is_mli_applicable) {
+        EvalMliQuantizedPerChannel(context, node, params, data, input, filter,
+                                   bias, output);
+      } else {
+        EvalQuantizedPerChannel(context, node, params, data, input, filter,
+                                bias, output, nullptr);
+      }
+      break;
+    case kTfLiteInt16:
+      EvalQuantizedPerChannelInt16(context, node, params, data, input, filter,
+                                   bias, output);
+      break;
+    default:
+      MicroPrintf("Type %s (%d) not supported.", TfLiteTypeGetName(input->type),
+                  input->type);
+      return kTfLiteError;
+  }
+  return kTfLiteOk;
+}
+
+}  // namespace
+
+TFLMRegistration Register_CONV_2D() {
+  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+}
+
+}  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/depthwise_conv.cc b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/depthwise_conv.cc
new file mode 100644
index 00000000..88188a35
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/depthwise_conv.cc
@@ -0,0 +1,706 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file 
+except in compliance with the License. You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/kernels/internal/reference/integer_ops/depthwise_conv.h"
+
+#include "mli_api.h"  // NOLINT
+#include "tensorflow/lite/c/builtin_op_data.h"
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/kernels/internal/common.h"
+#include "tensorflow/lite/kernels/internal/quantization_util.h"
+#include "tensorflow/lite/kernels/internal/reference/depthwiseconv_float.h"
+#include "tensorflow/lite/kernels/internal/reference/depthwiseconv_uint8.h"
+#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
+#include "tensorflow/lite/kernels/kernel_util.h"
+#include "tensorflow/lite/kernels/padding.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/mli_function_specializations.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/mli_slicers.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/mli_tf_utils.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/scratch_buf_mgr.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.h"
+#include "tensorflow/lite/micro/kernels/kernel_util.h"
+#include "tensorflow/lite/micro/micro_log.h"
+
+namespace tflite {
+namespace {
+
+constexpr int kInputTensor = 0;
+constexpr int kFilterTensor = 1;
+constexpr int kBiasTensor = 2;
+constexpr int kOutputTensor = 0;
+
+// Depthwise conv is quantized along dimension 3:
+// https://www.tensorflow.org/lite/performance/quantization_spec
+constexpr int kDepthwiseConvQuantizedDimension = 3;
+
+struct OpData {
+  TfLitePaddingValues padding;
+
+  // Cached tensor zero point values for quantized operations.
+  int32_t input_zero_point;
+  int32_t filter_zero_point;
+  int32_t output_zero_point;
+
+  // The scaling factor from input to output (aka the 'real multiplier') can
+  // be represented as a fixed point multiplier plus a left shift.
+  int32_t output_multiplier;
+  int output_shift;
+
+  // Per channel output multiplier and shift.
+  int32_t* per_channel_output_multiplier;
+  int32_t* per_channel_output_shift;
+#ifdef MLI_2_0
+  int8_t* per_channel_scale_frac_bits;
+#endif
+
+  // The range of the fused activation layer. For example for kNone and
+  // uint8_t these would be 0 and 255.
+  int32_t output_activation_min;
+  int32_t output_activation_max;
+
+  // The result of checking if MLI optimized version of tensors can be used.
+  bool is_mli_applicable;
+
+  // Tensors in MLI format.
+  mutable ops::micro::MliTensorInterface mli_in;
+  mutable ops::micro::MliTensorInterface mli_weights;
+  mutable ops::micro::MliTensorInterface mli_bias;
+  mutable ops::micro::MliTensorInterface mli_out;
+  mli_conv2d_cfg* cfg;
+
+  // Pointer to the required depthwise function. For Ã¢â‚¬Å“channel multiplierÃ¢â‚¬Â
+  // functionality group convolution is used.
+  depthwise_func_ptr p_mli_krn_depthwise_conv2d_sa8_sa8_sa32;
+};
+
+bool IsMliApplicable(TfLiteContext* context, const TfLiteTensor* input,
+                     const TfLiteTensor* filter, const TfLiteTensor* bias,
+                     const TfLiteDepthwiseConvParams* params) {
+  const auto* affine_quantization =
+      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);
+
+#ifndef MLI_2_0
+  const int in_ch = SizeOfDimension(input, 3);
+  const int filters_num = SizeOfDimension(filter, 3);
+#endif
+
+  // MLI optimized version only supports int8_t datatype, dilation factor of 1
+  // and per-axis quantization of weights (no broadcasting/per-tensor). For
+  // MLI 1.1 (in_ch == filters_num) || (in_ch == 1)) is used to prevent usage of
+  // channel multiplier logic for multichannel input.
+
+  bool ret_val = (filter->type == kTfLiteInt8) &&
+                 (input->type == kTfLiteInt8) && (bias->type == kTfLiteInt32) &&
+                 (params->dilation_width_factor == 1) &&
+                 (params->dilation_height_factor == 1) &&
+                 (affine_quantization->scale->size ==
+#ifdef MLI_2_0
+                  filter->dims->data[kDepthwiseConvQuantizedDimension]);
+#else
+                  filter->dims->data[kDepthwiseConvQuantizedDimension]) &&
+                 ((in_ch == filters_num) || (in_ch == 1));
+#endif
+  return ret_val;
+}
+
+TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,
+                             TfLiteDepthwiseConvParams* params, int width,
+                             int height, int filter_width, int filter_height,
+                             const TfLiteType data_type, OpData* data) {
+  bool has_bias = node->inputs->size == 3;
+  // Check number of inputs/outputs
+  TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
+  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);
+
+  int unused_output_height, unused_output_width;
+  data->padding = ComputePaddingHeightWidth(
+      params->stride_height, params->stride_width, 1, 1, height, width,
+      filter_height, filter_width, params->padding, &unused_output_height,
+      &unused_output_width);
+
+  // Note that quantized inference requires that all tensors have their
+  // parameters set. This is usually done during quantized training.
+#if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+  MicroContext* micro_context = GetMicroContext(context);
+
+  TfLiteTensor* input =
+      micro_context->AllocateTempInputTensor(node, kInputTensor);
+  TfLiteTensor* filter =
+      micro_context->AllocateTempInputTensor(node, kFilterTensor);
+  TfLiteTensor* bias =
+      micro_context->AllocateTempInputTensor(context, node, kBiasTensor);
+  TfLiteTensor* output =
+      micro_context->AllocateTempOutputTensor(node, kOutputTensor);
+
+  if (data_type != kTfLiteFloat32 && !data->is_mli_applicable) {
+    int num_channels = filter->dims->data[kDepthwiseConvQuantizedDimension];
+
+    return tflite::PopulateConvolutionQuantizationParams(
+        context, input, filter, bias, output, params->activation,
+        &data->output_multiplier, &data->output_shift,
+        &data->output_activation_min, &data->output_activation_max,
+        data->per_channel_output_multiplier,
+        reinterpret_cast<int*>(data->per_channel_output_shift), num_channels);
+  }
+  micro_context->DeallocateTempTfLiteTensor(input);
+  micro_context->DeallocateTempTfLiteTensor(filter);
+  micro_context->DeallocateTempTfLiteTensor(bias);
+  micro_context->DeallocateTempTfLiteTensor(output);
+
+#endif
+  return kTfLiteOk;
+}
+
+void* Init(TfLiteContext* context, const char* buffer, size_t length) {
+  TFLITE_DCHECK(context->AllocatePersistentBuffer != nullptr);
+  return context->AllocatePersistentBuffer(context, sizeof(OpData));
+}
+
+TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
+  TFLITE_DCHECK(node->user_data != nullptr);
+  TFLITE_DCHECK(node->builtin_data != nullptr);
+
+  auto* params =
+      reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);
+  OpData* data = static_cast<OpData*>(node->user_data);
+
+  TfLiteTensor* output = AllocateTempOutputTensor(node, kOutputTensor);
+  const TfLiteTensor* input = AllocateTempInputTensor(node, kInputTensor);
+  const TfLiteTensor* filter = AllocateTempInputTensor(node, kFilterTensor);
+  const TfLiteTensor* bias =
+      AllocateTempInputTensor(context, node, kBiasTensor);
+
+  const TfLiteType data_type = input->type;
+  int width = SizeOfDimension(input, 2);
+  int height = SizeOfDimension(input, 1);
+
+#if defined(MLI_2_0) && !defined(MLI_2_0_KRNL_TEST)
+  int filter_width = SizeOfDimension(filter, 1);
+  int filter_height = SizeOfDimension(filter, 0);
+#else
+  int filter_width = SizeOfDimension(filter, 2);
+  int filter_height = SizeOfDimension(filter, 1);
+#endif
+
+  // Per channel quantization is only needed for int8 inference. For other
+  // quantized types, only a single scale and zero point is needed.
+  const int num_channels = filter->dims->data[kDepthwiseConvQuantizedDimension];
+  // Dynamically allocate per-channel quantization parameters.
+  data->per_channel_output_multiplier =
+      reinterpret_cast<int32_t*>(context->AllocatePersistentBuffer(
+          context, num_channels * sizeof(int32_t)));
+  data->per_channel_output_shift =
+      reinterpret_cast<int32_t*>(context->AllocatePersistentBuffer(
+          context, num_channels * sizeof(int32_t)));
+
+  data->is_mli_applicable =
+      IsMliApplicable(context, input, filter, bias, params);
+
+  // All per-channel quantized tensors need valid zero point and scale arrays.
+  if (input->type == kTfLiteInt8) {
+    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
+                      kTfLiteAffineQuantization);
+
+    const auto* affine_quantization =
+        reinterpret_cast<TfLiteAffineQuantization*>(
+            filter->quantization.params);
+    TF_LITE_ENSURE(context, affine_quantization);
+    TF_LITE_ENSURE(context, affine_quantization->scale);
+    TF_LITE_ENSURE(context, affine_quantization->zero_point);
+    TF_LITE_ENSURE(
+        context, affine_quantization->scale->size == 1 ||
+                     affine_quantization->scale->size ==
+                         filter->dims->data[kDepthwiseConvQuantizedDimension]);
+    TF_LITE_ENSURE_EQ(context, affine_quantization->scale->size,
+                      affine_quantization->zero_point->size);
+  }
+
+  TF_LITE_ENSURE_STATUS(CalculateOpData(context, node, params, width, height,
+                                        filter_width, filter_height, data_type,
+                                        data));
+
+  data->input_zero_point = input->params.zero_point;
+  data->filter_zero_point = filter->params.zero_point;
+  data->output_zero_point = output->params.zero_point;
+
+  if (data->is_mli_applicable) {
+    data->mli_in = ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+    data->mli_weights = ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+    data->mli_bias = ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+    data->mli_out = ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+    data->cfg = static_cast<mli_conv2d_cfg*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_conv2d_cfg)));
+
+#ifdef MLI_2_0
+    const int num_buffers = 2;
+    data->per_channel_scale_frac_bits =
+        static_cast<int8_t*>(context->AllocatePersistentBuffer(
+            context, num_buffers * num_channels * sizeof(int16_t)));
+#endif
+
+    // Reuse space allocated for OpData parameters.
+#ifdef MLI_2_0
+    *data->mli_weights.Scale<int16_t**>() =
+        reinterpret_cast<int16_t*>(data->per_channel_output_multiplier);
+    *data->mli_bias.Scale<int16_t**>() =
+        reinterpret_cast<int16_t*>(data->per_channel_output_multiplier) +
+        num_channels;
+#else
+    *data->mli_weights.Scale<int32_t**>() =
+        static_cast<int32_t*>(data->per_channel_output_multiplier);
+    *data->mli_bias.Scale<int32_t**>() =
+        static_cast<int32_t*>(data->per_channel_output_shift);
+#endif
+
+#ifdef MLI_2_0
+    *data->mli_weights.ZeroPoint<int16_t**>() =
+        reinterpret_cast<int16_t*>(data->per_channel_output_shift);
+    *data->mli_bias.ZeroPoint<int16_t**>() =
+        reinterpret_cast<int16_t*>(data->per_channel_output_shift) +
+        num_channels;
+#else
+    *data->mli_weights.ZeroPoint<int16_t**>() =
+        reinterpret_cast<int16_t*>(&data->filter_zero_point);
+    *data->mli_bias.ZeroPoint<int16_t**>() =
+        reinterpret_cast<int16_t*>(&data->filter_zero_point) + sizeof(int16_t);
+#endif
+
+#ifdef MLI_2_0
+    *data->mli_weights.ScaleFracBits<int8_t**>() =
+        reinterpret_cast<int8_t*>(data->per_channel_scale_frac_bits);
+    *data->mli_bias.ScaleFracBits<int8_t**>() =
+        reinterpret_cast<int8_t*>(data->per_channel_scale_frac_bits) +
+        num_channels;
+#endif
+
+    ops::micro::ConvertToMliTensor(input, &data->mli_in);
+    ops::micro::ConvertToMliTensorPerChannel(filter, &data->mli_weights,
+                                             /* is_bias_tensor = */ false);
+    ops::micro::ConvertToMliTensorPerChannel(bias, &data->mli_bias,
+                                             /* is_bias_tensor = */ true);
+#ifdef MLI_2_0
+    ops::micro::AdjustBiasTensor(&data->mli_bias, &data->mli_in,
+                                 &data->mli_weights);
+#endif
+    ops::micro::ConvertToMliTensor(output, &data->mli_out);
+
+#ifdef MLI_2_0
+    // Choose group convolution function for "channel multiplier" functionality.
+    const int in_ch = SizeOfDimension(input, 3);
+    const int filters_num = SizeOfDimension(filter, 3);
+    const int channels_num = SizeOfDimension(filter, 2);
+    if (in_ch == filters_num && channels_num == 1) {
+      data->p_mli_krn_depthwise_conv2d_sa8_sa8_sa32 =
+          mli_krn_depthwise_conv2d(data->mli_weights.MliTensor());
+    } else {
+      data->p_mli_krn_depthwise_conv2d_sa8_sa8_sa32 =
+          mli_krn_group_conv2d(data->mli_weights.MliTensor());
+    }
+#else
+    data->p_mli_krn_depthwise_conv2d_sa8_sa8_sa32 =
+        mli_krn_depthwise_conv2d(data->mli_weights.MliTensor(), data->cfg);
+#endif
+
+#ifdef MLI_2_0
+    data->cfg->dilation_width = 1;
+    data->cfg->dilation_height = 1;
+#endif
+
+    if (data->output_activation_min == -128 &&
+        data->output_activation_max == 127) {
+      data->cfg->relu.type = MLI_RELU_NONE;
+    } else if (params->activation == kTfLiteActRelu) {
+      data->cfg->relu.type = MLI_RELU_GEN;
+    } else if (params->activation == kTfLiteActRelu6) {
+      data->cfg->relu.type = MLI_RELU_6;
+    } else if (params->activation == kTfLiteActReluN1To1) {
+      data->cfg->relu.type = MLI_RELU_1;
+    } else {
+      data->cfg->relu.type = MLI_RELU_NONE;
+    }
+
+    data->cfg->stride_width = params->stride_width;
+    data->cfg->stride_height = params->stride_height;
+    if (params->padding == kTfLitePaddingValid) {
+      data->cfg->padding_left = 0;
+      data->cfg->padding_right = 0;
+      data->cfg->padding_top = 0;
+      data->cfg->padding_bottom = 0;
+    } else {
+      data->cfg->padding_left = data->padding.width;
+      data->cfg->padding_right =
+          data->padding.width + data->padding.width_offset;
+      data->cfg->padding_top = data->padding.height;
+      data->cfg->padding_bottom =
+          data->padding.height + data->padding.height_offset;
+    }
+  }
+  return kTfLiteOk;
+}
+
+void EvalFloat(TfLiteContext* context, TfLiteNode* node,
+               TfLiteDepthwiseConvParams* params, const OpData& data,
+               const TfLiteEvalTensor* input, const TfLiteEvalTensor* filter,
+               const TfLiteEvalTensor* bias, TfLiteEvalTensor* output) {
+#if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+  float output_activation_min, output_activation_max;
+  CalculateActivationRange(params->activation, &output_activation_min,
+                           &output_activation_max);
+
+  tflite::DepthwiseParams op_params;
+  // Padding type is ignored, but still set.
+  op_params.padding_type = PaddingType::kSame;
+  op_params.padding_values.width = data.padding.width;
+  op_params.padding_values.height = data.padding.height;
+  op_params.stride_width = params->stride_width;
+  op_params.stride_height = params->stride_height;
+  op_params.dilation_width_factor = params->dilation_width_factor;
+  op_params.dilation_height_factor = params->dilation_height_factor;
+  op_params.depth_multiplier = params->depth_multiplier;
+  op_params.float_activation_min = output_activation_min;
+  op_params.float_activation_max = output_activation_max;
+
+  tflite::reference_ops::DepthwiseConv(
+      op_params, tflite::micro::GetTensorShape(input),
+      tflite::micro::GetTensorData<float>(input),
+      tflite::micro::GetTensorShape(filter),
+      tflite::micro::GetTensorData<float>(filter),
+      tflite::micro::GetTensorShape(bias),
+      tflite::micro::GetTensorData<float>(bias),
+      tflite::micro::GetTensorShape(output),
+      tflite::micro::GetTensorData<float>(output));
+#else
+  MicroPrintf("Type %s (%d) is not supported by ARC MLI Library.",
+              TfLiteTypeGetName(input->type), input->type);
+#endif
+}
+TfLiteStatus EvalMliQuantizedPerChannel(
+    TfLiteContext* context, TfLiteNode* node, TfLiteDepthwiseConvParams* params,
+    const OpData& data, const TfLiteEvalTensor* input,
+    const TfLiteEvalTensor* filter, const TfLiteEvalTensor* bias,
+    TfLiteEvalTensor* output) {
+  // Run Depthwise Conv MLI kernel
+  // MLI optimized version only supports int8_t dataype and dilation factor of 1
+  if (data.is_mli_applicable) {
+    // Copy configuration data from external to local memory
+    mli_conv2d_cfg cfg_local = *data.cfg;
+
+    ops::micro::MliTensorAttachBuffer<int8_t>(input, &data.mli_in);
+    ops::micro::MliTensorAttachBuffer<int8_t>(filter, &data.mli_weights);
+    ops::micro::MliTensorAttachBuffer<int32_t>(bias, &data.mli_bias);
+    ops::micro::MliTensorAttachBuffer<int8_t>(output, &data.mli_out);
+
+    // for height slicing
+    const int height_dimension = 1;
+    int in_slice_height = 0;
+    int out_slice_height = 0;
+    uint32_t* mli_weights_shape = data.mli_weights.Shape();
+#ifdef MLI_2_0
+    const int kernel_height =
+        static_cast<int>(mli_weights_shape[KRNL_DW_H_DIM_HW1N]);
+#else
+    const int kernel_height =
+        static_cast<int>(mli_weights_shape[KRNL_DW_H_DIM_HWC]);
+#endif
+    const int overlap = kernel_height - cfg_local.stride_height;
+
+    // for weight slicing (on output channels)
+    // HWCN layout for weights, output channel dimension is the first dimension.
+    const int weight_out_ch_dimension = 3;
+    // bias has only 1 dimension
+    const int bias_out_ch_dimension = 0;
+    // Batch-Height-Width-Channel layout means last dimension is output
+    // channels.
+    const int out_tensor_ch_dimension = 3;
+    const int32_t in_channels = data.mli_in.Shape()[out_tensor_ch_dimension];
+    const int32_t out_channels = data.mli_out.Shape()[out_tensor_ch_dimension];
+    int slice_channels =
+        static_cast<int>(mli_weights_shape[weight_out_ch_dimension]);
+
+    // Tensors for data in fast (local) memory
+    // and config to copy data from external to local memory
+    mli_tensor weights_local = *data.mli_weights.MliTensor();
+    mli_tensor bias_local = *data.mli_bias.MliTensor();
+    mli_tensor in_local = *data.mli_in.MliTensor();
+    mli_tensor out_local =
+        *data.mli_out.MliTensor();  // this assumes that output shape
+                                    // is already filled in the tensor struct.
+
+    ops::micro::MliTensorInterface weights_local_interface(&weights_local);
+    ops::micro::MliTensorInterface bias_local_interface(&bias_local);
+    ops::micro::MliTensorInterface in_local_interface(&in_local);
+    ops::micro::MliTensorInterface out_local_interface(&out_local);
+
+    mli_mov_cfg_t copy_config;
+    mli_mov_cfg_for_copy(&copy_config);
+
+    TF_LITE_ENSURE_STATUS(ops::micro::get_arc_scratch_buffer_for_conv_tensors(
+        context, &in_local_interface, &weights_local_interface,
+        &bias_local_interface, &out_local_interface));
+
+    /* is_local indicates that the tensor is already in local memory,
+     so in that case the original tensor can be used,
+     and there is no need to copy it to the local tensor*/
+    const bool in_is_local =
+        in_local_interface.Data<int8_t>() == data.mli_in.Data<int8_t>();
+    const bool out_is_local =
+        out_local_interface.Data<int8_t>() == data.mli_out.Data<int8_t>();
+    const bool w_is_local = weights_local_interface.Data<int8_t>() ==
+                            data.mli_weights.Data<int8_t>();
+    const bool b_is_local =
+        bias_local_interface.Data<int32_t>() == data.mli_bias.Data<int32_t>();
+
+    TF_LITE_ENSURE_STATUS(ops::micro::arc_scratch_buffer_calc_slice_size_io(
+        &in_local_interface, &out_local_interface, kernel_height,
+        cfg_local.stride_height, cfg_local.padding_top,
+        cfg_local.padding_bottom, &in_slice_height, &out_slice_height));
+    TF_LITE_ENSURE_STATUS(
+        ops::micro::arc_scratch_buffer_calc_slice_size_weights(
+            &weights_local_interface, &bias_local_interface,
+            weight_out_ch_dimension, &slice_channels));
+
+    /* if input channels is not equal to output channels, a channel multiplier
+       is used. in this case the slice channels needs to be rounded down to a
+       multiple of the input channels */
+    if (in_channels != out_channels) {
+      slice_channels = (slice_channels / in_channels) * in_channels;
+    }
+
+    ops::micro::TensorSlicer b_slice(data.mli_bias.MliTensor(),
+                                     bias_out_ch_dimension, slice_channels);
+    ops::micro::TensorSlicer w_slice(data.mli_weights.MliTensor(),
+                                     weight_out_ch_dimension, slice_channels, 0,
+                                     0, 0, true);
+    ops::micro::TensorSlicer out_ch_slice(data.mli_out.MliTensor(),
+                                          out_tensor_ch_dimension,
+                                          slice_channels, 0, 0, 0, true);
+    ops::micro::TensorSlicer in_ch_slice(data.mli_in.MliTensor(),
+                                         out_tensor_ch_dimension,
+                                         slice_channels, 0, 0, 0, true);
+
+    mli_tensor* w_ptr = w_is_local ? w_slice.Sub() : &weights_local;
+    mli_tensor* b_ptr = b_is_local ? b_slice.Sub() : &bias_local;
+
+    void* input_buffer_ptr = NULL;
+    uint32_t input_buffer_size = 0;
+    int padding_top = cfg_local.padding_top;
+    int padding_bottom = cfg_local.padding_bottom;
+
+    while (!w_slice.Done()) {
+      mli_mov_tensor_sync(w_slice.Sub(), &copy_config, w_ptr);
+      mli_mov_tensor_sync(b_slice.Sub(), &copy_config, b_ptr);
+
+      /* input tensor is already sliced in the  channel dimension.
+      out_ch_slice.Sub() is the tensor for the amount of channels of this
+      iteration of the weight slice loop. This tensor needs to be further
+      sliced over the batch and height dimension. in_ch_slice.Sub() tensor
+      contains batches of HWC tensors. so it is a 4 dimensional tensor. because
+      the mli kernel will process one HWC tensor at a time, the 4 dimensional
+      tensor needs to be sliced into nBatch 3 dimensional tensors. on top of
+      that there could be a need to also slice in the Height dimension. for that
+      the sliceHeight has been calculated. The tensor slicer is configured that
+      it will completely slice the nBatch dimension (0) and slice the height
+      dimension (1) in chunks of 'sliceHeight' */
+      ops::micro::TensorSlicer in_slice(in_ch_slice.Sub(), height_dimension,
+                                        in_slice_height, padding_top,
+                                        padding_bottom, overlap);
+
+      /* output tensor is already sliced in the output channel dimension.
+      out_ch_slice.Sub() is the tensor for the amount of output channels of this
+      iteration of the weight slice loop. This tensor needs to be further
+      sliced over the batch and height dimension. */
+      ops::micro::TensorSlicer out_slice(out_ch_slice.Sub(), height_dimension,
+                                         out_slice_height);
+
+      /* setup the pointers to the local or remote tensor to make the code
+       * inside the loop easier. */
+      mli_tensor* in_ptr = in_is_local ? in_slice.Sub() : &in_local;
+      mli_tensor* out_ptr = out_is_local ? out_slice.Sub() : &out_local;
+
+      while (!out_slice.Done()) {
+        if (!out_is_local) {
+          ops::micro::PrepareLocalTensor(out_slice.Sub(), &out_local);
+          ops::micro::PrepareLocalTensor(in_slice.Sub(), &in_local);
+        }
+        TF_LITE_ENSURE(context, !in_slice.Done());
+        cfg_local.padding_top = in_slice.GetPaddingPre();
+        cfg_local.padding_bottom = in_slice.GetPaddingPost();
+
+        // if same input copy as previous iteration, skip the copy of input
+#ifdef MLI_2_0
+        if ((in_slice.Sub()->data.mem.pi8 != input_buffer_ptr) ||
+            (mli_hlp_count_elem_num(in_slice.Sub(), 0) != input_buffer_size)) {
+          mli_mov_tensor_sync(in_slice.Sub(), &copy_config, in_ptr);
+          input_buffer_ptr = in_slice.Sub()->data.mem.pi8;
+          input_buffer_size = mli_hlp_count_elem_num(in_slice.Sub(), 0);
+        }
+
+#ifdef MLI_2_0_KRNL_TEST
+        // Checking conditions here to prevent usage non-contiguous buffer
+        // memory.
+        if (mli_weights_shape[weight_out_ch_dimension] !=
+            w_slice.Sub()->shape[3]) {
+          MicroPrintf("Slicing is not supported with real-time permutation.");
+          return kTfLiteError;
+        }
+        uint8_t dim_order[] = {1, 2, 0, 3};
+        ops::micro::change_shape(w_ptr, dim_order);
+#endif
+
+        data.p_mli_krn_depthwise_conv2d_sa8_sa8_sa32(in_ptr, w_ptr, b_ptr,
+                                                     &cfg_local, out_ptr);
+#else
+        if ((in_slice.Sub()->data != input_buffer_ptr) ||
+            (mli_hlp_count_elem_num(in_slice.Sub(), 0) != input_buffer_size)) {
+          mli_mov_tensor_sync(in_slice.Sub(), &copy_config, in_ptr);
+          input_buffer_ptr = in_slice.Sub()->data;
+          input_buffer_size = mli_hlp_count_elem_num(in_slice.Sub(), 0);
+        }
+        data.p_mli_krn_depthwise_conv2d_sa8_sa8_sa32(in_ptr, w_ptr, b_ptr,
+                                                     &cfg_local, out_ptr);
+#endif
+
+        mli_mov_tensor_sync(out_ptr, &copy_config, out_slice.Sub());
+
+        in_slice.Next();
+        out_slice.Next();
+      }
+      w_slice.Next();
+      b_slice.Next();
+      out_ch_slice.Next();
+      in_ch_slice.Next();
+      TF_LITE_ENSURE(context, in_slice.Done());
+    }
+  }
+  return kTfLiteOk;
+}
+
+void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
+                             TfLiteDepthwiseConvParams* params,
+                             const OpData& data, const TfLiteEvalTensor* input,
+                             const TfLiteEvalTensor* filter,
+                             const TfLiteEvalTensor* bias,
+                             TfLiteEvalTensor* output) {
+#if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+  DepthwiseParams op_params;
+  op_params.padding_type = PaddingType::kSame;
+  op_params.padding_values.width = data.padding.width;
+  op_params.padding_values.height = data.padding.height;
+  op_params.stride_width = params->stride_width;
+  op_params.stride_height = params->stride_height;
+  op_params.dilation_width_factor = params->dilation_width_factor;
+  op_params.dilation_height_factor = params->dilation_height_factor;
+  op_params.depth_multiplier = params->depth_multiplier;
+  op_params.input_offset = -data.input_zero_point;
+  op_params.weights_offset = 0;
+  op_params.output_offset = data.output_zero_point;
+  op_params.quantized_activation_min = std::numeric_limits<int8_t>::min();
+  op_params.quantized_activation_max = std::numeric_limits<int8_t>::max();
+
+  reference_integer_ops::DepthwiseConvPerChannel(
+      op_params, data.per_channel_output_multiplier,
+      data.per_channel_output_shift, tflite::micro::GetTensorShape(input),
+      tflite::micro::GetTensorData<int8_t>(input),
+      tflite::micro::GetTensorShape(filter),
+      tflite::micro::GetTensorData<int8_t>(filter),
+      tflite::micro::GetTensorShape(bias),
+      tflite::micro::GetTensorData<int32_t>(bias),
+      tflite::micro::GetTensorShape(output),
+      tflite::micro::GetTensorData<int8_t>(output));
+#else
+  MicroPrintf("Node configuration is not supported by ARC MLI Library.");
+#endif
+}
+
+TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
+  TFLITE_DCHECK(node->user_data != nullptr);
+  TFLITE_DCHECK(node->builtin_data != nullptr);
+
+  auto* params =
+      reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);
+  const OpData& data = *(static_cast<const OpData*>(node->user_data));
+
+  TfLiteEvalTensor* output =
+      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+  const TfLiteEvalTensor* input =
+      tflite::micro::GetEvalInput(context, node, kInputTensor);
+  const TfLiteEvalTensor* filter =
+      tflite::micro::GetEvalInput(context, node, kFilterTensor);
+  const TfLiteEvalTensor* bias =
+      (NumInputs(node) == 3)
+          ? tflite::micro::GetEvalInput(context, node, kBiasTensor)
+          : nullptr;
+
+  switch (input->type) {  // Already know in/out types are same.
+    case kTfLiteFloat32:
+      EvalFloat(context, node, params, data, input, filter, bias, output);
+      break;
+    case kTfLiteInt8:
+      if (data.is_mli_applicable) {
+        EvalMliQuantizedPerChannel(context, node, params, data, input, filter,
+                                   bias, output);
+      } else {
+        EvalQuantizedPerChannel(context, node, params, data, input, filter,
+                                bias, output);
+      }
+      break;
+    case kTfLiteInt16: {
+      switch (filter->type) {
+        case kTfLiteInt8: {
+          reference_integer_ops::DepthwiseConvPerChannel(
+              DepthwiseConvParamsQuantized(params, data),
+              data.per_channel_output_multiplier, data.per_channel_output_shift,
+              tflite::micro::GetTensorShape(input),
+              tflite::micro::GetTensorData<int16_t>(input),
+              tflite::micro::GetTensorShape(filter),
+              tflite::micro::GetTensorData<int8_t>(filter),
+              tflite::micro::GetTensorShape(bias),
+              tflite::micro::GetOptionalTensorData<int64_t>(bias),
+              tflite::micro::GetTensorShape(output),
+              tflite::micro::GetTensorData<int16_t>(output));
+          break;
+        }
+        default:
+          MicroPrintf("Filter type %s (%d) for input type %s not supported.",
+                      TfLiteTypeGetName(filter->type), filter->type,
+                      TfLiteTypeGetName(input->type));
+          return kTfLiteError;
+      }
+      break;
+    }
+    default:
+      MicroPrintf("Type %s (%d) not supported.", TfLiteTypeGetName(input->type),
+                  input->type);
+      #ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_POOLINGH
+          const mli_tensor *in;
+          const mli_pool_cfg *cfg;
+          mli_tensor *out;
+          mli_krn_maxpool_hwc_sa8(in, cfg, out);
+      #endif
+      return kTfLiteError;
+  }
+  return kTfLiteOk;
+}
+
+}  // namespace
+
+TFLMRegistration Register_DEPTHWISE_CONV_2D() {
+  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+}
+
+}  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/depthwise_conv.cc.orig b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/depthwise_conv.cc.orig
new file mode 100644
index 00000000..c2c9cd5c
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/depthwise_conv.cc.orig
@@ -0,0 +1,701 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/kernels/internal/reference/integer_ops/depthwise_conv.h"
+
+#include "mli_api.h"  // NOLINT
+#include "tensorflow/lite/c/builtin_op_data.h"
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/kernels/internal/common.h"
+#include "tensorflow/lite/kernels/internal/quantization_util.h"
+#include "tensorflow/lite/kernels/internal/reference/depthwiseconv_float.h"
+#include "tensorflow/lite/kernels/internal/reference/depthwiseconv_uint8.h"
+#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
+#include "tensorflow/lite/kernels/kernel_util.h"
+#include "tensorflow/lite/kernels/padding.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/mli_function_specializations.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/mli_slicers.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/mli_tf_utils.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/scratch_buf_mgr.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.h"
+#include "tensorflow/lite/micro/kernels/kernel_util.h"
+#include "tensorflow/lite/micro/micro_log.h"
+
+namespace tflite {
+namespace {
+
+constexpr int kInputTensor = 0;
+constexpr int kFilterTensor = 1;
+constexpr int kBiasTensor = 2;
+constexpr int kOutputTensor = 0;
+
+// Depthwise conv is quantized along dimension 3:
+// https://www.tensorflow.org/lite/performance/quantization_spec
+constexpr int kDepthwiseConvQuantizedDimension = 3;
+
+struct OpData {
+  TfLitePaddingValues padding;
+
+  // Cached tensor zero point values for quantized operations.
+  int32_t input_zero_point;
+  int32_t filter_zero_point;
+  int32_t output_zero_point;
+
+  // The scaling factor from input to output (aka the 'real multiplier') can
+  // be represented as a fixed point multiplier plus a left shift.
+  int32_t output_multiplier;
+  int output_shift;
+
+  // Per channel output multiplier and shift.
+  int32_t* per_channel_output_multiplier;
+  int32_t* per_channel_output_shift;
+#ifdef MLI_2_0
+  int8_t* per_channel_scale_frac_bits;
+#endif
+
+  // The range of the fused activation layer. For example for kNone and
+  // uint8_t these would be 0 and 255.
+  int32_t output_activation_min;
+  int32_t output_activation_max;
+
+  // The result of checking if MLI optimized version of tensors can be used.
+  bool is_mli_applicable;
+
+  // Tensors in MLI format.
+  mutable ops::micro::MliTensorInterface mli_in;
+  mutable ops::micro::MliTensorInterface mli_weights;
+  mutable ops::micro::MliTensorInterface mli_bias;
+  mutable ops::micro::MliTensorInterface mli_out;
+  mli_conv2d_cfg* cfg;
+
+  // Pointer to the required depthwise function. For Ã¢â‚¬Å“channel multiplierÃ¢â‚¬Â
+  // functionality group convolution is used.
+  depthwise_func_ptr p_mli_krn_depthwise_conv2d_sa8_sa8_sa32;
+};
+
+bool IsMliApplicable(TfLiteContext* context, const TfLiteTensor* input,
+                     const TfLiteTensor* filter, const TfLiteTensor* bias,
+                     const TfLiteDepthwiseConvParams* params) {
+  const auto* affine_quantization =
+      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);
+
+#ifndef MLI_2_0
+  const int in_ch = SizeOfDimension(input, 3);
+  const int filters_num = SizeOfDimension(filter, 3);
+#endif
+
+  // MLI optimized version only supports int8_t datatype, dilation factor of 1
+  // and per-axis quantization of weights (no broadcasting/per-tensor). For
+  // MLI 1.1 (in_ch == filters_num) || (in_ch == 1)) is used to prevent usage of
+  // channel multiplier logic for multichannel input.
+
+  bool ret_val = (filter->type == kTfLiteInt8) &&
+                 (input->type == kTfLiteInt8) && (bias->type == kTfLiteInt32) &&
+                 (params->dilation_width_factor == 1) &&
+                 (params->dilation_height_factor == 1) &&
+                 (affine_quantization->scale->size ==
+#ifdef MLI_2_0
+                  filter->dims->data[kDepthwiseConvQuantizedDimension]);
+#else
+                  filter->dims->data[kDepthwiseConvQuantizedDimension]) &&
+                 ((in_ch == filters_num) || (in_ch == 1));
+#endif
+  return ret_val;
+}
+
+TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,
+                             TfLiteDepthwiseConvParams* params, int width,
+                             int height, int filter_width, int filter_height,
+                             const TfLiteType data_type, OpData* data) {
+  bool has_bias = node->inputs->size == 3;
+  // Check number of inputs/outputs
+  TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
+  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);
+
+  int unused_output_height, unused_output_width;
+  data->padding = ComputePaddingHeightWidth(
+      params->stride_height, params->stride_width, 1, 1, height, width,
+      filter_height, filter_width, params->padding, &unused_output_height,
+      &unused_output_width);
+
+  // Note that quantized inference requires that all tensors have their
+  // parameters set. This is usually done during quantized training.
+#if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+  MicroContext* micro_context = GetMicroContext(context);
+
+  TfLiteTensor* input =
+      micro_context->AllocateTempInputTensor(node, kInputTensor);
+  TfLiteTensor* filter =
+      micro_context->AllocateTempInputTensor(node, kFilterTensor);
+  TfLiteTensor* bias =
+      micro_context->AllocateTempInputTensor(context, node, kBiasTensor);
+  TfLiteTensor* output =
+      micro_context->AllocateTempOutputTensor(node, kOutputTensor);
+
+  if (data_type != kTfLiteFloat32 && !data->is_mli_applicable) {
+    int num_channels = filter->dims->data[kDepthwiseConvQuantizedDimension];
+
+    return tflite::PopulateConvolutionQuantizationParams(
+        context, input, filter, bias, output, params->activation,
+        &data->output_multiplier, &data->output_shift,
+        &data->output_activation_min, &data->output_activation_max,
+        data->per_channel_output_multiplier,
+        reinterpret_cast<int*>(data->per_channel_output_shift), num_channels);
+  }
+  micro_context->DeallocateTempTfLiteTensor(input);
+  micro_context->DeallocateTempTfLiteTensor(filter);
+  micro_context->DeallocateTempTfLiteTensor(bias);
+  micro_context->DeallocateTempTfLiteTensor(output);
+
+#endif
+  return kTfLiteOk;
+}
+
+void* Init(TfLiteContext* context, const char* buffer, size_t length) {
+  TFLITE_DCHECK(context->AllocatePersistentBuffer != nullptr);
+  return context->AllocatePersistentBuffer(context, sizeof(OpData));
+}
+
+TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
+  TFLITE_DCHECK(node->user_data != nullptr);
+  TFLITE_DCHECK(node->builtin_data != nullptr);
+
+  auto* params =
+      reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);
+  OpData* data = static_cast<OpData*>(node->user_data);
+
+  TfLiteTensor* output = AllocateTempOutputTensor(node, kOutputTensor);
+  const TfLiteTensor* input = AllocateTempInputTensor(node, kInputTensor);
+  const TfLiteTensor* filter = AllocateTempInputTensor(node, kFilterTensor);
+  const TfLiteTensor* bias =
+      AllocateTempInputTensor(context, node, kBiasTensor);
+
+  const TfLiteType data_type = input->type;
+  int width = SizeOfDimension(input, 2);
+  int height = SizeOfDimension(input, 1);
+
+#if defined(MLI_2_0) && !defined(MLI_2_0_KRNL_TEST)
+  int filter_width = SizeOfDimension(filter, 1);
+  int filter_height = SizeOfDimension(filter, 0);
+#else
+  int filter_width = SizeOfDimension(filter, 2);
+  int filter_height = SizeOfDimension(filter, 1);
+#endif
+
+  // Per channel quantization is only needed for int8 inference. For other
+  // quantized types, only a single scale and zero point is needed.
+  const int num_channels = filter->dims->data[kDepthwiseConvQuantizedDimension];
+  // Dynamically allocate per-channel quantization parameters.
+  data->per_channel_output_multiplier =
+      reinterpret_cast<int32_t*>(context->AllocatePersistentBuffer(
+          context, num_channels * sizeof(int32_t)));
+  data->per_channel_output_shift =
+      reinterpret_cast<int32_t*>(context->AllocatePersistentBuffer(
+          context, num_channels * sizeof(int32_t)));
+
+  data->is_mli_applicable =
+      IsMliApplicable(context, input, filter, bias, params);
+
+  // All per-channel quantized tensors need valid zero point and scale arrays.
+  if (input->type == kTfLiteInt8) {
+    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
+                      kTfLiteAffineQuantization);
+
+    const auto* affine_quantization =
+        reinterpret_cast<TfLiteAffineQuantization*>(
+            filter->quantization.params);
+    TF_LITE_ENSURE(context, affine_quantization);
+    TF_LITE_ENSURE(context, affine_quantization->scale);
+    TF_LITE_ENSURE(context, affine_quantization->zero_point);
+    TF_LITE_ENSURE(
+        context, affine_quantization->scale->size == 1 ||
+                     affine_quantization->scale->size ==
+                         filter->dims->data[kDepthwiseConvQuantizedDimension]);
+    TF_LITE_ENSURE_EQ(context, affine_quantization->scale->size,
+                      affine_quantization->zero_point->size);
+  }
+
+  TF_LITE_ENSURE_STATUS(CalculateOpData(context, node, params, width, height,
+                                        filter_width, filter_height, data_type,
+                                        data));
+
+  data->input_zero_point = input->params.zero_point;
+  data->filter_zero_point = filter->params.zero_point;
+  data->output_zero_point = output->params.zero_point;
+
+  if (data->is_mli_applicable) {
+    data->mli_in = ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+    data->mli_weights = ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+    data->mli_bias = ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+    data->mli_out = ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+    data->cfg = static_cast<mli_conv2d_cfg*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_conv2d_cfg)));
+
+#ifdef MLI_2_0
+    const int num_buffers = 2;
+    data->per_channel_scale_frac_bits =
+        static_cast<int8_t*>(context->AllocatePersistentBuffer(
+            context, num_buffers * num_channels * sizeof(int16_t)));
+#endif
+
+    // Reuse space allocated for OpData parameters.
+#ifdef MLI_2_0
+    *data->mli_weights.Scale<int16_t**>() =
+        reinterpret_cast<int16_t*>(data->per_channel_output_multiplier);
+    *data->mli_bias.Scale<int16_t**>() =
+        reinterpret_cast<int16_t*>(data->per_channel_output_multiplier) +
+        num_channels;
+#else
+    *data->mli_weights.Scale<int32_t**>() =
+        static_cast<int32_t*>(data->per_channel_output_multiplier);
+    *data->mli_bias.Scale<int32_t**>() =
+        static_cast<int32_t*>(data->per_channel_output_shift);
+#endif
+
+#ifdef MLI_2_0
+    *data->mli_weights.ZeroPoint<int16_t**>() =
+        reinterpret_cast<int16_t*>(data->per_channel_output_shift);
+    *data->mli_bias.ZeroPoint<int16_t**>() =
+        reinterpret_cast<int16_t*>(data->per_channel_output_shift) +
+        num_channels;
+#else
+    *data->mli_weights.ZeroPoint<int16_t**>() =
+        reinterpret_cast<int16_t*>(&data->filter_zero_point);
+    *data->mli_bias.ZeroPoint<int16_t**>() =
+        reinterpret_cast<int16_t*>(&data->filter_zero_point) + sizeof(int16_t);
+#endif
+
+#ifdef MLI_2_0
+    *data->mli_weights.ScaleFracBits<int8_t**>() =
+        reinterpret_cast<int8_t*>(data->per_channel_scale_frac_bits);
+    *data->mli_bias.ScaleFracBits<int8_t**>() =
+        reinterpret_cast<int8_t*>(data->per_channel_scale_frac_bits) +
+        num_channels;
+#endif
+
+    ops::micro::ConvertToMliTensor(input, &data->mli_in);
+    ops::micro::ConvertToMliTensorPerChannel(filter, &data->mli_weights,
+                                             /* is_bias_tensor = */ false);
+    ops::micro::ConvertToMliTensorPerChannel(bias, &data->mli_bias,
+                                             /* is_bias_tensor = */ true);
+#ifdef MLI_2_0
+    ops::micro::AdjustBiasTensor(&data->mli_bias, &data->mli_in,
+                                 &data->mli_weights);
+#endif
+    ops::micro::ConvertToMliTensor(output, &data->mli_out);
+
+#ifdef MLI_2_0
+    // Choose group convolution function for "channel multiplier" functionality.
+    const int in_ch = SizeOfDimension(input, 3);
+    const int filters_num = SizeOfDimension(filter, 3);
+    const int channels_num = SizeOfDimension(filter, 2);
+    if (in_ch == filters_num && channels_num == 1) {
+      data->p_mli_krn_depthwise_conv2d_sa8_sa8_sa32 =
+          mli_krn_depthwise_conv2d(data->mli_weights.MliTensor());
+    } else {
+      data->p_mli_krn_depthwise_conv2d_sa8_sa8_sa32 =
+          mli_krn_group_conv2d(data->mli_weights.MliTensor());
+    }
+#else
+    data->p_mli_krn_depthwise_conv2d_sa8_sa8_sa32 =
+        mli_krn_depthwise_conv2d(data->mli_weights.MliTensor(), data->cfg);
+#endif
+
+#ifdef MLI_2_0
+    data->cfg->dilation_width = 1;
+    data->cfg->dilation_height = 1;
+#endif
+
+    if (data->output_activation_min == -128 &&
+        data->output_activation_max == 127) {
+      data->cfg->relu.type = MLI_RELU_NONE;
+    } else if (params->activation == kTfLiteActRelu) {
+      data->cfg->relu.type = MLI_RELU_GEN;
+    } else if (params->activation == kTfLiteActRelu6) {
+      data->cfg->relu.type = MLI_RELU_6;
+    } else if (params->activation == kTfLiteActReluN1To1) {
+      data->cfg->relu.type = MLI_RELU_1;
+    } else {
+      data->cfg->relu.type = MLI_RELU_NONE;
+    }
+
+    data->cfg->stride_width = params->stride_width;
+    data->cfg->stride_height = params->stride_height;
+    if (params->padding == kTfLitePaddingValid) {
+      data->cfg->padding_left = 0;
+      data->cfg->padding_right = 0;
+      data->cfg->padding_top = 0;
+      data->cfg->padding_bottom = 0;
+    } else {
+      data->cfg->padding_left = data->padding.width;
+      data->cfg->padding_right =
+          data->padding.width + data->padding.width_offset;
+      data->cfg->padding_top = data->padding.height;
+      data->cfg->padding_bottom =
+          data->padding.height + data->padding.height_offset;
+    }
+  }
+  return kTfLiteOk;
+}
+
+void EvalFloat(TfLiteContext* context, TfLiteNode* node,
+               TfLiteDepthwiseConvParams* params, const OpData& data,
+               const TfLiteEvalTensor* input, const TfLiteEvalTensor* filter,
+               const TfLiteEvalTensor* bias, TfLiteEvalTensor* output) {
+#if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+  float output_activation_min, output_activation_max;
+  CalculateActivationRange(params->activation, &output_activation_min,
+                           &output_activation_max);
+
+  tflite::DepthwiseParams op_params;
+  // Padding type is ignored, but still set.
+  op_params.padding_type = PaddingType::kSame;
+  op_params.padding_values.width = data.padding.width;
+  op_params.padding_values.height = data.padding.height;
+  op_params.stride_width = params->stride_width;
+  op_params.stride_height = params->stride_height;
+  op_params.dilation_width_factor = params->dilation_width_factor;
+  op_params.dilation_height_factor = params->dilation_height_factor;
+  op_params.depth_multiplier = params->depth_multiplier;
+  op_params.float_activation_min = output_activation_min;
+  op_params.float_activation_max = output_activation_max;
+
+  tflite::reference_ops::DepthwiseConv(
+      op_params, tflite::micro::GetTensorShape(input),
+      tflite::micro::GetTensorData<float>(input),
+      tflite::micro::GetTensorShape(filter),
+      tflite::micro::GetTensorData<float>(filter),
+      tflite::micro::GetTensorShape(bias),
+      tflite::micro::GetTensorData<float>(bias),
+      tflite::micro::GetTensorShape(output),
+      tflite::micro::GetTensorData<float>(output));
+#else
+  MicroPrintf("Type %s (%d) is not supported by ARC MLI Library.",
+              TfLiteTypeGetName(input->type), input->type);
+#endif
+}
+TfLiteStatus EvalMliQuantizedPerChannel(
+    TfLiteContext* context, TfLiteNode* node, TfLiteDepthwiseConvParams* params,
+    const OpData& data, const TfLiteEvalTensor* input,
+    const TfLiteEvalTensor* filter, const TfLiteEvalTensor* bias,
+    TfLiteEvalTensor* output) {
+  // Run Depthwise Conv MLI kernel
+  // MLI optimized version only supports int8_t dataype and dilation factor of 1
+  if (data.is_mli_applicable) {
+    // Copy configuration data from external to local memory
+    mli_conv2d_cfg cfg_local = *data.cfg;
+
+    ops::micro::MliTensorAttachBuffer<int8_t>(input, &data.mli_in);
+    ops::micro::MliTensorAttachBuffer<int8_t>(filter, &data.mli_weights);
+    ops::micro::MliTensorAttachBuffer<int32_t>(bias, &data.mli_bias);
+    ops::micro::MliTensorAttachBuffer<int8_t>(output, &data.mli_out);
+
+    // for height slicing
+    const int height_dimension = 1;
+    int in_slice_height = 0;
+    int out_slice_height = 0;
+    uint32_t* mli_weights_shape = data.mli_weights.Shape();
+#ifdef MLI_2_0
+    const int kernel_height =
+        static_cast<int>(mli_weights_shape[KRNL_DW_H_DIM_HW1N]);
+#else
+    const int kernel_height =
+        static_cast<int>(mli_weights_shape[KRNL_DW_H_DIM_HWC]);
+#endif
+    const int overlap = kernel_height - cfg_local.stride_height;
+
+    // for weight slicing (on output channels)
+    // HWCN layout for weights, output channel dimension is the first dimension.
+    const int weight_out_ch_dimension = 3;
+    // bias has only 1 dimension
+    const int bias_out_ch_dimension = 0;
+    // Batch-Height-Width-Channel layout means last dimension is output
+    // channels.
+    const int out_tensor_ch_dimension = 3;
+    const int32_t in_channels = data.mli_in.Shape()[out_tensor_ch_dimension];
+    const int32_t out_channels = data.mli_out.Shape()[out_tensor_ch_dimension];
+    int slice_channels =
+        static_cast<int>(mli_weights_shape[weight_out_ch_dimension]);
+
+    // Tensors for data in fast (local) memory
+    // and config to copy data from external to local memory
+    mli_tensor weights_local = *data.mli_weights.MliTensor();
+    mli_tensor bias_local = *data.mli_bias.MliTensor();
+    mli_tensor in_local = *data.mli_in.MliTensor();
+    mli_tensor out_local =
+        *data.mli_out.MliTensor();  // this assumes that output shape
+                                    // is already filled in the tensor struct.
+
+    ops::micro::MliTensorInterface weights_local_interface(&weights_local);
+    ops::micro::MliTensorInterface bias_local_interface(&bias_local);
+    ops::micro::MliTensorInterface in_local_interface(&in_local);
+    ops::micro::MliTensorInterface out_local_interface(&out_local);
+
+    mli_mov_cfg_t copy_config;
+    mli_mov_cfg_for_copy(&copy_config);
+
+    TF_LITE_ENSURE_STATUS(ops::micro::get_arc_scratch_buffer_for_conv_tensors(
+        context, &in_local_interface, &weights_local_interface,
+        &bias_local_interface, &out_local_interface));
+
+    /* is_local indicates that the tensor is already in local memory,
+     so in that case the original tensor can be used,
+     and there is no need to copy it to the local tensor*/
+    const bool in_is_local =
+        in_local_interface.Data<int8_t>() == data.mli_in.Data<int8_t>();
+    const bool out_is_local =
+        out_local_interface.Data<int8_t>() == data.mli_out.Data<int8_t>();
+    const bool w_is_local = weights_local_interface.Data<int8_t>() ==
+                            data.mli_weights.Data<int8_t>();
+    const bool b_is_local =
+        bias_local_interface.Data<int32_t>() == data.mli_bias.Data<int32_t>();
+
+    TF_LITE_ENSURE_STATUS(ops::micro::arc_scratch_buffer_calc_slice_size_io(
+        &in_local_interface, &out_local_interface, kernel_height,
+        cfg_local.stride_height, cfg_local.padding_top,
+        cfg_local.padding_bottom, &in_slice_height, &out_slice_height));
+    TF_LITE_ENSURE_STATUS(
+        ops::micro::arc_scratch_buffer_calc_slice_size_weights(
+            &weights_local_interface, &bias_local_interface,
+            weight_out_ch_dimension, &slice_channels));
+
+    /* if input channels is not equal to output channels, a channel multiplier
+       is used. in this case the slice channels needs to be rounded down to a
+       multiple of the input channels */
+    if (in_channels != out_channels) {
+      slice_channels = (slice_channels / in_channels) * in_channels;
+    }
+
+    ops::micro::TensorSlicer b_slice(data.mli_bias.MliTensor(),
+                                     bias_out_ch_dimension, slice_channels);
+    ops::micro::TensorSlicer w_slice(data.mli_weights.MliTensor(),
+                                     weight_out_ch_dimension, slice_channels, 0,
+                                     0, 0, true);
+    ops::micro::TensorSlicer out_ch_slice(data.mli_out.MliTensor(),
+                                          out_tensor_ch_dimension,
+                                          slice_channels, 0, 0, 0, true);
+    ops::micro::TensorSlicer in_ch_slice(data.mli_in.MliTensor(),
+                                         out_tensor_ch_dimension,
+                                         slice_channels, 0, 0, 0, true);
+
+    mli_tensor* w_ptr = w_is_local ? w_slice.Sub() : &weights_local;
+    mli_tensor* b_ptr = b_is_local ? b_slice.Sub() : &bias_local;
+
+    void* input_buffer_ptr = NULL;
+    uint32_t input_buffer_size = 0;
+    int padding_top = cfg_local.padding_top;
+    int padding_bottom = cfg_local.padding_bottom;
+
+    while (!w_slice.Done()) {
+      mli_mov_tensor_sync(w_slice.Sub(), &copy_config, w_ptr);
+      mli_mov_tensor_sync(b_slice.Sub(), &copy_config, b_ptr);
+
+      /* input tensor is already sliced in the  channel dimension.
+      out_ch_slice.Sub() is the tensor for the amount of channels of this
+      iteration of the weight slice loop. This tensor needs to be further
+      sliced over the batch and height dimension. in_ch_slice.Sub() tensor
+      contains batches of HWC tensors. so it is a 4 dimensional tensor. because
+      the mli kernel will process one HWC tensor at a time, the 4 dimensional
+      tensor needs to be sliced into nBatch 3 dimensional tensors. on top of
+      that there could be a need to also slice in the Height dimension. for that
+      the sliceHeight has been calculated. The tensor slicer is configured that
+      it will completely slice the nBatch dimension (0) and slice the height
+      dimension (1) in chunks of 'sliceHeight' */
+      ops::micro::TensorSlicer in_slice(in_ch_slice.Sub(), height_dimension,
+                                        in_slice_height, padding_top,
+                                        padding_bottom, overlap);
+
+      /* output tensor is already sliced in the output channel dimension.
+      out_ch_slice.Sub() is the tensor for the amount of output channels of this
+      iteration of the weight slice loop. This tensor needs to be further
+      sliced over the batch and height dimension. */
+      ops::micro::TensorSlicer out_slice(out_ch_slice.Sub(), height_dimension,
+                                         out_slice_height);
+
+      /* setup the pointers to the local or remote tensor to make the code
+       * inside the loop easier. */
+      mli_tensor* in_ptr = in_is_local ? in_slice.Sub() : &in_local;
+      mli_tensor* out_ptr = out_is_local ? out_slice.Sub() : &out_local;
+
+      while (!out_slice.Done()) {
+        if (!out_is_local) {
+          ops::micro::PrepareLocalTensor(out_slice.Sub(), &out_local);
+          ops::micro::PrepareLocalTensor(in_slice.Sub(), &in_local);
+        }
+        TF_LITE_ENSURE(context, !in_slice.Done());
+        cfg_local.padding_top = in_slice.GetPaddingPre();
+        cfg_local.padding_bottom = in_slice.GetPaddingPost();
+
+        // if same input copy as previous iteration, skip the copy of input
+#ifdef MLI_2_0
+        if ((in_slice.Sub()->data.mem.pi8 != input_buffer_ptr) ||
+            (mli_hlp_count_elem_num(in_slice.Sub(), 0) != input_buffer_size)) {
+          mli_mov_tensor_sync(in_slice.Sub(), &copy_config, in_ptr);
+          input_buffer_ptr = in_slice.Sub()->data.mem.pi8;
+          input_buffer_size = mli_hlp_count_elem_num(in_slice.Sub(), 0);
+        }
+
+#ifdef MLI_2_0_KRNL_TEST
+        // Checking conditions here to prevent usage non-contiguous buffer
+        // memory.
+        if (mli_weights_shape[weight_out_ch_dimension] !=
+            w_slice.Sub()->shape[3]) {
+          MicroPrintf("Slicing is not supported with real-time permutation.");
+          return kTfLiteError;
+        }
+        uint8_t dim_order[] = {1, 2, 0, 3};
+        ops::micro::change_shape(w_ptr, dim_order);
+#endif
+
+        data.p_mli_krn_depthwise_conv2d_sa8_sa8_sa32(in_ptr, w_ptr, b_ptr,
+                                                     &cfg_local, out_ptr);
+#else
+        if ((in_slice.Sub()->data != input_buffer_ptr) ||
+            (mli_hlp_count_elem_num(in_slice.Sub(), 0) != input_buffer_size)) {
+          mli_mov_tensor_sync(in_slice.Sub(), &copy_config, in_ptr);
+          input_buffer_ptr = in_slice.Sub()->data;
+          input_buffer_size = mli_hlp_count_elem_num(in_slice.Sub(), 0);
+        }
+        data.p_mli_krn_depthwise_conv2d_sa8_sa8_sa32(in_ptr, w_ptr, b_ptr,
+                                                     &cfg_local, out_ptr);
+#endif
+
+        mli_mov_tensor_sync(out_ptr, &copy_config, out_slice.Sub());
+
+        in_slice.Next();
+        out_slice.Next();
+      }
+      w_slice.Next();
+      b_slice.Next();
+      out_ch_slice.Next();
+      in_ch_slice.Next();
+      TF_LITE_ENSURE(context, in_slice.Done());
+    }
+  }
+  return kTfLiteOk;
+}
+
+void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
+                             TfLiteDepthwiseConvParams* params,
+                             const OpData& data, const TfLiteEvalTensor* input,
+                             const TfLiteEvalTensor* filter,
+                             const TfLiteEvalTensor* bias,
+                             TfLiteEvalTensor* output) {
+#if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+  DepthwiseParams op_params;
+  op_params.padding_type = PaddingType::kSame;
+  op_params.padding_values.width = data.padding.width;
+  op_params.padding_values.height = data.padding.height;
+  op_params.stride_width = params->stride_width;
+  op_params.stride_height = params->stride_height;
+  op_params.dilation_width_factor = params->dilation_width_factor;
+  op_params.dilation_height_factor = params->dilation_height_factor;
+  op_params.depth_multiplier = params->depth_multiplier;
+  op_params.input_offset = -data.input_zero_point;
+  op_params.weights_offset = 0;
+  op_params.output_offset = data.output_zero_point;
+  op_params.quantized_activation_min = std::numeric_limits<int8_t>::min();
+  op_params.quantized_activation_max = std::numeric_limits<int8_t>::max();
+
+  reference_integer_ops::DepthwiseConvPerChannel(
+      op_params, data.per_channel_output_multiplier,
+      data.per_channel_output_shift, tflite::micro::GetTensorShape(input),
+      tflite::micro::GetTensorData<int8_t>(input),
+      tflite::micro::GetTensorShape(filter),
+      tflite::micro::GetTensorData<int8_t>(filter),
+      tflite::micro::GetTensorShape(bias),
+      tflite::micro::GetTensorData<int32_t>(bias),
+      tflite::micro::GetTensorShape(output),
+      tflite::micro::GetTensorData<int8_t>(output));
+#else
+  MicroPrintf("Node configuration is not supported by ARC MLI Library.");
+#endif
+}
+
+TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
+  TFLITE_DCHECK(node->user_data != nullptr);
+  TFLITE_DCHECK(node->builtin_data != nullptr);
+
+  auto* params =
+      reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);
+  const OpData& data = *(static_cast<const OpData*>(node->user_data));
+
+  TfLiteEvalTensor* output =
+      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+  const TfLiteEvalTensor* input =
+      tflite::micro::GetEvalInput(context, node, kInputTensor);
+  const TfLiteEvalTensor* filter =
+      tflite::micro::GetEvalInput(context, node, kFilterTensor);
+  const TfLiteEvalTensor* bias =
+      (NumInputs(node) == 3)
+          ? tflite::micro::GetEvalInput(context, node, kBiasTensor)
+          : nullptr;
+
+  switch (input->type) {  // Already know in/out types are same.
+    case kTfLiteFloat32:
+      EvalFloat(context, node, params, data, input, filter, bias, output);
+      break;
+    case kTfLiteInt8:
+      if (data.is_mli_applicable) {
+        EvalMliQuantizedPerChannel(context, node, params, data, input, filter,
+                                   bias, output);
+      } else {
+        EvalQuantizedPerChannel(context, node, params, data, input, filter,
+                                bias, output);
+      }
+      break;
+    case kTfLiteInt16: {
+      switch (filter->type) {
+        case kTfLiteInt8: {
+          reference_integer_ops::DepthwiseConvPerChannel(
+              DepthwiseConvParamsQuantized(params, data),
+              data.per_channel_output_multiplier, data.per_channel_output_shift,
+              tflite::micro::GetTensorShape(input),
+              tflite::micro::GetTensorData<int16_t>(input),
+              tflite::micro::GetTensorShape(filter),
+              tflite::micro::GetTensorData<int8_t>(filter),
+              tflite::micro::GetTensorShape(bias),
+              tflite::micro::GetOptionalTensorData<int64_t>(bias),
+              tflite::micro::GetTensorShape(output),
+              tflite::micro::GetTensorData<int16_t>(output));
+          break;
+        }
+        default:
+          MicroPrintf("Filter type %s (%d) for input type %s not supported.",
+                      TfLiteTypeGetName(filter->type), filter->type,
+                      TfLiteTypeGetName(input->type));
+          return kTfLiteError;
+      }
+      break;
+    }
+    default:
+      MicroPrintf("Type %s (%d) not supported.", TfLiteTypeGetName(input->type),
+                  input->type);
+      return kTfLiteError;
+  }
+  return kTfLiteOk;
+}
+
+}  // namespace
+
+TFLMRegistration Register_DEPTHWISE_CONV_2D() {
+  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+}
+
+}  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/depthwise_conv.cc.rej b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/depthwise_conv.cc.rej
new file mode 100644
index 00000000..080f4ddc
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/depthwise_conv.cc.rej
@@ -0,0 +1,15 @@
+--- tensorflow/lite/micro/kernels/arc_mli/depthwise_conv.cc	2021-04-09 13:12:53.000000000 +0800
++++ tensorflow/lite/micro/kernels/arc_mli/depthwise_conv.cc	2023-07-08 01:12:31.559204895 +0800
+@@ -509,6 +509,12 @@
+     default:
+       TF_LITE_KERNEL_LOG(context, "Type %s (%d) not supported.",
+                          TfLiteTypeGetName(input->type), input->type);
++      #ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_POOLINGH
++        const mli_tensor *in;
++        const mli_pool_cfg *cfg;
++        mli_tensor *out;
++        mli_krn_maxpool_hwc_sa8(in, cfg, out);
++      #endif
+       return kTfLiteError;
+   }
+   return kTfLiteOk;
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/fully_connected.cc b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/fully_connected.cc
new file mode 100644
index 00000000..4af06601
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/fully_connected.cc
@@ -0,0 +1,476 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/kernels/internal/reference/fully_connected.h"
+
+#include "mli_api.h"  // NOLINT
+#include "tensorflow/lite/c/builtin_op_data.h"
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/kernels/internal/common.h"
+#include "tensorflow/lite/kernels/internal/quantization_util.h"
+#include "tensorflow/lite/kernels/internal/reference/integer_ops/fully_connected.h"
+#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
+#include "tensorflow/lite/kernels/kernel_util.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/mli_slicers.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/mli_tf_utils.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/scratch_buf_mgr.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.h"
+#include "tensorflow/lite/micro/kernels/kernel_util.h"
+#include "tensorflow/lite/micro/micro_log.h"
+
+namespace tflite {
+namespace {
+
+struct OpData {
+  // The scaling factor from input to output (aka the 'real multiplier') can
+  // be represented as a fixed point multiplier plus a left shift.
+  int32_t output_multiplier;
+  int output_shift;
+  // The range of the fused activation layer. For example for kNone and
+  // uint8_t these would be 0 and 255.
+  int32_t output_activation_min;
+  int32_t output_activation_max;
+  // The index of the temporary tensor where the quantized inputs are cached.
+  int input_quantized_index;
+  // Cached tensor zero point values for quantized operations.
+  int32_t input_zero_point;
+  int32_t filter_zero_point;
+  int32_t output_zero_point;
+
+  // The result of checking if MLI optimized version of tensors can be used.
+  bool is_mli_applicable;
+
+  // Tensors in MLI format.
+  mutable ops::micro::MliTensorInterface mli_in;
+  mutable ops::micro::MliTensorInterface mli_weights;
+  mutable ops::micro::MliTensorInterface mli_bias;
+  mutable ops::micro::MliTensorInterface mli_out;
+
+#ifdef MLI_2_0
+  mli_fully_connected_cfg* cfg;
+#endif
+};
+
+constexpr int kInputTensor = 0;
+constexpr int kWeightsTensor = 1;
+constexpr int kBiasTensor = 2;
+constexpr int kOutputTensor = 0;
+
+bool IsMliApplicable(TfLiteContext* context, const TfLiteTensor* input,
+                     const TfLiteTensor* filter, const TfLiteTensor* bias,
+                     const TfLiteFullyConnectedParams* params,
+                     int32_t output_activation_min,
+                     int32_t output_activation_max) {
+  // MLI optimized version only supports int8_t datatype and no fused Relu and
+  // symmetric per-tensor quantization of weights (not per-axis)
+  bool ret_val =
+      (filter->type == kTfLiteInt8) && (input->type == kTfLiteInt8) &&
+      (bias->type == kTfLiteInt32) &&
+#ifndef MLI_2_0
+      (params->activation == kTfLiteActNone ||
+       (output_activation_min == -128 && output_activation_max == 127)) &&
+#endif
+      (filter->params.zero_point == 0);
+  return ret_val;
+}
+
+TfLiteStatus CalculateOpData(TfLiteContext* context,
+                             const TfLiteFullyConnectedParams* params,
+                             TfLiteType data_type, const TfLiteTensor* input,
+                             const TfLiteTensor* filter,
+                             const TfLiteTensor* bias, TfLiteTensor* output,
+                             OpData* data) {
+  TfLiteStatus status = kTfLiteOk;
+#if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+  if (data_type != kTfLiteFloat32 && !data->is_mli_applicable) {
+    double real_multiplier = 0.0;
+    TF_LITE_ENSURE_STATUS(GetQuantizedConvolutionMultipler(
+        context, input, filter, bias, output, &real_multiplier));
+    int exponent;
+    QuantizeMultiplier(real_multiplier, &data->output_multiplier, &exponent);
+    data->output_shift = -exponent;
+    TF_LITE_ENSURE_STATUS(CalculateActivationRangeQuantized(
+        context, params->activation, output, &data->output_activation_min,
+        &data->output_activation_max));
+  }
+#endif
+  return status;
+}
+
+}  // namespace
+
+void* Init(TfLiteContext* context, const char* buffer, size_t length) {
+  TFLITE_DCHECK(context->AllocatePersistentBuffer != nullptr);
+  return context->AllocatePersistentBuffer(context, sizeof(OpData));
+}
+
+TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
+  TFLITE_DCHECK(node->user_data != nullptr);
+  TFLITE_DCHECK(node->builtin_data != nullptr);
+
+  OpData* data = static_cast<OpData*>(node->user_data);
+  const auto params =
+      static_cast<const TfLiteFullyConnectedParams*>(node->builtin_data);
+
+  MicroContext* micro_context = GetMicroContext(context);
+
+  TfLiteTensor* input =
+      micro_context->AllocateTempInputTensor(node, kInputTensor);
+  TfLiteTensor* filter =
+      micro_context->AllocateTempInputTensor(node, kWeightsTensor);
+  TfLiteTensor* bias =
+      micro_context->AllocateTempInputTensor(context, node, kBiasTensor);
+  TfLiteTensor* output = AllocateTempOutputTensor(node, kOutputTensor);
+
+  TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);
+  TF_LITE_ENSURE_MSG(context, input->type == filter->type,
+                     "Hybrid models are not supported on TFLite Micro.");
+
+  data->input_zero_point = input->params.zero_point;
+  data->filter_zero_point = filter->params.zero_point;
+  data->output_zero_point = output->params.zero_point;
+
+  TfLiteStatus status = CalculateOpData(context, params, input->type, input,
+                                        filter, bias, output, data);
+
+  data->is_mli_applicable =
+      IsMliApplicable(context, input, filter, bias, params,
+                      data->output_activation_min, data->output_activation_max);
+
+  if (input->type == kTfLiteInt8 && data->is_mli_applicable) {
+    data->mli_in = ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+    data->mli_weights = ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+    data->mli_bias = ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+    data->mli_out = ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+
+    ops::micro::ConvertToMliTensor(input, &data->mli_in);
+    ops::micro::ConvertToMliTensor(filter, &data->mli_weights);
+    ops::micro::ConvertToMliTensor(bias, &data->mli_bias);
+#ifdef MLI_2_0
+    ops::micro::AdjustBiasTensor(&data->mli_bias, &data->mli_in,
+                                 &data->mli_weights);
+#endif
+    ops::micro::ConvertToMliTensor(output, &data->mli_out);
+
+#ifdef MLI_2_0
+    if (data->output_activation_min == -128 &&
+        data->output_activation_max == 127) {
+      data->cfg->relu.type = MLI_RELU_NONE;
+    } else if (params->activation == kTfLiteActRelu) {
+      data->cfg->relu.type = MLI_RELU_GEN;
+    } else if (params->activation == kTfLiteActRelu6) {
+      data->cfg->relu.type = MLI_RELU_6;
+    } else if (params->activation == kTfLiteActReluN1To1) {
+      data->cfg->relu.type = MLI_RELU_1;
+    } else {
+      data->cfg->relu.type = MLI_RELU_NONE;
+    }
+#endif
+
+    /* The input tensor can have more than 2 dimensions. for the compute this
+   doesn't make any difference because all the inputs or a batch entry will
+   be used anyway. because the MLI kernel doesn't recognize the multiple
+   dimensions, the tensor shape is casted to a {batchnum, inputsize} shape. */
+    data->mli_in.Shape()[0] = data->mli_out.Shape()[0];
+#if defined(MLI_2_0) && !defined(MLI_2_0_KRNL_TEST)
+    data->mli_in.Shape()[1] = data->mli_weights.Shape()[0];
+#else
+    data->mli_in.Shape()[1] = data->mli_weights.Shape()[1];
+#endif
+    data->mli_in.Shape()[2] = 0;
+    data->mli_in.Shape()[3] = 0;
+    data->mli_in.MemStride()[0] = data->mli_in.Shape()[1];
+    data->mli_in.MemStride()[1] = 0;
+    *data->mli_in.Rank() = 2;
+  }
+
+  micro_context->DeallocateTempTfLiteTensor(input);
+  micro_context->DeallocateTempTfLiteTensor(filter);
+  micro_context->DeallocateTempTfLiteTensor(bias);
+  micro_context->DeallocateTempTfLiteTensor(output);
+  return status;
+}
+
+TfLiteStatus EvalMliQuantizedInt8(TfLiteContext* context, TfLiteNode* node,
+                                  const TfLiteFullyConnectedParams* params,
+                                  const OpData& data,
+                                  const TfLiteEvalTensor* input,
+                                  const TfLiteEvalTensor* filter,
+                                  const TfLiteEvalTensor* bias,
+                                  TfLiteEvalTensor* output) {
+  ops::micro::MliTensorAttachBuffer<int8_t>(input, &data.mli_in);
+  ops::micro::MliTensorAttachBuffer<int8_t>(filter, &data.mli_weights);
+  ops::micro::MliTensorAttachBuffer<int32_t>(bias, &data.mli_bias);
+  ops::micro::MliTensorAttachBuffer<int8_t>(output, &data.mli_out);
+
+  // Tensors for data in fast (local) memory and config to copy data from
+  // external to local memory
+  mli_tensor weights_local = *data.mli_weights.MliTensor();
+  mli_tensor bias_local = *data.mli_bias.MliTensor();
+  mli_tensor in_local = *data.mli_in.MliTensor();
+  mli_tensor out_local = *data.mli_out.MliTensor();
+
+  ops::micro::MliTensorInterface weights_local_interface(&weights_local);
+  ops::micro::MliTensorInterface bias_local_interface(&bias_local);
+  ops::micro::MliTensorInterface in_local_interface(&in_local);
+  ops::micro::MliTensorInterface out_local_interface(&out_local);
+
+  mli_mov_cfg_t copy_config;
+  mli_mov_cfg_for_copy(&copy_config);
+#if defined(MLI_2_0) && !defined(MLI_2_0_KRNL_TEST)
+  const int weight_out_dimension = 1;
+#else
+  const int weight_out_dimension = 0;
+#endif
+  // bias has only 1 dimension
+  const int bias_out_ch_dimension = 0;
+  const int out_tensor_dimension = 1;
+  const int input_size_dimension = 1;
+  int slice_size = data.mli_weights.Shape()[weight_out_dimension];
+
+  /* allocate the local buffers, and compute the slice size */
+  TF_LITE_ENSURE_STATUS(
+      ops::micro::get_arc_scratch_buffer_for_fully_connect_tensors(
+          context, &in_local_interface, &weights_local_interface,
+          &bias_local_interface, &out_local_interface));
+  TF_LITE_ENSURE_STATUS(ops::micro::arc_scratch_buffer_calc_slice_size_weights(
+      &weights_local_interface, &bias_local_interface, weight_out_dimension,
+      &slice_size));
+
+  int max_out_slice_size = *out_local_interface.DataCapacity() /
+                           mli_hlp_tensor_element_size(&out_local);
+
+  if (slice_size > max_out_slice_size) slice_size = max_out_slice_size;
+
+  /* is_local indicates that the tensor is already in local memory,
+     so in that case the original tensor can be used,
+     and there is no need to copy it to the local tensor*/
+  const bool in_is_local =
+      in_local_interface.Data<int8_t>() == data.mli_in.Data<int8_t>();
+  const bool out_is_local =
+      out_local_interface.Data<int8_t>() == data.mli_out.Data<int8_t>();
+  const bool b_is_local =
+      bias_local_interface.Data<int32_t>() == data.mli_bias.Data<int32_t>();
+#ifndef MLI_2_0_KRNL_TEST
+  const bool w_is_local =
+      weights_local_interface.Data<int8_t>() == data.mli_weights.Data<int8_t>();
+#endif
+
+#if defined(MLI_2_0) && !defined(MLI_2_0_KRNL_TEST)
+  ops::micro::TensorSlicer w_slice(data.mli_weights.MliTensor(),
+                                   weight_out_dimension, slice_size, 0, 0, 0,
+                                   true);
+#else
+  ops::micro::TensorSlicer w_slice(data.mli_weights.MliTensor(),
+                                   weight_out_dimension, slice_size);
+#endif
+  ops::micro::TensorSlicer b_slice(data.mli_bias.MliTensor(),
+                                   bias_out_ch_dimension, slice_size);
+  ops::micro::TensorSlicer out_ch_slice(data.mli_out.MliTensor(),
+                                        out_tensor_dimension, slice_size, 0, 0,
+                                        0, true);
+
+#ifdef MLI_2_0_KRNL_TEST
+  mli_tensor* w_ptr = &weights_local;
+#else
+  mli_tensor* w_ptr = w_is_local ? w_slice.Sub() : &weights_local;
+#endif
+  mli_tensor* b_ptr = b_is_local ? b_slice.Sub() : &bias_local;
+
+  void* input_buffer_ptr = NULL;
+
+  while (!w_slice.Done()) {
+#if defined(MLI_2_0) && !defined(MLI_2_0_KRNL_TEST)
+    w_ptr->el_params.sa.scale.mem.pi16 = NULL;
+    b_ptr->el_params.sa.scale.mem.pi16 = NULL;
+#endif
+
+#ifndef MLI_2_0_KRNL_TEST
+    mli_mov_tensor_sync(w_slice.Sub(), &copy_config, w_ptr);
+#endif
+    mli_mov_tensor_sync(b_slice.Sub(), &copy_config, b_ptr);
+
+    // Slice the input over the batches (one at a time with the size of a
+    // complete input)
+    ops::micro::TensorSlicer in_slice(
+        data.mli_in.MliTensor(), input_size_dimension,
+        data.mli_in.Shape()[input_size_dimension]);
+
+    /* output tensor is already sliced in the output size dimension.
+    out_ch_slice.Sub() is the tensor for the amount of output size of this
+    iteration of the weight slice loop. This tensor needs to be further
+    sliced over the batch */
+    ops::micro::TensorSlicer out_slice(out_ch_slice.Sub(), out_tensor_dimension,
+                                       slice_size);
+
+    /* setup the pointers to the local or remote tensor to make the code
+     * inside the loop easier. */
+    mli_tensor* in_ptr = in_is_local ? in_slice.Sub() : &in_local;
+    mli_tensor* out_ptr = out_is_local ? out_slice.Sub() : &out_local;
+
+#ifdef MLI_2_0_KRNL_TEST
+    /* Permute weights tensor to the HWCN layout */
+    // Assertion here to prevent usage non-contiguous buffer memory.
+    if (data.mli_out.Shape()[out_tensor_dimension] !=
+        out_slice.Sub()->shape[0]) {
+      MicroPrintf("Slicing is not supported with real-time permutation.");
+      return kTfLiteError;
+    }
+    mli_permute_cfg permute_cfg = {{1, 0, 2, 3}};
+    ops::micro::permute_weights(data.mli_weights.MliTensor(), &permute_cfg,
+                                w_ptr, &out_ptr->data);
+#endif
+
+    while (!out_slice.Done()) {
+      if (!out_is_local) {
+        ops::micro::PrepareLocalTensor(out_slice.Sub(), &out_local);
+        ops::micro::PrepareLocalTensor(in_slice.Sub(), &in_local);
+      }
+      // if same input copy as previous iteration, skip the copy of input
+#ifdef MLI_2_0
+      if (in_slice.Sub()->data.mem.pi8 != input_buffer_ptr) {
+        mli_mov_tensor_sync(in_slice.Sub(), &copy_config, in_ptr);
+        input_buffer_ptr = in_slice.Sub()->data.mem.pi8;
+      }
+      mli_fully_connected_cfg cfg;
+      cfg.relu.type = MLI_RELU_NONE;
+      mli_krn_fully_connected_sa8_sa8_sa32(in_ptr, w_ptr, b_ptr, &cfg, out_ptr);
+#else
+      if (in_slice.Sub()->data != input_buffer_ptr) {
+        mli_mov_tensor_sync(in_slice.Sub(), &copy_config, in_ptr);
+        input_buffer_ptr = in_slice.Sub()->data;
+      }
+      mli_krn_fully_connected_sa8_sa8_sa32(in_ptr, w_ptr, b_ptr, out_ptr);
+#endif
+
+      mli_mov_tensor_sync(out_ptr, &copy_config, out_slice.Sub());
+
+      in_slice.Next();
+      out_slice.Next();
+    }
+    w_slice.Next();
+    b_slice.Next();
+    out_ch_slice.Next();
+  }
+  return kTfLiteOk;
+}
+
+TfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,
+                           const OpData& data, const TfLiteEvalTensor* input,
+                           const TfLiteEvalTensor* filter,
+                           const TfLiteEvalTensor* bias,
+                           TfLiteEvalTensor* output) {
+#if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+  tflite::FullyConnectedParams op_params;
+  op_params.input_offset = -data.input_zero_point;
+  op_params.weights_offset = -data.filter_zero_point;
+  op_params.output_offset = data.output_zero_point;
+  op_params.output_multiplier = data.output_multiplier;
+  op_params.output_shift = -data.output_shift;
+  op_params.quantized_activation_min = data.output_activation_min;
+  op_params.quantized_activation_max = data.output_activation_max;
+
+  reference_integer_ops::FullyConnected(
+      op_params, tflite::micro::GetTensorShape(input),
+      tflite::micro::GetTensorData<int8_t>(input),
+      tflite::micro::GetTensorShape(filter),
+      tflite::micro::GetTensorData<int8_t>(filter),
+      tflite::micro::GetTensorShape(bias),
+      tflite::micro::GetTensorData<int32_t>(bias),
+      tflite::micro::GetTensorShape(output),
+      tflite::micro::GetTensorData<int8_t>(output));
+  return kTfLiteOk;
+#else
+  MicroPrintf("Node configuration is not supported by ARC MLI Library.");
+  return kTfLiteError;
+#endif
+}
+
+TfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,
+                       TfLiteFusedActivation activation,
+                       const TfLiteEvalTensor* input,
+                       const TfLiteEvalTensor* filter,
+                       const TfLiteEvalTensor* bias, TfLiteEvalTensor* output) {
+#if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+  float output_activation_min, output_activation_max;
+  CalculateActivationRange(activation, &output_activation_min,
+                           &output_activation_max);
+  tflite::FullyConnectedParams op_params;
+  op_params.float_activation_min = output_activation_min;
+  op_params.float_activation_max = output_activation_max;
+  tflite::reference_ops::FullyConnected(
+      op_params, tflite::micro::GetTensorShape(input),
+      tflite::micro::GetTensorData<float>(input),
+      tflite::micro::GetTensorShape(filter),
+      tflite::micro::GetTensorData<float>(filter),
+      tflite::micro::GetTensorShape(bias),
+      tflite::micro::GetTensorData<float>(bias),
+      tflite::micro::GetTensorShape(output),
+      tflite::micro::GetTensorData<float>(output));
+  return kTfLiteOk;
+#else
+  MicroPrintf("Type %s (%d) is not supported by ARC MLI Library.",
+              TfLiteTypeGetName(input->type), input->type);
+  return kTfLiteError;
+#endif
+}
+
+TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
+  TFLITE_DCHECK(node->builtin_data != nullptr);
+  const auto* params =
+      static_cast<const TfLiteFullyConnectedParams*>(node->builtin_data);
+
+  TfLiteEvalTensor* output =
+      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+  const TfLiteEvalTensor* input =
+      tflite::micro::GetEvalInput(context, node, kInputTensor);
+  const TfLiteEvalTensor* filter =
+      tflite::micro::GetEvalInput(context, node, kWeightsTensor);
+  const TfLiteEvalTensor* bias =
+      tflite::micro::GetEvalInput(context, node, kBiasTensor);
+
+  TFLITE_DCHECK(node->user_data != nullptr);
+  const OpData& data = *(static_cast<const OpData*>(node->user_data));
+
+  // Checks in Prepare ensure input, output and filter types are all the same.
+  switch (input->type) {
+    case kTfLiteFloat32:
+      return EvalFloat(context, node, params->activation, input, filter, bias,
+                       output);
+    case kTfLiteInt8:
+      if (data.is_mli_applicable) {
+        return EvalMliQuantizedInt8(context, node, params, data, input, filter,
+                                    bias, output);
+      } else {
+        return EvalQuantized(context, node, data, input, filter, bias, output);
+      }
+
+    default:
+      MicroPrintf("Type %s (%d) not supported.", TfLiteTypeGetName(input->type),
+                  input->type);
+      return kTfLiteError;
+  }
+  return kTfLiteOk;
+}
+
+TFLMRegistration Register_FULLY_CONNECTED() {
+  return tflite::micro::RegisterOp(Init, Prepare, Eval);
+}
+
+}  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/mli_function_specializations.h b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/mli_function_specializations.h
new file mode 100644
index 00000000..6276fe73
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/mli_function_specializations.h
@@ -0,0 +1,141 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "mli_api.h"  // NOLINT
+
+namespace tflite {
+
+// Convolution specialized function.
+typedef mli_status (*conv_func_ptr)(const mli_tensor* /*in*/,
+                                    const mli_tensor* /*weights*/,
+                                    const mli_tensor* /*bias*/,
+                                    const mli_conv2d_cfg* /*cfg*/,
+                                    mli_tensor* /*out*/);
+
+#ifdef MLI_2_0
+conv_func_ptr __attribute__((weak))
+mli_krn_conv2d_hwcn(const mli_tensor* weights) {
+  int filter_w = weights->shape[KRNL_W_DIM_HWCN];
+  int filter_h = weights->shape[KRNL_H_DIM_HWCN];
+
+  if (filter_w == 1 && filter_h == 1) {
+    return mli_krn_conv2d_hwcn_sa8_sa8_sa32_k1x1;
+  } else if (filter_w == 3 && filter_h == 3) {
+    return mli_krn_conv2d_hwcn_sa8_sa8_sa32_k3x3;
+  } else if (filter_w == 5 && filter_h == 5) {
+    return mli_krn_conv2d_hwcn_sa8_sa8_sa32_k5x5;
+  } else {
+    return mli_krn_conv2d_hwcn_sa8_sa8_sa32;
+  }
+}
+#else
+conv_func_ptr __attribute__((weak))
+mli_krn_conv2d_hwcn(const mli_tensor* weights, const mli_conv2d_cfg* cfg) {
+  return mli_krn_conv2d_nhwc_sa8_sa8_sa32;
+}
+#endif
+
+// Depthwise convolution specialized function.
+typedef mli_status (*depthwise_func_ptr)(const mli_tensor* /*in*/,
+                                         const mli_tensor* /*weights*/,
+                                         const mli_tensor* /*bias*/,
+                                         const mli_conv2d_cfg* /*cfg*/,
+                                         mli_tensor* /*out*/);
+
+#ifdef MLI_2_0
+depthwise_func_ptr __attribute__((weak))
+mli_krn_depthwise_conv2d(const mli_tensor* weights) {
+  int filter_w = weights->shape[KRNL_DW_W_DIM_HW1N];
+  int filter_h = weights->shape[KRNL_DW_H_DIM_HW1N];
+
+  if (filter_w == 3 && filter_h == 3) {
+    return mli_krn_depthwise_conv2d_hwcn_sa8_sa8_sa32_k3x3;
+  } else if (filter_w == 5 && filter_h == 5) {
+    return mli_krn_depthwise_conv2d_hwcn_sa8_sa8_sa32_k5x5;
+  } else {
+    return mli_krn_depthwise_conv2d_hwcn_sa8_sa8_sa32;
+  }
+}
+#else
+depthwise_func_ptr __attribute__((weak))
+mli_krn_depthwise_conv2d(const mli_tensor* weights, const mli_conv2d_cfg* cfg) {
+  return mli_krn_depthwise_conv2d_hwcn_sa8_sa8_sa32;
+}
+#endif
+
+#ifdef MLI_2_0
+depthwise_func_ptr __attribute__((weak))
+mli_krn_group_conv2d(const mli_tensor* weights) {
+  int filter_w = weights->shape[KRNL_DW_W_DIM_HW1N];
+  int filter_h = weights->shape[KRNL_DW_H_DIM_HW1N];
+
+  if (filter_w == 3 && filter_h == 3) {
+    return mli_krn_group_conv2d_hwcn_sa8_sa8_sa32_k3x3;
+  } else if (filter_w == 5 && filter_h == 5) {
+    return mli_krn_group_conv2d_hwcn_sa8_sa8_sa32_k5x5;
+  } else {
+    return mli_krn_group_conv2d_hwcn_sa8_sa8_sa32;
+  }
+}
+#endif
+
+// Pooling specialized functions.
+typedef mli_status (*pooling_func_ptr)(const mli_tensor* /*in*/,
+                                       const mli_pool_cfg* /*cfg*/,
+                                       mli_tensor* /*out*/);
+
+#ifdef MLI_2_0
+pooling_func_ptr __attribute__((weak))
+mli_krn_avepool(const mli_pool_cfg* cfg) {
+  int filter_w = cfg->kernel_width;
+  int filter_h = cfg->kernel_height;
+
+  if (filter_w == 2 && filter_h == 2) {
+    return mli_krn_avepool_hwc_sa8_k2x2;
+  } else if (filter_w == 3 && filter_h == 3) {
+    return mli_krn_avepool_hwc_sa8_k3x3;
+  } else {
+    return mli_krn_avepool_hwc_sa8;
+  }
+}
+#else
+pooling_func_ptr __attribute__((weak))
+mli_krn_avepool(const mli_pool_cfg* cfg) {
+  return mli_krn_avepool_hwc_sa8;
+}
+#endif
+
+#ifdef MLI_2_0
+pooling_func_ptr __attribute__((weak))
+mli_krn_maxpool(const mli_pool_cfg* cfg) {
+  int filter_w = cfg->kernel_width;
+  int filter_h = cfg->kernel_height;
+
+  if (filter_w == 2 && filter_h == 2) {
+    return mli_krn_maxpool_hwc_sa8_k2x2;
+  } else if (filter_w == 3 && filter_h == 3) {
+    return mli_krn_maxpool_hwc_sa8_k3x3;
+  } else {
+    return mli_krn_maxpool_hwc_sa8;
+  }
+}
+#else
+pooling_func_ptr __attribute__((weak))
+mli_krn_maxpool(const mli_pool_cfg* cfg) {
+  return mli_krn_maxpool_hwc_sa8;
+}
+#endif
+
+}  // namespace tflite
\ No newline at end of file
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/mli_interface.cc b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/mli_interface.cc
new file mode 100644
index 00000000..3a9890b2
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/mli_interface.cc
@@ -0,0 +1,155 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "mli_interface.h"  // NOLINT
+
+#include <math.h>
+
+#include "tensorflow/lite/micro/micro_log.h"
+
+namespace tflite {
+namespace ops {
+namespace micro {
+
+#ifndef MLI_2_0
+template <>
+int8_t* MliTensorInterface::Data<int8_t>(void) {
+  TFLITE_DCHECK(tensor_->el_type == MLI_EL_ASYM_I8);
+  return static_cast<int8_t*>(tensor_->data);
+}
+
+template <>
+int32_t* MliTensorInterface::Data<int32_t>(void) {
+  TFLITE_DCHECK(tensor_->el_type == MLI_EL_ASYM_I32);
+  return static_cast<int32_t*>(tensor_->data);
+}
+
+template <>
+int32_t* MliTensorInterface::Scale(void) {
+  return &tensor_->el_params.asym.scale.i32;
+}
+
+template <>
+int32_t** MliTensorInterface::Scale(void) {
+  return &tensor_->el_params.asym.scale.pi32;
+}
+
+template <>
+void MliTensorInterface::SetData(int8_t* data, uint32_t capacity) const {
+  TFLITE_DCHECK(tensor_->el_type == MLI_EL_ASYM_I8);
+  tensor_->data = data;
+  tensor_->capacity = capacity;
+}
+
+template <>
+void MliTensorInterface::SetData(int32_t* data, uint32_t capacity) const {
+  TFLITE_DCHECK(tensor_->el_type == MLI_EL_ASYM_I32);
+  tensor_->data = data;
+  tensor_->capacity = capacity;
+}
+
+mli_tensor* MliTensorInterface::MliTensor(void) { return tensor_; }
+
+const mli_tensor* MliTensorInterface::MliTensor(void) const {
+  return static_cast<const mli_tensor*>(
+      const_cast<MliTensorInterface*>(this)->MliTensor());
+}
+
+uint32_t* MliTensorInterface::Rank(void) { return &tensor_->rank; }
+
+const uint32_t* MliTensorInterface::DataCapacity(void) const {
+  return &tensor_->capacity;
+}
+
+mli_element_type* MliTensorInterface::ElType(void) { return &tensor_->el_type; }
+
+template <>
+int16_t* MliTensorInterface::ZeroPoint(void) {
+  return &tensor_->el_params.asym.zero_point.i16;
+}
+
+template <>
+int16_t** MliTensorInterface::ZeroPoint(void) {
+  return &tensor_->el_params.asym.zero_point.pi16;
+}
+
+uint32_t* MliTensorInterface::ZeroPointCapacity(void) { return nullptr; }
+
+int32_t* MliTensorInterface::Dim(void) { return &tensor_->el_params.asym.dim; }
+
+uint32_t* MliTensorInterface::ScaleCapacity(void) { return nullptr; }
+
+template <>
+int8_t* MliTensorInterface::ScaleFracBits(void) {
+  return &tensor_->el_params.asym.scale_frac_bits;
+}
+
+uint32_t* MliTensorInterface::ScaleFracBitsCapacity(void) { return nullptr; }
+
+int32_t* MliTensorInterface::MemStride(void) { return tensor_->mem_stride; }
+
+uint32_t* MliTensorInterface::Shape(void) { return tensor_->shape; }
+
+const uint32_t* MliTensorInterface::Shape(void) const {
+  return static_cast<const uint32_t*>(
+      const_cast<MliTensorInterface*>(this)->Shape());
+}
+
+void MliTensorInterface::SetScale(float fscale) {
+  int exp;
+  frexpf(fscale, &exp);
+  int frac_bits = 31 - exp;
+  int32_t iscale = (int32_t)((1ll << frac_bits) * fscale + 0.5f);
+  *(this->ScaleFracBits<int8_t*>()) = frac_bits;
+  *(this->Scale<int32_t*>()) = (int32_t)iscale;
+}
+
+void MliTensorInterface::SetScalePerChannel(float* fscale,
+                                            const int num_channels) {
+  int min_frac_bits;
+  for (int i = 0; i < num_channels; i++) {
+    int exp;
+    frexpf(fscale[i], &exp);
+    int cur_frac_bits = 31 - exp;
+    if (i == 0) {
+      min_frac_bits = cur_frac_bits;
+    } else {
+      min_frac_bits =
+          min_frac_bits < cur_frac_bits ? min_frac_bits : cur_frac_bits;
+    }
+  }
+  *this->ScaleFracBits<int8_t*>() = min_frac_bits;
+
+  for (int i = 0; i < num_channels; i++) {
+    int32_t iscale = (int32_t)((1ll << min_frac_bits) * fscale[i] + 0.5f);
+    (*this->Scale<int32_t**>())[i] = iscale;
+  }
+}
+
+void MliTensorInterface::SetElType(TfLiteType type) {
+  if (type == kTfLiteInt8) {
+    *this->ElType() = MLI_EL_ASYM_I8;
+  } else if (type == kTfLiteInt32) {
+    *this->ElType() = MLI_EL_ASYM_I32;
+  } else {
+    MicroPrintf("Wrong data type. Expected int8_t or int32_t.");
+    TFLITE_ABORT;
+  }
+}
+#endif
+
+}  // namespace micro
+}  // namespace ops
+}  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/mli_interface.h b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/mli_interface.h
new file mode 100644
index 00000000..b4087f3b
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/mli_interface.h
@@ -0,0 +1,75 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_MICRO_KERNELS_ARC_MLI_INTERFACE_H_
+#define TENSORFLOW_LITE_MICRO_KERNELS_ARC_MLI_INTERFACE_H_
+
+#include "mli_api.h"  // NOLINT
+#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
+namespace tflite {
+namespace ops {
+namespace micro {
+
+// Abstracts access to mli_tensor fields to use different versions of MLI
+// Library (1.x and 2.x)
+// Example:
+//    ops::micro::MliTensorInterface mli_in =
+//    ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+//        context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+
+class MliTensorInterface {
+ public:
+  // Make sure that lifetime of MliTensorInterface instance isn't bigger than
+  // related mli_tensor.
+  MliTensorInterface(mli_tensor* tensor) : tensor_(tensor){};
+  MliTensorInterface() = default;
+  ~MliTensorInterface() = default;
+
+  template <typename T>
+  T* Data();
+  template <typename T>
+  T Scale();
+  template <typename T>
+  T ZeroPoint();
+  template <typename T>
+  T ScaleFracBits();
+  mli_tensor* MliTensor();
+  const mli_tensor* MliTensor() const;
+  int32_t* Dim();
+  uint32_t* Rank();
+  uint32_t* Shape();
+  const uint32_t* Shape() const;
+  const uint32_t* DataCapacity() const;
+  uint32_t* ScaleCapacity();
+  mli_element_type* ElType();
+  uint32_t* ScaleFracBitsCapacity();
+  int32_t* MemStride();
+  uint32_t* ZeroPointCapacity();
+
+  template <typename T>
+  void SetData(T* data, uint32_t capacity) const;
+  void SetScale(float fscale);
+  void SetScalePerChannel(float* fscale, const int num_channels);
+  void SetElType(TfLiteType type);
+
+ private:
+  mli_tensor* tensor_;
+};
+
+}  // namespace micro
+}  // namespace ops
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_MICRO_KERNELS_ARC_MLI_SLICERS_H_
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/mli_interface_mli_20.cc b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/mli_interface_mli_20.cc
new file mode 100644
index 00000000..cef2a6e9
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/mli_interface_mli_20.cc
@@ -0,0 +1,164 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <math.h>
+
+#include "mli_interface.h"  // NOLINT
+#include "tensorflow/lite/micro/micro_log.h"
+
+namespace tflite {
+namespace ops {
+namespace micro {
+
+#ifdef MLI_2_0
+template <>
+int8_t* MliTensorInterface::Data(void) {
+  TFLITE_DCHECK(tensor_->el_type == MLI_EL_SA_8);
+  return tensor_->data.mem.pi8;
+}
+
+template <>
+int32_t* MliTensorInterface::Data(void) {
+  TFLITE_DCHECK(tensor_->el_type == MLI_EL_SA_32);
+  return tensor_->data.mem.pi32;
+}
+
+template <>
+int16_t** MliTensorInterface::Scale(void) {
+  return &tensor_->el_params.sa.scale.mem.pi16;
+}
+
+template <>
+int16_t* MliTensorInterface::Scale(void) {
+  return &tensor_->el_params.sa.scale.mem.i16;
+}
+
+template <>
+void MliTensorInterface::SetData(int8_t* data, uint32_t capacity) const {
+  TFLITE_DCHECK(tensor_->el_type == MLI_EL_SA_8);
+  tensor_->data.mem.pi8 = data;
+  tensor_->data.capacity = capacity;
+}
+
+template <>
+void MliTensorInterface::SetData(int32_t* data, uint32_t capacity) const {
+  TFLITE_DCHECK(tensor_->el_type == MLI_EL_SA_32);
+  tensor_->data.mem.pi32 = data;
+  tensor_->data.capacity = capacity;
+}
+
+mli_tensor* MliTensorInterface::MliTensor(void) { return tensor_; }
+
+const mli_tensor* MliTensorInterface::MliTensor(void) const {
+  return static_cast<const mli_tensor*>(
+      const_cast<MliTensorInterface*>(this)->MliTensor());
+}
+
+uint32_t* MliTensorInterface::Rank(void) { return &tensor_->rank; }
+
+const uint32_t* MliTensorInterface::DataCapacity(void) const {
+  return &tensor_->data.capacity;
+}
+
+mli_element_type* MliTensorInterface::ElType(void) { return &tensor_->el_type; }
+
+template <>
+int16_t* MliTensorInterface::ZeroPoint(void) {
+  return &tensor_->el_params.sa.zero_point.mem.i16;
+}
+
+template <>
+int16_t** MliTensorInterface::ZeroPoint(void) {
+  return &tensor_->el_params.sa.zero_point.mem.pi16;
+}
+
+uint32_t* MliTensorInterface::ZeroPointCapacity(void) {
+  return &tensor_->el_params.sa.zero_point.capacity;
+}
+
+int32_t* MliTensorInterface::Dim(void) { return &tensor_->el_params.sa.dim; }
+
+uint32_t* MliTensorInterface::ScaleCapacity(void) {
+  return &tensor_->el_params.sa.scale.capacity;
+}
+
+template <>
+int8_t** MliTensorInterface::ScaleFracBits(void) {
+  return &tensor_->el_params.sa.scale_frac_bits.mem.pi8;
+}
+
+template <>
+int8_t* MliTensorInterface::ScaleFracBits(void) {
+  return &tensor_->el_params.sa.scale_frac_bits.mem.i8;
+}
+
+uint32_t* MliTensorInterface::ScaleFracBitsCapacity(void) {
+  return &tensor_->el_params.sa.scale_frac_bits.capacity;
+}
+
+int32_t* MliTensorInterface::MemStride(void) { return tensor_->mem_stride; }
+
+uint32_t* MliTensorInterface::Shape(void) { return tensor_->shape; }
+
+const uint32_t* MliTensorInterface::Shape(void) const {
+  return static_cast<const uint32_t*>(
+      const_cast<MliTensorInterface*>(this)->Shape());
+}
+
+void MliTensorInterface::SetScale(float fscale) {
+  int exp;
+  frexpf(fscale, &exp);
+  int frac_bits = 15 - exp;
+  int16_t iscale = (int16_t)((1ll << frac_bits) * fscale + 0.5f);
+  *(this->Scale<int16_t*>()) = (int16_t)iscale;
+  *(this->ScaleFracBits<int8_t*>()) = frac_bits;
+  *this->ScaleCapacity() = 0;
+  *this->ScaleFracBitsCapacity() = 0;
+}
+
+void MliTensorInterface::SetScalePerChannel(float* fscale,
+                                            const int num_channels) {
+  for (int i = 0; i < num_channels; i++) {
+    int exp;
+    frexpf(fscale[i], &exp);
+    int cur_frac_bits = 15 - exp;
+    (*this->ScaleFracBits<int8_t**>())[i] = cur_frac_bits;
+  }
+
+  for (int i = 0; i < num_channels; i++) {
+    int16_t iscale =
+        (int16_t)((1ll << (*this->ScaleFracBits<int8_t**>())[i]) * fscale[i] +
+                  0.5f);
+    (*this->Scale<int16_t**>())[i] = iscale;
+  }
+  *this->ScaleCapacity() = num_channels * sizeof(int16_t);
+  *this->ScaleFracBitsCapacity() = num_channels * sizeof(int8_t);
+}
+
+void MliTensorInterface::SetElType(TfLiteType type) {
+  if (type == kTfLiteInt8) {
+    *this->ElType() = MLI_EL_SA_8;
+  } else if (type == kTfLiteInt32) {
+    *this->ElType() = MLI_EL_SA_32;
+  } else {
+    MicroPrintf("Wrong data type. Expected int8_t or int32_t.");
+    TFLITE_ABORT;
+  }
+}
+#endif
+
+}  // namespace micro
+}  // namespace ops
+}  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/mli_slicers.cc b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/mli_slicers.cc
new file mode 100644
index 00000000..905c6fed
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/mli_slicers.cc
@@ -0,0 +1,126 @@
+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "mli_slicers.h"  // NOLINT
+
+#include <algorithm>
+
+namespace tflite {
+namespace ops {
+namespace micro {
+
+TensorSlicer::TensorSlicer(const mli_tensor* full_tensor, int slice_dim,
+                           int slice_size, int padding_pre, int padding_post,
+                           int overlap, bool interleave_mode)
+    : full_tensor_(full_tensor),
+      sub_tensor_{},
+      sub_cfg_{},
+      done_(false),
+      sliceDim_(slice_dim),
+      pad_pre_(padding_pre),
+      pad_post_(padding_post),
+      overlap_(overlap) {
+  /* In the interleave mode, the slicing happens from the deepest dimension up
+  to the slice_dim for example in an HWC layout this can mode can be used to
+  slice in the C dimenstion. in this mode the data is not contiguous in memory
+  anymore */
+  if (interleave_mode) {
+    for (int i = 0; i < static_cast<int>(full_tensor->rank); i++) {
+      if (i > slice_dim) {
+        sub_cfg_.size[i] = 1;
+      } else if (i == slice_dim) {
+        sub_cfg_.size[i] = slice_size;
+      } else {
+        sub_cfg_.size[i] = full_tensor->shape[i];
+      }
+    }
+    sub_cfg_.sub_tensor_rank = full_tensor->rank;
+
+  } else {
+    /* In the not interleaved mode, the slicing happens from the outer most
+    dimension up to the slice_dim for example in an HWC layout this mode can be
+    used to slice in the H dimension. in this mode the data of the slice is
+    still contiguous in memory (if that was the case in the input tensor */
+    for (int i = 0; i < static_cast<int>(full_tensor->rank); i++) {
+      if (i < slice_dim) {
+        sub_cfg_.size[i] = 1;
+      } else if (i == slice_dim) {
+        sub_cfg_.size[i] = slice_size;
+      } else {
+        sub_cfg_.size[i] = full_tensor->shape[i];
+      }
+    }
+    sub_cfg_.sub_tensor_rank = full_tensor->rank - slice_dim;
+  }
+
+  ComputeSubTensor();
+}
+
+void TensorSlicer::ComputeSubTensor(void) {
+  // subtsr_cfg_ is used to keep track of the iteration.
+  // A copy is created to update it with the correct clipping and padding for
+  // the current slice
+  mli_sub_tensor_cfg cfg_new = sub_cfg_;
+
+  // begin and end spans the complete input region including padding areas.
+  const int begin = (int)sub_cfg_.offset[sliceDim_] - pad_pre_;
+  // end is clipped to the end of the full input region. this is needed for
+  // cases where the last slice is smaller than the rest.
+  const int end = std::min(begin + sub_cfg_.size[sliceDim_] + overlap_,
+                           full_tensor_->shape[sliceDim_] + pad_post_);
+  // The start coordinate of the subtensor is clipped to zero
+  cfg_new.offset[sliceDim_] = std::max(begin, 0);
+  // and the stop coordinate is clipped to the size of the full tensor
+  const int stop_coord =
+      std::min(end, static_cast<int>(full_tensor_->shape[sliceDim_]));
+  // compute the size of the subtensor
+  cfg_new.size[sliceDim_] = stop_coord - cfg_new.offset[sliceDim_];
+
+  // compute the padding configuration for the current slice.
+  actual_padding_pre = cfg_new.offset[sliceDim_] - begin;
+  actual_padding_post = end - stop_coord;
+
+  mli_hlp_create_subtensor(full_tensor_, &cfg_new, &sub_tensor_);
+}
+
+void TensorSlicer::Next(void) {
+  for (int i = full_tensor_->rank - 1; i >= 0; i--) {
+    sub_cfg_.offset[i] += sub_cfg_.size[i];
+    if (sub_cfg_.offset[i] >= full_tensor_->shape[i]) {
+      // wrap
+      sub_cfg_.offset[i] = 0;
+      // and continue to the next dimension, if no next dimension we are done.
+      if (i == 0) done_ = true;
+      continue;
+    } else {
+      // carry is false, so break from the loop
+      break;
+    }
+  }
+
+  if (!done_) ComputeSubTensor();
+}
+
+bool TensorSlicer::Done(void) { return done_; }
+
+int TensorSlicer::GetPaddingPre(void) { return actual_padding_pre; }
+
+int TensorSlicer::GetPaddingPost(void) { return actual_padding_post; }
+
+mli_tensor* TensorSlicer::Sub(void) { return &sub_tensor_; }
+
+}  // namespace micro
+}  // namespace ops
+}  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/mli_slicers.h b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/mli_slicers.h
new file mode 100644
index 00000000..b21a5b68
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/mli_slicers.h
@@ -0,0 +1,56 @@
+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_MICRO_KERNELS_ARC_MLI_SLICERS_H_
+#define TENSORFLOW_LITE_MICRO_KERNELS_ARC_MLI_SLICERS_H_
+
+#include "mli_api.h"  // NOLINT
+namespace tflite {
+namespace ops {
+namespace micro {
+
+class TensorSlicer {
+ public:
+  TensorSlicer(const mli_tensor* full_tensor, int slice_dim, int slice_size,
+               int padding_pre = 0, int padding_post = 0, int overlap = 0,
+               bool interleave_mode = false);
+  ~TensorSlicer() = default;
+
+  void Next();
+  bool Done();
+  int GetPaddingPre();
+  int GetPaddingPost();
+
+  mli_tensor* Sub();
+
+  // Default constructor is deleted
+  TensorSlicer() = delete;
+
+ private:
+  const mli_tensor* full_tensor_;
+  mli_tensor sub_tensor_;
+  mli_sub_tensor_cfg sub_cfg_;
+  bool done_;
+  int sliceDim_;
+  int pad_pre_, pad_post_, overlap_;
+  int actual_padding_pre, actual_padding_post;
+
+  void ComputeSubTensor();
+};
+
+}  // namespace micro
+}  // namespace ops
+}  // namespace tflite
+#endif  // TENSORFLOW_LITE_MICRO_KERNELS_ARC_MLI_SLICERS_H_
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/mli_tf_utils.h b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/mli_tf_utils.h
new file mode 100644
index 00000000..6e4e16e8
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/mli_tf_utils.h
@@ -0,0 +1,310 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_MICRO_KERNELS_ARC_MLI_TF_UTILS_H_
+#define TENSORFLOW_LITE_MICRO_KERNELS_ARC_MLI_TF_UTILS_H_
+
+#include "mli_api.h"  // NOLINT
+#include "mli_interface.h"
+#include "tensorflow/lite/kernels/internal/common.h"
+#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
+#include "tensorflow/lite/micro/kernels/kernel_util.h"
+#include "tensorflow/lite/micro/micro_log.h"
+
+#define KRNL_C_DIM_NHWC 0  // output channels
+
+namespace tflite {
+namespace ops {
+namespace micro {
+
+inline void ConvertToMliTensorData(const TfLiteTensor* tfT,
+                                   MliTensorInterface* mliT,
+                                   bool is_bias_tensor) {
+  // Data is NULL until MliTensorAttachBuffer is called.
+  mliT->SetElType(tfT->type);
+  if (tfT->type == kTfLiteInt8) {
+    mliT->SetData<int8_t>(nullptr, tfT->bytes);
+  } else if (tfT->type == kTfLiteInt32) {
+    mliT->SetData<int32_t>(nullptr, tfT->bytes);
+  } else {
+    MicroPrintf("Wrong data type. Expected int8_t or int32_t.");
+    TFLITE_ABORT;
+  }
+  const int32_t dims_count = GetTensorShape(tfT).DimensionsCount();
+  *mliT->Rank() = is_bias_tensor ? 1 : dims_count;
+
+  int mli_tensor_memstride = 1;
+  if (is_bias_tensor) {
+    mliT->Shape()[0] = GetTensorShape(tfT).Dims(dims_count - 1);
+    mliT->MemStride()[0] = mli_tensor_memstride;
+  } else {
+    for (int i = dims_count - 1; i >= 0; --i) {
+      mliT->Shape()[i] = GetTensorShape(tfT).Dims(i);
+      mliT->MemStride()[i] = mli_tensor_memstride;
+      mli_tensor_memstride *= GetTensorShape(tfT).Dims(i);
+    }
+  }
+}
+
+inline void ConvertToMliQuantParams(const TfLiteTensor* tfT,
+                                    MliTensorInterface* mliT) {
+  *mliT->Dim() = -1;
+#ifdef MLI_2_0
+  *mliT->ZeroPointCapacity() = 0;
+#endif
+  *mliT->ZeroPoint<int16_t*>() = tfT->params.zero_point;
+  float fscale = tfT->params.scale;
+  mliT->SetScale(fscale);
+}
+
+inline void ConvertToMliQuantParamsPerChannel(const TfLiteTensor* tfT,
+                                              MliTensorInterface* mliT,
+                                              bool is_bias_tensor) {
+  // mli tensor scale and zero_point arrays should be allocated at this point
+#ifdef MLI_2_0
+  TFLITE_DCHECK_NE(*mliT->Scale<int16_t**>(), 0);
+  TFLITE_DCHECK_NE(*mliT->ZeroPoint<int16_t**>(), 0);
+#else
+  TFLITE_DCHECK_NE(*mliT->Scale<int32_t**>(), 0);
+  TFLITE_DCHECK_NE(*mliT->ZeroPoint<int16_t**>(), 0);
+#endif
+
+  // get per channel quantization parameters
+  const auto* affine_quantization =
+      reinterpret_cast<TfLiteAffineQuantization*>(tfT->quantization.params);
+  int32_t quantized_dimension =
+      is_bias_tensor ? 0 : affine_quantization->quantized_dimension;
+  const int num_channels = mliT->Shape()[quantized_dimension];
+
+  *mliT->Dim() = quantized_dimension;
+
+  // set capacities
+#ifdef MLI_2_0
+  *mliT->ScaleFracBitsCapacity() = num_channels * sizeof(int8_t);
+  *mliT->ScaleCapacity() = num_channels * sizeof(int16_t);
+  *mliT->ZeroPointCapacity() = num_channels * sizeof(int16_t);
+#endif
+  float* fscale = affine_quantization->scale->data;
+  mliT->SetScalePerChannel(fscale, num_channels);
+
+#ifdef MLI_2_0
+  int16_t* zero_point = *mliT->ZeroPoint<int16_t**>();
+  for (int i = 0; i < num_channels; i++) {
+    zero_point[i] = tfT->params.zero_point;
+  }
+#endif
+}
+
+template <typename datatype>
+inline void MliTensorAttachBuffer(const TfLiteEvalTensor*,
+                                  const MliTensorInterface*);
+
+template <>
+inline void MliTensorAttachBuffer<int8_t>(const TfLiteEvalTensor* tfT,
+                                          const MliTensorInterface* mliT) {
+  // "const_cast" here used to attach const data buffer to the initially
+  // non-const mli_tensor. This is required by current implementation of MLI
+  // backend and planned for redesign due to this and some other aspects.
+  mliT->SetData<int8_t>(
+      const_cast<int8_t*>(tflite::micro::GetTensorData<int8_t>(tfT)),
+      *mliT->DataCapacity());
+}
+
+template <>
+inline void MliTensorAttachBuffer<int32_t>(const TfLiteEvalTensor* tfT,
+                                           const MliTensorInterface* mliT) {
+  // "const_cast" here used to attach const data buffer to the initially
+  // non-const mli_tensor. This is required by current implementation of MLI
+  // backend and planned for redesign due to this and some other aspects.
+  mliT->SetData<int32_t>(
+      const_cast<int32_t*>(tflite::micro::GetTensorData<int32_t>(tfT)),
+      *mliT->DataCapacity());
+}
+
+inline void ConvertToMliTensor(const TfLiteTensor* tfT,
+                               MliTensorInterface* mliT) {
+  ConvertToMliTensorData(tfT, mliT, false);
+  ConvertToMliQuantParams(tfT, mliT);
+}
+
+inline void ConvertToMliTensorPerChannel(const TfLiteTensor* tfT,
+                                         MliTensorInterface* mliT,
+                                         bool is_bias_tensor) {
+  ConvertToMliTensorData(tfT, mliT, is_bias_tensor);
+  ConvertToMliQuantParamsPerChannel(tfT, mliT, is_bias_tensor);
+}
+
+inline void PrepareLocalTensor(mli_tensor* tensor, mli_tensor* tensor_local) {
+#ifdef MLI_2_0
+  int8_t* local_data = tensor_local->data.mem.pi8;
+  *tensor_local = *tensor;
+  tensor_local->data.mem.pi8 = local_data;
+#else
+  int8_t* local_data = static_cast<int8_t*>(tensor_local->data);
+  *tensor_local = *tensor;
+  tensor_local->data = local_data;
+#endif
+}
+
+inline void AdjustBiasTensor(MliTensorInterface* bias, MliTensorInterface* in,
+                             MliTensorInterface* weights) {
+  int32_t quantized_dimension = *bias->Dim();
+  const int num_channels =
+      quantized_dimension < 0 ? 1 : bias->Shape()[quantized_dimension];
+  for (int i = 0; i < num_channels; i++) {
+    int32_t adjusted_bias_scale =
+        (*in->Scale<int16_t*>()) * (*weights->Scale<int16_t**>())[i];
+    int in_shift = *in->ScaleFracBits<int8_t*>();
+    int w_shift = (*weights->ScaleFracBits<int8_t**>())[i];
+    int b_shift = (*bias->ScaleFracBits<int8_t**>())[i];
+    int bias_shift = in_shift + w_shift - b_shift;
+    (*bias->Scale<int16_t**>())[i] =
+        (int16_t)(adjusted_bias_scale >> bias_shift);
+  }
+}
+
+#ifdef MLI_2_0_KRNL_TEST
+// Reorder an array according to given indexes. If backward is true, order of
+// index array must be reversed.
+inline static void reorder(uint32_t* arr, const uint8_t index[],
+                           bool backward) {
+  uint32_t temp[MLI_MAX_RANK];
+  for (int8_t i = 0; i < MLI_MAX_RANK; i++) {
+    if (backward)
+      temp[index[i]] = arr[i];
+    else
+      temp[i] = arr[index[i]];
+  }
+  for (int8_t i = 0; i < MLI_MAX_RANK; i++) {
+    arr[i] = temp[i];
+  }
+}
+
+// Change shape of mli tensor and recalculate mem strides.
+inline void change_shape(mli_tensor* mliT, const uint8_t dim_order[]) {
+  reorder(mliT->shape, dim_order, false);
+
+  // Calculate strides for new layout
+  int mli_tensor_memstride = 1;
+  for (int shape_idx = mliT->rank - 1; shape_idx >= 0; --shape_idx) {
+    mliT->mem_stride[shape_idx] = mli_tensor_memstride;
+    mli_tensor_memstride *= mliT->shape[shape_idx];
+  }
+}
+
+inline void permute_weights(const mli_tensor* weights_src,
+                            const mli_permute_cfg* permute_cfg,
+                            mli_tensor* weights_dst,
+                            mli_data_container* buffer_data) {
+  mli_tensor buffer = {};
+  buffer.el_params = weights_dst->el_params;
+  buffer.data = *buffer_data;
+  // Compare weights tensor size and avaliable buffer capacity.
+  int buffer_size = buffer_data->capacity;
+  int weights_size = mli_hlp_count_elem_num(weights_src, 0) *
+                     mli_hlp_tensor_element_size(weights_src);
+
+  // Need to change shape of distanation weights buffer according to permute
+  // dimensions order to calculate slice sizes
+  change_shape(weights_dst, permute_cfg->perm_dim);
+
+  if (buffer_size >= weights_size) {
+    mli_mov_cfg_t copy_config;
+    mli_mov_cfg_for_copy(&copy_config);
+    mli_mov_tensor_sync(weights_src, &copy_config, &buffer);
+    mli_krn_permute_sa8(&buffer, permute_cfg, weights_dst);
+  } else {
+    // Weights shape is NHWC and output (buffer) shape is HWC where N_w = C_o.
+    // Buffer size (H_o * W_o) must be more or equal then the weights size (H_w
+    // * W_w * C_w). So, this is the reason, why buffer size (output tensor) is
+    // divided by channel shape.
+    uint32_t slice_size = buffer_size / weights_src->shape[KRNL_C_DIM_NHWC];
+
+    mli_mov_cfg_t copy_config = {};
+    uint32_t src_offsets[] = {0, 0, 0, 0};
+    uint32_t src_sizes[] = {0, 0, 0, 0};
+    int dst_mem_stride[] = {0, 0, 0, 0};
+
+    mli_tensor weights_dst_sub_tensor;
+    mli_sub_tensor_cfg sub_tensor_cfg = {};
+    sub_tensor_cfg.sub_tensor_rank = weights_src->rank;
+
+    // Calculate dimensions for slice accroding to buffer capacity.
+    // Now, after calling change_shape() function, dst weights buffer has the
+    // MLI layout (HWCN). This means, the innermost dimension (N) of dst weights
+    // tensor is equal to the innermost dimension of output tensor (N).
+    sub_tensor_cfg.size[weights_dst->rank - 1] =
+        src_sizes[weights_dst->rank - 1] = weights_src->shape[KRNL_C_DIM_NHWC];
+    // Now need to calculate other shapes for weights slice. Total slice size is
+    // H*W*C*N, so to calculate sizes for each axis, avaliable slice size is
+    // divided by shape for each axis.
+    uint32_t slice_size_left = slice_size;
+    for (uint32_t i = 0; i < weights_dst->rank - 1; i++) {
+      sub_tensor_cfg.size[i] = src_sizes[i] =
+          slice_size_left / weights_dst->shape[i] > 0 ? weights_dst->shape[i]
+                                                      : slice_size_left;
+      slice_size_left /= weights_dst->shape[i];
+      slice_size_left = slice_size_left > 0 ? slice_size_left : 1;
+    }
+    // Need to reorder src tensor sizes because it is still in TFLM format
+    // (NHWC) and src_sizes array calculated as (HWCN).
+    reorder(src_sizes, permute_cfg->perm_dim, true);
+
+    sub_tensor_cfg.offset[KRNL_C_DIM_HWCN] = src_offsets[KRNL_H_DIM_HWCN] = 0;
+    sub_tensor_cfg.offset[KRNL_H_DIM_HWCN] = src_offsets[KRNL_W_DIM_HWCN] = 0;
+    sub_tensor_cfg.offset[KRNL_W_DIM_HWCN] = src_offsets[KRNL_D_DIM_HWCN] = 0;
+    sub_tensor_cfg.offset[KRNL_D_DIM_HWCN] = src_offsets[KRNL_C_DIM_HWCN] = 0;
+    do {
+      do {
+        do {
+          do {
+            mli_mov_cfg_for_slice(&copy_config, (int*)src_offsets,
+                                  (int*)src_sizes, dst_mem_stride);
+            mli_mov_tensor_sync(weights_src, &copy_config, &buffer);
+
+            mli_hlp_create_subtensor(weights_dst, &sub_tensor_cfg,
+                                     &weights_dst_sub_tensor);
+            mli_krn_permute_sa8(&buffer, permute_cfg, &weights_dst_sub_tensor);
+
+            // For each axis, it is necessary to recalculate the offsets and
+            // slice sizes.
+            sub_tensor_cfg.offset[2] = src_offsets[3] += src_sizes[3];
+            src_sizes[3] =
+                std::min(src_sizes[3], weights_src->shape[3] - src_offsets[3]);
+          } while (src_offsets[3] < weights_src->shape[3]);
+
+          sub_tensor_cfg.offset[1] = src_offsets[2] += src_sizes[2];
+          src_sizes[2] =
+              std::min(src_sizes[2], weights_src->shape[2] - src_offsets[2]);
+        } while (src_offsets[2] < weights_src->shape[2]);
+
+        sub_tensor_cfg.offset[0] = src_offsets[1] += src_sizes[1];
+        src_sizes[1] =
+            std::min(src_sizes[1], weights_src->shape[1] - src_offsets[1]);
+      } while (src_offsets[1] < weights_src->shape[1]);
+
+      sub_tensor_cfg.offset[3] = src_offsets[0] += src_sizes[0];
+      src_sizes[0] =
+          std::min(src_sizes[0], weights_src->shape[0] - src_offsets[0]);
+    } while (src_offsets[0] < weights_src->shape[0]);
+  }
+}
+#endif
+
+}  // namespace micro
+}  // namespace ops
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_MICRO_KERNELS_ARC_MLI_TF_UTILS_H_
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/pooling.cc b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/pooling.cc
new file mode 100644
index 00000000..104ec311
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/pooling.cc
@@ -0,0 +1,419 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+#include "tensorflow/lite/kernels/internal/reference/pooling.h"
+
+#include "mli_api.h"  // NOLINT
+#include "tensorflow/lite/c/builtin_op_data.h"
+#include "tensorflow/lite/kernels/internal/reference/integer_ops/pooling.h"
+#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
+#include "tensorflow/lite/kernels/kernel_util.h"
+#include "tensorflow/lite/kernels/padding.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/mli_function_specializations.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/mli_slicers.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/mli_tf_utils.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/scratch_buf_mgr.h"
+#include "tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.h"
+#include "tensorflow/lite/micro/kernels/kernel_util.h"
+#include "tensorflow/lite/micro/micro_log.h"
+
+namespace tflite {
+
+namespace {
+
+constexpr int kInputTensor = 0;
+constexpr int kOutputTensor = 0;
+
+struct OpData {
+  TfLitePaddingValues padding;
+  int32_t activation_min;
+  int32_t activation_max;
+  float activation_min_f32;
+  float activation_max_f32;
+
+  // The result of checking if MLI optimized version of tensors can be used.
+  bool is_mli_applicable;
+
+  // Tensors in MLI format.
+  mutable ops::micro::MliTensorInterface mli_in;
+  mutable ops::micro::MliTensorInterface mli_out;
+  mli_pool_cfg* cfg;
+
+  // Pointer to the mli convolution function.
+  pooling_func_ptr p_mli_krn_avepool_hwc_sa8;
+  pooling_func_ptr p_mli_krn_maxpool_hwc_sa8;
+};
+
+enum MliPoolingType { AveragePooling = 0, MaxPooling = 1 };
+
+bool IsMliApplicable(TfLiteContext* context, const TfLiteTensor* input,
+                     const TfLitePoolParams* params) {
+  // MLI optimized version only supports int8_t datatype and no fused Relu
+  return (input->type == kTfLiteInt8 && params->activation == kTfLiteActNone);
+}
+
+TfLiteStatus CalculateOpData(TfLiteContext* context,
+                             const TfLitePoolParams* params,
+                             const TfLiteTensor* input,
+                             const TfLiteTensor* output, OpData* data) {
+  // input: batch, height, width, channel
+  int height = SizeOfDimension(input, 1);
+  int width = SizeOfDimension(input, 2);
+
+  int out_height, out_width;
+
+  data->padding = ComputePaddingHeightWidth(
+      params->stride_height, params->stride_width,
+      /*dilation_rate_height=*/1,
+      /*dilation_rate_width=*/1, height, width, params->filter_height,
+      params->filter_width, params->padding, &out_height, &out_width);
+  return kTfLiteOk;
+}
+
+void* Init(TfLiteContext* context, const char* buffer, size_t length) {
+  TFLITE_DCHECK(context->AllocatePersistentBuffer != nullptr);
+  return context->AllocatePersistentBuffer(context, sizeof(OpData));
+}
+
+TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
+  TFLITE_DCHECK(node->builtin_data != nullptr);
+  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);
+
+  TFLITE_DCHECK(node->user_data != nullptr);
+  OpData* data = static_cast<OpData*>(node->user_data);
+
+  MicroContext* micro_context = GetMicroContext(context);
+
+  TfLiteTensor* input =
+      micro_context->AllocateTempInputTensor(node, kInputTensor);
+  TF_LITE_ENSURE(context, input != nullptr);
+  TfLiteTensor* output =
+      micro_context->AllocateTempOutputTensor(node, kOutputTensor);
+  TF_LITE_ENSURE(context, output != nullptr);
+
+  data->is_mli_applicable = IsMliApplicable(context, input, params);
+
+  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, input, output, data));
+
+  if (input->type == kTfLiteFloat32) {
+    CalculateActivationRange(params->activation, &data->activation_min_f32,
+                             &data->activation_max_f32);
+  } else if (input->type == kTfLiteInt8) {
+    CalculateActivationRangeQuantized(context, params->activation, output,
+                                      &data->activation_min,
+                                      &data->activation_max);
+  }
+
+  if (data->is_mli_applicable) {
+    data->mli_in = ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+    data->mli_out = ops::micro::MliTensorInterface(static_cast<mli_tensor*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_tensor))));
+    data->cfg = static_cast<mli_pool_cfg*>(
+        context->AllocatePersistentBuffer(context, sizeof(mli_pool_cfg)));
+
+    ops::micro::ConvertToMliTensor(input, &data->mli_in);
+    ops::micro::ConvertToMliTensor(output, &data->mli_out);
+
+    data->cfg->kernel_width = params->filter_width;
+    data->cfg->kernel_height = params->filter_height;
+    data->cfg->stride_width = params->stride_width;
+    data->cfg->stride_height = params->stride_height;
+
+    if (params->padding == kTfLitePaddingValid) {
+      data->cfg->padding_left = 0;
+      data->cfg->padding_right = 0;
+      data->cfg->padding_top = 0;
+      data->cfg->padding_bottom = 0;
+    } else {
+      data->cfg->padding_left = data->padding.width;
+      data->cfg->padding_right =
+          data->padding.width + data->padding.width_offset;
+      data->cfg->padding_top = data->padding.height;
+      data->cfg->padding_bottom =
+          data->padding.height + data->padding.height_offset;
+    }
+
+    // Choose pooling mli specialized functions.
+    data->p_mli_krn_avepool_hwc_sa8 = mli_krn_avepool(data->cfg);
+    data->p_mli_krn_maxpool_hwc_sa8 = mli_krn_maxpool(data->cfg);
+  }
+
+  micro_context->DeallocateTempTfLiteTensor(input);
+  micro_context->DeallocateTempTfLiteTensor(output);
+  return kTfLiteOk;
+}
+
+void AverageEvalFloat(TfLiteContext* context, const TfLiteNode* node,
+                      const TfLitePoolParams* params, const OpData& data,
+                      const TfLiteEvalTensor* input, TfLiteEvalTensor* output) {
+#if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+  float activation_min, activation_max;
+  CalculateActivationRange(params->activation, &activation_min,
+                           &activation_max);
+
+  PoolParams op_params;
+  op_params.stride_height = params->stride_height;
+  op_params.stride_width = params->stride_width;
+  op_params.filter_height = params->filter_height;
+  op_params.filter_width = params->filter_width;
+  op_params.padding_values.height = data.padding.height;
+  op_params.padding_values.width = data.padding.width;
+  op_params.float_activation_min = activation_min;
+  op_params.float_activation_max = activation_max;
+  reference_ops::AveragePool(op_params, tflite::micro::GetTensorShape(input),
+                             tflite::micro::GetTensorData<float>(input),
+                             tflite::micro::GetTensorShape(output),
+                             tflite::micro::GetTensorData<float>(output));
+#else
+  MicroPrintf("Type %s (%d) is not supported by ARC MLI Library.",
+              TfLiteTypeGetName(input->type), input->type);
+#endif
+}
+
+// Prepare MLI tensors and run Average or Max Pooling
+TfLiteStatus EvalMli(TfLiteContext* context, const TfLitePoolParams* params,
+                     const OpData& data, const TfLiteEvalTensor* input,
+                     TfLiteEvalTensor* output,
+                     const MliPoolingType pooling_type) {
+  mli_pool_cfg cfg_local = *data.cfg;
+
+  ops::micro::MliTensorAttachBuffer<int8_t>(input, &data.mli_in);
+  ops::micro::MliTensorAttachBuffer<int8_t>(output, &data.mli_out);
+
+  const int height_dimension = 1;
+  int in_slice_height = 0;
+  int out_slice_height = 0;
+  const int overlap = cfg_local.kernel_height - cfg_local.stride_height;
+
+  // Tensors for data in fast (local) memory and config to copy data from
+  // external to local memory
+  mli_tensor in_local = *data.mli_in.MliTensor();
+  mli_tensor out_local = *data.mli_out.MliTensor();
+
+  ops::micro::MliTensorInterface in_local_interface(&in_local);
+  ops::micro::MliTensorInterface out_local_interface(&out_local);
+
+  mli_mov_cfg_t copy_config;
+  mli_mov_cfg_for_copy(&copy_config);
+  TF_LITE_ENSURE_STATUS(get_arc_scratch_buffer_for_pooling_tensors(
+      context, &in_local_interface, &out_local_interface));
+
+  bool in_is_local =
+      in_local_interface.Data<int8_t>() == data.mli_in.Data<int8_t>();
+  bool out_is_local =
+      out_local_interface.Data<int8_t>() == data.mli_out.Data<int8_t>();
+
+  TF_LITE_ENSURE_STATUS(arc_scratch_buffer_calc_slice_size_io(
+      &in_local_interface, &out_local_interface, cfg_local.kernel_height,
+      cfg_local.stride_height, cfg_local.padding_top, cfg_local.padding_bottom,
+      &in_slice_height, &out_slice_height));
+
+  /* mli_in tensor contains batches of HWC tensors. so it is a 4 dimensional
+     tensor. because the mli kernel will process one HWC tensor at a time, the 4
+     dimensional tensor needs to be sliced into nBatch 3 dimensional tensors. on
+     top of that there could be a need to also slice in the Height dimension.
+     for that the sliceHeight has been calculated. The tensor slicer is
+     configured that it will completely slice the nBatch dimension (0) and slice
+     the height dimension (1) in chunks of 'sliceHeight' */
+  ops::micro::TensorSlicer in_slice(data.mli_in.MliTensor(), height_dimension,
+                                    in_slice_height, cfg_local.padding_top,
+                                    cfg_local.padding_bottom, overlap);
+  ops::micro::TensorSlicer out_slice(data.mli_out.MliTensor(), height_dimension,
+                                     out_slice_height);
+
+  /* is_local indicates that the tensor is already in local memory,
+     so in that case the original tensor can be used,
+     and there is no need to copy it to the local tensor*/
+  mli_tensor* in_ptr = in_is_local ? in_slice.Sub() : &in_local;
+  mli_tensor* out_ptr = out_is_local ? out_slice.Sub() : &out_local;
+
+  while (!out_slice.Done()) {
+    if (!out_is_local) {
+      ops::micro::PrepareLocalTensor(out_slice.Sub(), &out_local);
+      ops::micro::PrepareLocalTensor(in_slice.Sub(), &in_local);
+    }
+    cfg_local.padding_top = in_slice.GetPaddingPre();
+    cfg_local.padding_bottom = in_slice.GetPaddingPost();
+
+    mli_mov_tensor_sync(in_slice.Sub(), &copy_config, in_ptr);
+    if (pooling_type == AveragePooling) {
+      TFLITE_DCHECK(data.p_mli_krn_avepool_hwc_sa8 != nullptr);
+      data.p_mli_krn_avepool_hwc_sa8(in_ptr, &cfg_local, out_ptr);
+    } else if (pooling_type == MaxPooling) {
+      TFLITE_DCHECK(data.p_mli_krn_maxpool_hwc_sa8 != nullptr);
+      data.p_mli_krn_maxpool_hwc_sa8(in_ptr, &cfg_local, out_ptr);
+    }
+    mli_mov_tensor_sync(out_ptr, &copy_config, out_slice.Sub());
+
+    in_slice.Next();
+    out_slice.Next();
+  }
+  return kTfLiteOk;
+}
+
+void AverageEvalQuantized(TfLiteContext* context, const TfLiteNode* node,
+                          const TfLitePoolParams* params, const OpData& data,
+                          const TfLiteEvalTensor* input,
+                          TfLiteEvalTensor* output) {
+#if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+  TFLITE_DCHECK(input->type == kTfLiteInt8);
+
+  PoolParams op_params;
+  op_params.stride_height = params->stride_height;
+  op_params.stride_width = params->stride_width;
+  op_params.filter_height = params->filter_height;
+  op_params.filter_width = params->filter_width;
+  op_params.padding_values.height = data.padding.height;
+  op_params.padding_values.width = data.padding.width;
+  op_params.quantized_activation_min = data.activation_min;
+  op_params.quantized_activation_max = data.activation_max;
+
+  reference_integer_ops::AveragePool(
+      op_params, tflite::micro::GetTensorShape(input),
+      tflite::micro::GetTensorData<int8_t>(input),
+      tflite::micro::GetTensorShape(output),
+      tflite::micro::GetTensorData<int8_t>(output));
+#else
+  MicroPrintf("Type %s (%d) is not supported by ARC MLI Library.",
+              TfLiteTypeGetName(input->type), input->type);
+#endif
+}
+
+void MaxEvalFloat(TfLiteContext* context, TfLiteNode* node,
+                  TfLitePoolParams* params, const OpData& data,
+                  const TfLiteEvalTensor* input, TfLiteEvalTensor* output) {
+#if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+  tflite::PoolParams op_params;
+  op_params.stride_height = params->stride_height;
+  op_params.stride_width = params->stride_width;
+  op_params.filter_height = params->filter_height;
+  op_params.filter_width = params->filter_width;
+  op_params.padding_values.height = data.padding.height;
+  op_params.padding_values.width = data.padding.width;
+  op_params.float_activation_min = data.activation_min_f32;
+  op_params.float_activation_max = data.activation_max_f32;
+  reference_ops::MaxPool(op_params, tflite::micro::GetTensorShape(input),
+                         tflite::micro::GetTensorData<float>(input),
+                         tflite::micro::GetTensorShape(output),
+                         tflite::micro::GetTensorData<float>(output));
+#else
+  MicroPrintf(
+
+      "Node configuration or type %s (%d) is not supported by ARC MLI Library.",
+      TfLiteTypeGetName(input->type), input->type);
+#endif
+}
+
+void MaxEvalQuantized(TfLiteContext* context, TfLiteNode* node,
+                      TfLitePoolParams* params, const OpData& data,
+                      const TfLiteEvalTensor* input, TfLiteEvalTensor* output) {
+#if !defined(TF_LITE_STRIP_REFERENCE_IMPL)
+  TFLITE_DCHECK(input->type == kTfLiteInt8);
+  tflite::PoolParams op_params;
+  op_params.stride_height = params->stride_height;
+  op_params.stride_width = params->stride_width;
+  op_params.filter_height = params->filter_height;
+  op_params.filter_width = params->filter_width;
+  op_params.padding_values.height = data.padding.height;
+  op_params.padding_values.width = data.padding.width;
+  op_params.quantized_activation_min = data.activation_min;
+  op_params.quantized_activation_max = data.activation_max;
+
+  reference_integer_ops::MaxPool(op_params,
+                                 tflite::micro::GetTensorShape(input),
+                                 tflite::micro::GetTensorData<int8_t>(input),
+                                 tflite::micro::GetTensorShape(output),
+                                 tflite::micro::GetTensorData<int8_t>(output));
+#else
+  MicroPrintf(
+
+      "Node configuration or type %s (%d) is not supported by ARC MLI Library.",
+      TfLiteTypeGetName(input->type), input->type);
+#endif
+}
+
+TfLiteStatus AverageEval(TfLiteContext* context, TfLiteNode* node) {
+  TFLITE_DCHECK(node->builtin_data != nullptr);
+  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);
+
+  const TfLiteEvalTensor* input =
+      tflite::micro::GetEvalInput(context, node, kInputTensor);
+  TfLiteEvalTensor* output =
+      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+
+  TFLITE_DCHECK(node->user_data != nullptr);
+  const OpData& data = *(static_cast<const OpData*>(node->user_data));
+
+  // Inputs and outputs share the same type, guaranteed by the converter.
+  switch (input->type) {
+    case kTfLiteFloat32:
+      AverageEvalFloat(context, node, params, data, input, output);
+      break;
+    case kTfLiteInt8:
+      if (data.is_mli_applicable) {
+        EvalMli(context, params, data, input, output, AveragePooling);
+      } else {
+        AverageEvalQuantized(context, node, params, data, input, output);
+      }
+      break;
+    default:
+      MicroPrintf("Input type %s is not currently supported",
+                  TfLiteTypeGetName(input->type));
+      return kTfLiteError;
+  }
+  return kTfLiteOk;
+}
+
+TfLiteStatus MaxEval(TfLiteContext* context, TfLiteNode* node) {
+  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);
+
+  const TfLiteEvalTensor* input =
+      tflite::micro::GetEvalInput(context, node, kInputTensor);
+  TfLiteEvalTensor* output =
+      tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+
+  TFLITE_DCHECK(node->user_data != nullptr);
+  const OpData& data = *(static_cast<const OpData*>(node->user_data));
+
+  switch (input->type) {
+    case kTfLiteFloat32:
+      MaxEvalFloat(context, node, params, data, input, output);
+      break;
+    case kTfLiteInt8:
+      if (data.is_mli_applicable) {
+        EvalMli(context, params, data, input, output, MaxPooling);
+      } else {
+        MaxEvalQuantized(context, node, params, data, input, output);
+      }
+      break;
+    default:
+      MicroPrintf("Type %s not currently supported.",
+                  TfLiteTypeGetName(input->type));
+      return kTfLiteError;
+  }
+  return kTfLiteOk;
+}
+
+}  // namespace
+
+TFLMRegistration Register_AVERAGE_POOL_2D() {
+  return tflite::micro::RegisterOp(Init, Prepare, AverageEval);
+}
+
+TFLMRegistration Register_MAX_POOL_2D() {
+  return tflite::micro::RegisterOp(Init, Prepare, MaxEval);
+}
+
+}  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/scratch_buf_mgr.cc b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/scratch_buf_mgr.cc
new file mode 100644
index 00000000..ef489fa6
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/scratch_buf_mgr.cc
@@ -0,0 +1,392 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/micro/kernels/arc_mli/scratch_buf_mgr.h"
+
+#include <limits.h>
+
+#include <algorithm>
+
+#include "tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.h"
+
+namespace tflite {
+namespace ops {
+namespace micro {
+
+#if (defined(__Xxy)) || (defined(__Xvdsp))
+static void get_arc_two_buffer_sizes(int request_size_1, int request_size_2,
+                                     int* grant_size_1, int* grant_size_2) {
+  int maxrequest = 0;
+  int secondrequest = 0;
+  int maxavailable = 0;
+  int secondavail = 0;
+
+  // determine the largest requested buffer.
+  if (request_size_1 > request_size_2) {
+    maxrequest = request_size_1;
+    secondrequest = request_size_2;
+  } else {
+    maxrequest = request_size_2;
+    secondrequest = request_size_1;
+  }
+
+  // find the two largest available buffers.
+  get_arc_scratch_buffer_two_max_sizes(&maxavailable, &secondavail);
+
+  // in case two buffers are available, the largest buffer can go to the largest
+  // request.
+  if (secondavail > 0) {  // this condition can be enhanced to prevent cases
+                          // where the second buffer is so small that it is
+                          // better to use one buffer and split it.
+    if (request_size_1 > request_size_2) {
+      *grant_size_1 = maxavailable;
+      *grant_size_2 = secondavail;
+    } else {
+      *grant_size_1 = secondavail;
+      *grant_size_2 = maxavailable;
+    }
+  } else {
+    // In case only one buffer is available,
+    // use only the max buffer, and split it.
+    *grant_size_1 = maxavailable / 2;
+    *grant_size_2 = maxavailable / 2;
+  }
+}
+
+static TfLiteStatus get_arc_scratch_buffer_for_io_tensors(
+    TfLiteContext* context, MliTensorInterface* in, MliTensorInterface* out) {
+  int request_size_in = 0;
+  int request_size_out = 0;
+  int grant_size_in = 0;
+  int grant_size_out = 0;
+  if (!inside_arc_ccm(in->Data<int8_t>())) {
+    // In case the input tensor contains multiple batches, it has rank 4
+    // because the mli kernel cannot operate on batches, we need to have the
+    // size of a single HWC tensor. that is why the start_rank is 1 in case of
+    // input rank 4
+    int start_rank = *in->Rank() - 3;
+    request_size_in = mli_hlp_count_elem_num(in->MliTensor(), start_rank) *
+                      mli_hlp_tensor_element_size(in->MliTensor());
+  }
+  if (!inside_arc_ccm(out->Data<int8_t>())) {
+    // In case the input tensor contains multiple batches, it has rank 4
+    // because the mli kernel cannot operate on batches, we need to have the
+    // size of a single batch. that is why the start_rank is 1 in case of input
+    // rank 4
+    int start_rank = *out->Rank() - 3;
+    request_size_out = mli_hlp_count_elem_num(out->MliTensor(), start_rank) *
+                       mli_hlp_tensor_element_size(out->MliTensor());
+  }
+
+  get_arc_two_buffer_sizes(request_size_in, request_size_out, &grant_size_in,
+                           &grant_size_out);
+  if (!inside_arc_ccm(in->Data<int8_t>())) {
+    in->SetData<int8_t>(
+        static_cast<int8_t*>(get_arc_scratch_buffer(grant_size_in)),
+        grant_size_in);
+    if (in->Data<int8_t>() == NULL) return kTfLiteError;
+  }
+
+  if (!inside_arc_ccm(out->Data<int8_t>())) {
+    out->SetData<int8_t>(
+        static_cast<int8_t*>(get_arc_scratch_buffer(grant_size_out)),
+        grant_size_out);
+    if (out->Data<int8_t>() == NULL) return kTfLiteError;
+  }
+
+  return kTfLiteOk;
+}
+#endif
+
+TfLiteStatus get_arc_scratch_buffer_for_conv_tensors(
+    TfLiteContext* context, MliTensorInterface* in, MliTensorInterface* weights,
+    MliTensorInterface* bias, MliTensorInterface* out) {
+  TfLiteStatus ret_val = kTfLiteOk;
+#if (defined(__Xxy)) || (defined(__Xvdsp))
+  init_arc_scratch_buffers();
+
+  if (!inside_arc_ccm(bias->Data<int32_t>())) {
+    uint32_t bias_mem_requirements =
+        mli_hlp_count_elem_num(bias->MliTensor(), 0) *
+        mli_hlp_tensor_element_size(bias->MliTensor());
+    bias->SetData<int32_t>(
+        static_cast<int32_t*>(get_arc_scratch_buffer(bias_mem_requirements)),
+        bias_mem_requirements);
+  }
+
+  if (bias->Data<int32_t>() == NULL) {
+    int max_bias_size = 0;
+    get_arc_scratch_buffer_max_size(&max_bias_size);
+    bias->SetData<int32_t>(
+        static_cast<int32_t*>(get_arc_scratch_buffer(max_bias_size)),
+        max_bias_size);
+    if (max_bias_size == 0) ret_val = kTfLiteError;
+  }
+  if (bias->Data<int32_t>() == NULL) ret_val = kTfLiteError;
+
+  if (!inside_arc_ccm(weights->Data<int8_t>())) {
+    int weights_size = mli_hlp_count_elem_num(weights->MliTensor(), 0) *
+                       mli_hlp_tensor_element_size(weights->MliTensor());
+    int max_weights_size = 0;
+    weights->SetData<int8_t>(
+        static_cast<int8_t*>(get_arc_scratch_buffer(weights_size)),
+        weights_size);
+    if (weights->Data<int8_t>() == NULL) {
+      get_arc_scratch_buffer_max_size(&max_weights_size);
+      weights->SetData<int8_t>(
+          static_cast<int8_t*>(get_arc_scratch_buffer(max_weights_size)),
+          max_weights_size);
+      if (max_weights_size == 0) ret_val = kTfLiteError;
+    }
+    if (weights->Data<int8_t>() == NULL) ret_val = kTfLiteError;
+  }
+
+  if (ret_val == kTfLiteOk) {
+    ret_val = get_arc_scratch_buffer_for_io_tensors(context, in, out);
+  }
+#endif
+  return ret_val;
+}
+
+TfLiteStatus get_arc_scratch_buffer_for_fully_connect_tensors(
+    TfLiteContext* context, MliTensorInterface* in, MliTensorInterface* weights,
+    MliTensorInterface* bias, MliTensorInterface* out) {
+  TfLiteStatus ret_val = kTfLiteOk;
+
+#if (defined(__Xxy)) || (defined(__Xvdsp))
+  init_arc_scratch_buffers();
+
+  if (!inside_arc_ccm(bias->Data<int32_t>())) {
+    int bias_mem_requirements = mli_hlp_count_elem_num(bias->MliTensor(), 0) *
+                                mli_hlp_tensor_element_size(bias->MliTensor());
+    bias->SetData<int32_t>(
+        static_cast<int32_t*>(get_arc_scratch_buffer(bias_mem_requirements)),
+        bias_mem_requirements);
+  }
+
+  if (bias->Data<int32_t>() == NULL) {
+    int max_bias_size = 0;
+    get_arc_scratch_buffer_max_size(&max_bias_size);
+    bias->SetData<int32_t>(
+        static_cast<int32_t*>(get_arc_scratch_buffer(max_bias_size)),
+        max_bias_size);
+    if (max_bias_size == 0) ret_val = kTfLiteError;
+  }
+  if (bias->Data<int32_t>() == NULL) ret_val = kTfLiteError;
+
+  if (!inside_arc_ccm(weights->Data<int8_t>())) {
+    int weights_size = mli_hlp_count_elem_num(weights->MliTensor(), 0) *
+                       mli_hlp_tensor_element_size(weights->MliTensor());
+    int max_weights_size = 0;
+    weights->SetData<int8_t>(
+        static_cast<int8_t*>(get_arc_scratch_buffer(weights_size)),
+        weights_size);
+    if (weights->Data<int8_t>() == NULL) {
+      get_arc_scratch_buffer_max_size(&max_weights_size);
+      weights->SetData<int8_t>(
+          static_cast<int8_t*>(get_arc_scratch_buffer(max_weights_size)),
+          max_weights_size);
+      if (max_weights_size == 0) ret_val = kTfLiteError;
+    }
+    if (weights->Data<int8_t>() == NULL) ret_val = kTfLiteError;
+  }
+
+  /* strategy for FC kernels:
+     first allocate input, because this cannot be sliced. (in case of batch
+     processing, only a single input needs to be allocated) then weights &
+     bias because if fully loaded, they can be reused over batches. then
+     output. The number of output channels (for weights slicing) depends on
+     size of output and size of weights&bias */
+
+  if (!inside_arc_ccm(in->Data<int8_t>())) {
+    /* In case the input tensor contains multiple batches,
+       only count the size if the inner most dimension */
+    int size_in = mli_hlp_count_elem_num(in->MliTensor(), *in->Rank() - 1) *
+                  mli_hlp_tensor_element_size(in->MliTensor());
+    in->SetData<int8_t>(static_cast<int8_t*>(get_arc_scratch_buffer(size_in)),
+                        size_in);
+    if (in->Data<int8_t>() == NULL) {
+      in->SetData<int8_t>(nullptr, 0);
+      ret_val = kTfLiteError;
+    }
+  }
+  if (!inside_arc_ccm(out->Data<int8_t>())) {
+    /* In case the input tensor contains multiple batches,
+       only count the size if the inner most dimension */
+    int out_size = mli_hlp_count_elem_num(out->MliTensor(), *out->Rank() - 1) *
+                   mli_hlp_tensor_element_size(out->MliTensor());
+    int max_out_size = 0;
+    out->SetData<int8_t>(static_cast<int8_t*>(get_arc_scratch_buffer(out_size)),
+                         out_size);
+    if (out->Data<int8_t>() == NULL) {
+      get_arc_scratch_buffer_max_size(&max_out_size);
+      out->SetData<int8_t>(
+          static_cast<int8_t*>(get_arc_scratch_buffer(max_out_size)),
+          max_out_size);
+      if (max_out_size == 0) ret_val = kTfLiteError;
+    }
+    if (out->Data<int8_t>() == NULL) ret_val = kTfLiteError;
+  }
+#endif
+  return ret_val;
+}
+
+TfLiteStatus get_arc_scratch_buffer_for_eltwise_tensors(
+    TfLiteContext* context, MliTensorInterface* in1, MliTensorInterface* in2,
+    MliTensorInterface* out) {
+  TfLiteStatus ret_val = kTfLiteOk;
+#if (defined(__Xxy)) || (defined(__Xvdsp))
+  init_arc_scratch_buffers();
+  constexpr int tsr_num = 3;
+  int in1_size = mli_hlp_count_elem_num(in1->MliTensor(), 0) *
+                 mli_hlp_tensor_element_size(in1->MliTensor());
+  int in2_size = mli_hlp_count_elem_num(in2->MliTensor(), 0) *
+                 mli_hlp_tensor_element_size(in2->MliTensor());
+  int out_size = mli_hlp_count_elem_num(out->MliTensor(), 0) *
+                 mli_hlp_tensor_element_size(out->MliTensor());
+  int sizes[tsr_num] = {in1_size, in2_size, out_size};
+  MliTensorInterface* in_tensors[tsr_num] = {in1, in2, out};
+  for (int i = 0; i < tsr_num; ++i) {
+    if (!inside_arc_ccm(in_tensors[i]->Data<int8_t>())) {
+      auto* data_ptr = get_arc_scratch_buffer(sizes[i]);
+      if (data_ptr == nullptr) {
+        get_arc_scratch_buffer_max_size(&sizes[i]);
+        data_ptr = get_arc_scratch_buffer(sizes[i]);
+      }
+      if (data_ptr == nullptr || sizes[i] == 0) {
+        in_tensors[i]->SetData<int8_t>(nullptr, 0);
+        ret_val = kTfLiteError;
+      } else {
+        in_tensors[i]->SetData<int8_t>(static_cast<int8_t*>(data_ptr),
+                                       sizes[i]);
+      }
+    }
+  }
+#endif
+  return ret_val;
+}
+
+TfLiteStatus arc_scratch_buffer_calc_slice_size_io(
+    const MliTensorInterface* in, const MliTensorInterface* out,
+    const int kernel_height, const int stride_height, const int padding_top,
+    const int padding_bot, int* in_slice_height, int* out_slice_height) {
+  const int height_dimension = 1;
+  const int in_height = in->Shape()[height_dimension];
+  const int out_height = out->Shape()[height_dimension];
+  const int line_size_in =
+      mli_hlp_count_elem_num(in->MliTensor(), height_dimension + 1) *
+      mli_hlp_tensor_element_size(in->MliTensor());
+  const int line_size_out =
+      mli_hlp_count_elem_num(out->MliTensor(), height_dimension + 1) *
+      mli_hlp_tensor_element_size(out->MliTensor());
+  int max_lines_in = 0;
+  int max_lines_out = 0;
+  int max_out_lines_for_input = 0;
+  bool fit =
+      (static_cast<int>(*in->DataCapacity()) >= in_height * line_size_in) &&
+      (static_cast<int>(*out->DataCapacity()) >= out_height * line_size_out);
+  if (fit) {
+    // in case both tensors completely fit in the capacity, there is no need
+    // for slicing. As padding can affect effective input region, we also
+    // derive it from output height, and rely on a clipping logic which intend
+    // to reduce last smaller slice. I.e the only slice is a kind of "smaller
+    // last slice that need to be corrected"
+    *in_slice_height = std::max(in_height, out_height * stride_height);
+    *out_slice_height = out_height;
+  } else {
+    // First compute how many lines fit into the input tensor, and compute how
+    // many output lines can be computed with that.
+    max_lines_in = std::min(
+        in_height, static_cast<int>(*in->DataCapacity()) / line_size_in);
+    if (max_lines_in >= in_height) {
+      max_out_lines_for_input = out_height;
+    } else if (2 * max_lines_in >= in_height) {
+      // in this case only two slices are needed, so both could benefit from
+      // padding. take the MIN to get the worst case.
+      max_out_lines_for_input =
+          (max_lines_in + std::min(padding_top, padding_bot) - kernel_height +
+           1) /
+          stride_height;
+    } else {
+      max_out_lines_for_input =
+          (max_lines_in - kernel_height + 1) / stride_height;
+    }
+    // Then compute how many output lines fit into the output tensor.
+    max_lines_out = std::min(
+        out_height, static_cast<int>(*out->DataCapacity()) / line_size_out);
+    // the smallest of the two determines the slice height for the output, and
+    // the derived sliceheight for the input.
+    *out_slice_height = std::min(max_out_lines_for_input, max_lines_out);
+    *in_slice_height = *out_slice_height * stride_height;
+  }
+
+  if ((*in_slice_height > 0) && (*out_slice_height > 0)) {
+    return kTfLiteOk;
+  } else {
+    return kTfLiteError;
+  }
+}
+
+TfLiteStatus arc_scratch_buffer_calc_slice_size_weights(
+    const MliTensorInterface* weights, const MliTensorInterface* bias,
+    const int weight_out_ch_dimension, int* slice_channels) {
+  const int channels = weights->Shape()[weight_out_ch_dimension];
+  const int ch_size_w =
+      (mli_hlp_count_elem_num(weights->MliTensor(), 0) / channels) *
+      mli_hlp_tensor_element_size(weights->MliTensor());
+  const int ch_size_b =
+      (mli_hlp_count_elem_num(bias->MliTensor(), 0) / channels) *
+      mli_hlp_tensor_element_size(bias->MliTensor());
+  int max_ch_weigths = 0;
+  int max_ch_bias = 0;
+
+  bool fit =
+      (static_cast<int>(*weights->DataCapacity()) >= channels * ch_size_w) &&
+      (static_cast<int>(*bias->DataCapacity()) >= channels * ch_size_b);
+  if (fit) {
+    // in case both tensors completely fit in the capacity, there is no need
+    // for slicing
+    *slice_channels = channels;
+  } else {
+    // First compute how many channels fit into the weights tensor
+    max_ch_weigths = std::min(
+        channels, static_cast<int>(*weights->DataCapacity()) / ch_size_w);
+    // Ten compute how many channels fit into the bias tensor.
+    max_ch_bias =
+        std::min(channels, static_cast<int>(*bias->DataCapacity()) / ch_size_b);
+    // the smallest of the two determines the slice size
+    *slice_channels = std::min(max_ch_weigths, max_ch_bias);
+  }
+
+  if (*slice_channels > 0) {
+    return kTfLiteOk;
+  } else {
+    return kTfLiteError;
+  }
+}
+
+TfLiteStatus get_arc_scratch_buffer_for_pooling_tensors(
+    TfLiteContext* context, MliTensorInterface* in, MliTensorInterface* out) {
+#if (defined(__Xxy)) || (defined(__Xvdsp))
+  init_arc_scratch_buffers();
+  return get_arc_scratch_buffer_for_io_tensors(context, in, out);
+#else
+  return kTfLiteOk;
+#endif
+}
+
+}  // namespace micro
+}  // namespace ops
+}  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/scratch_buf_mgr.h b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/scratch_buf_mgr.h
new file mode 100644
index 00000000..be6dd8fb
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/scratch_buf_mgr.h
@@ -0,0 +1,145 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_MICRO_ARC_SCRATCH_BUF_MGR_H_
+#define TENSORFLOW_LITE_MICRO_ARC_SCRATCH_BUF_MGR_H_
+
+#include "mli_api.h"  // NOLINT
+#include "mli_interface.h"
+#include "tensorflow/lite/c/common.h"
+
+namespace tflite {
+namespace ops {
+namespace micro {
+
+/**
+ * @brief Function to allocate scratch buffers for the convolution tensors
+ *
+ * @detail This function will update the data pointers in the 4 tensors with
+ * pointers to scratch buffers in fast local memory.
+ *
+ * @param context   [I] pointer to TfLite context (needed for error handling)
+ * @param in        [IO] pointer to the input tensor
+ * @param weights   [IO] pointer to the weights tensor
+ * @param bias      [IO] pointer to the bias tensor
+ * @param output    [IO] pointer to the output tensor
+ *
+ * @return Tf Lite status code
+ */
+TfLiteStatus get_arc_scratch_buffer_for_conv_tensors(
+    TfLiteContext* context, MliTensorInterface* in, MliTensorInterface* weights,
+    MliTensorInterface* bias, MliTensorInterface* out);
+
+/**
+ * @brief Function to allocate scratch buffers for pooling kernels with only
+ * input and output buffers
+ *
+ * @detail This function will update the data pointers in the 2 tensors with
+ * pointers to scratch buffers in fast local memory.
+ *
+ * @param context   [I] pointer to TfLite context (needed for error handling)
+ * @param in        [IO] pointer to the input tensor
+ * @param output    [IO] pointer to the output tensor
+ *
+ * @return Tf Lite status code
+ */
+TfLiteStatus get_arc_scratch_buffer_for_pooling_tensors(
+    TfLiteContext* context, MliTensorInterface* in, MliTensorInterface* out);
+
+/**
+ * @brief Function to allocate scratch buffers for the fully connect tensors
+ *
+ * @detail This function will update the data pointers in the 4 tensors with
+ * pointers to scratch buffers in fast local memory.
+ *
+ * @param context   [I] pointer to TfLite context (needed for error handling)
+ * @param in        [IO] pointer to the input tensor
+ * @param weights   [IO] pointer to the weights tensor
+ * @param bias      [IO] pointer to the bias tensor
+ * @param output    [IO] pointer to the output tensor
+ *
+ * @return Tf Lite status code
+ */
+TfLiteStatus get_arc_scratch_buffer_for_fully_connect_tensors(
+    TfLiteContext* context, MliTensorInterface* in, MliTensorInterface* weights,
+    MliTensorInterface* bias, MliTensorInterface* out);
+
+/**
+ * @brief Function to allocate scratch buffers for the eltwise function tensors
+ *
+ * @detail This function will update the data pointers in the 3 tensors with
+ * pointers to scratch buffers in fast local memory.
+ *
+ * @param context   [I] pointer to TfLite context (needed for error handling)
+ * @param in1       [IO] pointer to the first input tensor
+ * @param in2       [IO] pointer to the second input tensor
+ * @param output    [IO] pointer to the output tensor
+ *
+ * @return Tf Lite status code
+ */
+TfLiteStatus get_arc_scratch_buffer_for_eltwise_tensors(
+    TfLiteContext* context, MliTensorInterface* in1, MliTensorInterface* in2,
+    MliTensorInterface* out);
+
+/**
+ * @brief Function to calculate slice size for io tensors
+ *
+ * @detail This function will calculate the slice size in the height dimension
+ * for input and output tensors. it takes into account the kernel size and the
+ * padding. the function will look at the capacity filed in the in and out
+ * tensor to determine the available buffersize.
+ *
+ * @param in                [I] pointer to the input tensor
+ * @param out               [I] pointer to the output tensor
+ * @param kernelHeight      [I] size of the kernel in height dimension
+ * @param strideHeight      [I] input stride in height dimension
+ * @param padding_top       [I] number of lines with zeros at the top
+ * @param padding_bot       [I] number of lines with zeros at the bottom
+ * @param inSliceHeight     [O] slice size in height dimension for the input
+ * tensor
+ * @param outSliceHeight    [O] slice size in height dimension for the output
+ * tensor
+ *
+ * @return Tf Lite status code
+ */
+TfLiteStatus arc_scratch_buffer_calc_slice_size_io(
+    const MliTensorInterface* in, const MliTensorInterface* out,
+    const int kernelHeight, const int strideHeight, const int padding_top,
+    const int padding_bot, int* in_slice_height, int* out_slice_height);
+
+/**
+ * @brief Function to calculate slice size for weight slicing
+ *
+ * @detail This function will calculate the slice size in the output channel
+ * dimension for weight and bias tensors. the function will look at the capacity
+ * filed in the weights and bias tensor to determine the available buffersize.
+ *
+ * @param weights               [I] pointer to the input tensor
+ * @param bias                  [I] pointer to the output tensor
+ * @param weightOutChDimension  [I] dimension of the output channels in the
+ * weights tensor
+ * @param sliceChannels         [O] slice size in output channel dimension
+ *
+ * @return Tf Lite status code
+ */
+TfLiteStatus arc_scratch_buffer_calc_slice_size_weights(
+    const MliTensorInterface* weights, const MliTensorInterface* bias,
+    const int weight_out_ch_dimension, int* slice_channels);
+
+}  // namespace micro
+}  // namespace ops
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_MICRO_ARC_SCRATCH_BUF_MGR_H_
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.cc b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.cc
new file mode 100644
index 00000000..bf87122e
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.cc
@@ -0,0 +1,192 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.h"
+
+#include <limits.h>
+
+namespace tflite {
+namespace ops {
+namespace micro {
+
+/* by default use all the XY memory, and half of the DCCM because DCCM is also
+ * used for the data section and the stack. the values can be overruled by
+ * adding a -D option to the makefile of the application
+ */
+
+#ifdef __Xxy
+
+#ifndef SCRATCH_MEM_X_SIZE
+#ifdef core_config_xy_size
+#define SCRATCH_MEM_X_SIZE (core_config_xy_size)
+#endif
+#endif
+
+#ifndef SCRATCH_MEM_Y_SIZE
+#ifdef core_config_xy_size
+#define SCRATCH_MEM_Y_SIZE (core_config_xy_size)
+#endif
+#endif
+
+#ifndef SCRATCH_MEM_Z_SIZE
+#ifdef core_config_dccm_size
+#define SCRATCH_MEM_Z_SIZE ((core_config_dccm_size) / 2)
+#endif
+#endif
+
+#elif defined(__Xvdsp)
+
+#ifndef SCRATCH_MEM_VEC_SIZE
+#ifdef core_config_vec_mem_size
+#define SCRATCH_MEM_VEC_SIZE ((core_config_vec_mem_size * 3) / 4)
+#endif
+#endif
+
+#else
+
+#define SCRATCH_MEM_SIZE (65536)
+
+#endif
+
+namespace {
+
+#ifdef __Xxy
+
+#pragma Bss(".Xdata")
+static int8_t scratch_mem_x[SCRATCH_MEM_X_SIZE];
+#pragma Bss()
+
+#pragma Bss(".Ydata")
+static int8_t scratch_mem_y[SCRATCH_MEM_Y_SIZE];
+#pragma Bss()
+
+#pragma Bss(".Zdata")
+static int8_t scratch_mem_z[SCRATCH_MEM_Z_SIZE];
+#pragma Bss()
+
+#elif defined(__Xvdsp)
+
+#pragma Bss(".vecmem_data")
+static int8_t scratch_mem_vec_1[SCRATCH_MEM_VEC_SIZE / 4];
+static int8_t scratch_mem_vec_2[SCRATCH_MEM_VEC_SIZE / 4];
+static int8_t scratch_mem_vec_3[SCRATCH_MEM_VEC_SIZE / 2];
+#pragma Bss()
+
+#else
+
+static int8_t scratch_mem_stack[SCRATCH_MEM_SIZE];
+
+#endif
+}  // namespace
+
+#ifdef __Xxy
+
+static int8_t* scratch_mem[] = {scratch_mem_x, scratch_mem_y, scratch_mem_z};
+static uint32_t scratch_sizes[] = {SCRATCH_MEM_X_SIZE, SCRATCH_MEM_Y_SIZE,
+                                   SCRATCH_MEM_Z_SIZE};
+
+#elif defined(__Xvdsp)
+
+static int8_t* scratch_mem[] = {scratch_mem_vec_1, scratch_mem_vec_2,
+                                scratch_mem_vec_3};
+static uint32_t scratch_sizes[] = {SCRATCH_MEM_VEC_SIZE / 4,
+                                   SCRATCH_MEM_VEC_SIZE / 4,
+                                   SCRATCH_MEM_VEC_SIZE / 2};
+
+#else
+
+static int8_t* scratch_mem[] = {scratch_mem_stack};
+static uint32_t scratch_sizes[] = {SCRATCH_MEM_SIZE};
+
+#endif
+
+void* get_arc_scratch_buffer(int size) {
+  // Function to asign fast memory from one of 3 scratch buffers.
+  // Best Fit strategy - memory is allocated from that memory bank that leaves
+  // the least unused memory.
+  void* buf = NULL;
+  int best_mem_idx = -1;
+  int best_mem_delta = INT_MAX;
+  const int num_mem = sizeof(scratch_mem) / sizeof(scratch_mem[0]);
+  // find a local memory that fits the data size.
+  for (int mem_idx = 0; mem_idx < num_mem; ++mem_idx) {
+    // Best Fit
+    if ((size <= static_cast<int>(scratch_sizes[mem_idx])) &&
+        (static_cast<int>(scratch_sizes[mem_idx]) - size < best_mem_delta)) {
+      best_mem_idx = mem_idx;
+      best_mem_delta = scratch_sizes[mem_idx] - size;
+    }
+  }
+  if (best_mem_idx >= 0) {
+    buf = scratch_mem[best_mem_idx];
+    scratch_mem[best_mem_idx] += size;
+    scratch_sizes[best_mem_idx] -= size;
+  }
+  return buf;
+}
+
+void get_arc_scratch_buffer_max_size(int* size) {
+  int maxavailable = 0;
+  const int num_mem = sizeof(scratch_mem) / sizeof(scratch_mem[0]);
+  // find the largest available buffer.
+  for (int i = 0; i < num_mem; i++) {
+    if (static_cast<int>(scratch_sizes[i]) > maxavailable) {
+      maxavailable = scratch_sizes[i];
+    }
+  }
+  *size = maxavailable;
+}
+
+void get_arc_scratch_buffer_two_max_sizes(int* size1, int* size2) {
+  int maxavailable = 0;
+  int secondavail = 0;
+  const int num_mem = sizeof(scratch_mem) / sizeof(scratch_mem[0]);
+  // find the two largest available buffers.
+  for (int i = 0; i < num_mem; i++) {
+    if (static_cast<int>(scratch_sizes[i]) > maxavailable) {
+      secondavail = maxavailable;
+      maxavailable = scratch_sizes[i];
+    } else if (static_cast<int>(scratch_sizes[i]) > secondavail) {
+      secondavail = scratch_sizes[i];
+    }
+  }
+  *size1 = maxavailable;
+  *size2 = secondavail;
+}
+
+void init_arc_scratch_buffers(void) {
+#ifdef __Xxy
+  scratch_mem[0] = scratch_mem_x;
+  scratch_mem[1] = scratch_mem_y;
+  scratch_mem[2] = scratch_mem_z;
+  scratch_sizes[0] = SCRATCH_MEM_X_SIZE;
+  scratch_sizes[1] = SCRATCH_MEM_Y_SIZE;
+  scratch_sizes[2] = SCRATCH_MEM_Z_SIZE;
+#elif defined(__Xvdsp)
+  scratch_mem[0] = scratch_mem_vec_1;
+  scratch_mem[1] = scratch_mem_vec_2;
+  scratch_mem[2] = scratch_mem_vec_3;
+  scratch_sizes[0] = SCRATCH_MEM_VEC_SIZE / 4;
+  scratch_sizes[1] = SCRATCH_MEM_VEC_SIZE / 4;
+  scratch_sizes[2] = SCRATCH_MEM_VEC_SIZE / 2;
+#else
+  scratch_mem[0] = scratch_mem_stack;
+  scratch_sizes[0] = SCRATCH_MEM_SIZE;
+#endif
+}
+
+}  // namespace micro
+}  // namespace ops
+}  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.h b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.h
new file mode 100644
index 00000000..645781bc
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/arc_mli/scratch_buffers.h
@@ -0,0 +1,78 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_MICRO_ARC_SCRATCH_BUFFERS_H_
+#define TENSORFLOW_LITE_MICRO_ARC_SCRATCH_BUFFERS_H_
+
+#include "mli_api.h"  // NOLINT
+#include "tensorflow/lite/c/common.h"
+
+namespace tflite {
+namespace ops {
+namespace micro {
+
+void init_arc_scratch_buffers(void);
+void* get_arc_scratch_buffer(int size);  // Function to assign fast memory
+                                         // from one of 3 scratch buffers.
+
+void get_arc_scratch_buffer_max_size(int* size);
+void get_arc_scratch_buffer_two_max_sizes(int* size1, int* size2);
+
+static inline bool inside_arc_dccm(void* p) {
+#if core_config_dccm_present
+  return ((unsigned)p >= core_config_dccm_base) &&
+         ((unsigned)p < core_config_dccm_base + core_config_dccm_size);
+#else
+  return false;
+#endif
+}
+
+static inline bool inside_arc_xccm(void* p) {
+#if core_config_xy
+  return ((unsigned)p >= core_config_xy_x_base) &&
+         ((unsigned)p < core_config_xy_x_base + core_config_xy_size);
+#else
+  return false;
+#endif
+}
+
+static inline bool inside_arc_yccm(void* p) {
+#if core_config_xy_size
+  return ((unsigned)p >= core_config_xy_y_base) &&
+         ((unsigned)p < core_config_xy_y_base + core_config_xy_size);
+#else
+  return false;
+#endif
+}
+
+static inline bool inside_arc_vccm(void* p) {
+#if core_config_vec_mem_size
+  return ((unsigned)p >= core_config_vec_mem_base) &&
+         ((unsigned)p < core_config_vec_mem_base + core_config_vec_mem_size);
+#else
+  return false;
+#endif
+}
+
+static inline bool inside_arc_ccm(void* p) {
+  return inside_arc_dccm(p) || inside_arc_xccm(p) || inside_arc_yccm(p) ||
+         inside_arc_vccm(p);
+}
+
+}  // namespace micro
+}  // namespace ops
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_MICRO_ARC_SCRATCH_BUFFERS_H_
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/audio_spectrogram.cc b/third_party/tflite-micro/tensorflow/lite/micro/kernels/audio_spectrogram.cc
new file mode 100644
index 00000000..772ec26e
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/audio_spectrogram.cc
@@ -0,0 +1,263 @@
+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <math.h>
+#include <stddef.h>
+#include <stdint.h>
+#include <cstdio>
+
+
+#include "flatbuffers/flexbuffers.h"  // from @flatbuffers
+#include "tensorflow/lite/c/common.h"
+//#include "tensorflow/lite/kernels/internal/optimized/optimized_ops.h"
+//#include "tensorflow/lite/kernels/internal/reference/reference_ops.h"
+#include "tensorflow/lite/kernels/internal/spectrogram.h"
+//#include "tensorflow/lite/kernels/internal/tensor.h"
+#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
+#include "tensorflow/lite/kernels/kernel_util.h"
+#include "tensorflow/lite/micro/kernels/kernel_util.h"
+#include "tensorflow/lite/micro/memory_helpers.h"
+#include "tensorflow/lite/kernels/op_macros.h"
+
+
+
+namespace tflite {
+namespace ops {
+namespace micro {
+//namespace custom{
+namespace audio_spectrogram {
+
+constexpr int kInputTensor = 0;
+constexpr int kOutputTensor = 0;
+
+/*
+
+inline int Log2Floor(uint32_t n) {
+  if (n == 0) return -1;
+  int log = 0;
+  uint32_t value = n;
+  for (int i = 4; i >= 0; --i) {
+    int shift = (1 << i);
+    uint32_t x = value >> shift;
+    if (x != 0) {
+      value = x;
+      log += shift;
+    }
+  }
+  return log;
+}
+
+inline int Log2Ceiling(uint32_t n) {
+  int floor = Log2Floor(n);
+  if (n == (n & ~(n - 1)))  // zero or a power of two
+    return floor;
+  else
+    return floor + 1;
+}
+
+inline uint32_t NextPowerOfTwo(uint32_t value) {
+  int exponent = Log2Ceiling(value);
+  // DCHECK_LT(exponent, std::numeric_limits<uint32>::digits);
+  return 1 << exponent;
+}
+*/
+
+enum KernelType {
+  kReference,
+};
+
+typedef struct {
+  int window_size;
+  int stride;
+  bool magnitude_squared;
+  int output_height;
+  int idx_for_spec_output;
+  int idx_for_input_channel;
+  internal::Spectrogram spectrogram;
+} TfLiteAudioSpectrogramParams;
+
+
+
+void* Init(TfLiteContext* context, const char* buffer, size_t length) {
+
+  const uint8_t* buffer_t = reinterpret_cast<const uint8_t*>(buffer);
+  const flexbuffers::Map& m = flexbuffers::GetRoot(buffer_t, length).AsMap();
+  
+  // allocate buffer
+  TFLITE_DCHECK(context->AllocatePersistentBuffer != nullptr);
+  void *ptr = context->AllocatePersistentBuffer(context, sizeof(TfLiteAudioSpectrogramParams));
+
+  // assign custom_op_value
+  auto *params = reinterpret_cast<TfLiteAudioSpectrogramParams*>(ptr);
+  params->window_size = m["window_size"].AsInt64();
+  params->stride = m["stride"].AsInt64();
+  params->magnitude_squared = m["magnitude_squared"].AsBool();
+
+  return ptr;
+}
+
+
+TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
+
+  MicroContext* micro_context = GetMicroContext(context);
+  auto* params = reinterpret_cast<TfLiteAudioSpectrogramParams*>(node->user_data);
+  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
+  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
+  //const TfLiteTensor* input = GetInput(context, node, kInputTensor);
+  //TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
+  TfLiteTensor* input =
+      micro_context->AllocateTempInputTensor(node, kInputTensor);
+  TF_LITE_ENSURE(context, input != nullptr);
+  TfLiteTensor* output =
+      micro_context->AllocateTempOutputTensor(node, kOutputTensor);
+  TF_LITE_ENSURE(context, output != nullptr);
+  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 2);
+  TF_LITE_ENSURE_TYPES_EQ(context, output->type, kTfLiteFloat32);
+  TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);
+  
+
+  const int64_t sample_count = input->dims->data[0];
+  const int64_t length_minus_window = (sample_count - params->window_size);
+  if (length_minus_window < 0) {
+    params->output_height = 0;
+  } else {
+    params->output_height = 1 + (length_minus_window / params->stride);
+  }
+
+
+  // allocate buffer
+  TFLITE_DCHECK(context->RequestScratchBufferInArena != nullptr);
+  const TfLiteStatus scratch_input_for_channel = context->RequestScratchBufferInArena(
+        context, sample_count * sizeof(float), &(params->idx_for_input_channel));
+  TF_LITE_ENSURE_OK(context, scratch_input_for_channel);
+
+  //int fft_length = NextPowerOfTwo(params->window_size); // 1024
+  //int output_freqency_channels = 1 + fft_length >> 1;
+  int fft_length = 1024;
+  int output_freqency_channels = 513;
+  params->window_size = 640;
+  params->stride = 320;
+  const TfLiteStatus scratch_spectrogram_output = context->RequestScratchBufferInArena(
+        context, output_freqency_channels * params->output_height * sizeof(float), &(params->idx_for_spec_output));
+  TF_LITE_ENSURE_OK(context, scratch_spectrogram_output);
+  TF_LITE_ENSURE(context, params->spectrogram.Initialize(params->window_size, params->stride, input->dims->data[0], fft_length, output_freqency_channels));
+  /*
+  printf("output->[0] = %d\n", output->dims->data[0]);
+  printf("output->[1] = %d\n", output->dims->data[1]);
+  printf("output->[2] = %d\n", output->dims->data[2]);
+  */
+
+  micro_context->DeallocateTempTfLiteTensor(input);
+  micro_context->DeallocateTempTfLiteTensor(output);
+  return kTfLiteOk;
+}
+
+template <KernelType kernel_type>
+TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
+
+  auto* params =
+      reinterpret_cast<TfLiteAudioSpectrogramParams*>(node->user_data);
+
+  const TfLiteEvalTensor* input =  tflite::micro::GetEvalInput(context, node, kInputTensor);
+  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+
+  // get allocate buffer
+  TFLITE_DCHECK(context->GetScratchBuffer != nullptr);
+  float* spectrogram_output = static_cast<float*>(context->GetScratchBuffer(context, params->idx_for_spec_output));
+  float* input_for_channel = static_cast<float*>(context->GetScratchBuffer(context, params->idx_for_input_channel));
+
+  //TF_LITE_ENSURE(context, params->spectrogram.Initialize(params->window_size, params->stride, input->dims->data[0]));
+
+  const int64_t sample_count = input->dims->data[0]; // non stream : 16000 , stream : 640
+  const int64_t channel_count = input->dims->data[1]; // 1
+  const int64_t output_width = params->spectrogram.output_frequency_channels();
+
+  const float* input_data = tflite::micro::GetTensorData<float>(input);
+  float* output_flat = tflite::micro::GetTensorData<float>(output);
+  
+  //printf("sample_count = %d\n", sample_count);
+  //printf("channel count = %d\n", input->dims->data[1]);
+  //printf("params->output_height = %d\n", params->output_height); // non stream :Ã£â‚¬â‚¬49 , stream : 1
+  //printf("output_width = %d\n", output_width); // 513
+  //printf("params->output_height = %d\n", params->output_height); // 49
+
+
+  //std::vector<float> input_for_channel(sample_count);
+  //float* input_for_channel = params->spectrogram.get_input_for_channel_();
+  //float* spectrogram_output = params->spectrogram.get_spectrogram_output_();
+
+  for (int64_t channel = 0; channel < channel_count; ++channel) {
+    float* output_slice =
+        output_flat + (channel * params->output_height * output_width);
+
+    memcpy(input_for_channel, input_data, sample_count * sizeof(float));
+    /*
+    for (int i = 0; i < sample_count; ++i) {
+      input_for_channel[i] = input_data[i * channel_count + channel]; // channel_count = 1, channel = 0
+    }
+    */
+    
+    //std::vector<std::vector<float>> spectrogram_output;
+
+    TF_LITE_ENSURE(context,
+                   params->spectrogram.ComputeSquaredMagnitudeSpectrogram(
+                       input_for_channel, spectrogram_output));
+                 
+                    
+    //TF_LITE_ENSURE_EQ(context, spectrogram_output.size(), params->output_height);
+    //TF_LITE_ENSURE(context, spectrogram_output.empty() || (spectrogram_output[0].size() == output_width));
+    
+    for (int row_index = 0; row_index < params->output_height; ++row_index) {
+
+      const float* spectrogram_row = spectrogram_output + (row_index * output_width);
+      float* output_row = output_slice + (row_index * output_width);
+      
+      memcpy(output_row, spectrogram_row, output_width * sizeof(float));
+      /* 
+      if (params->magnitude_squared) {
+        for (int i = 0; i < output_width; ++i) {
+          output_row[i] = spectrogram_row[i];
+        }
+      } else {
+        for (int i = 0; i < output_width; ++i) {
+          output_row[i] = sqrtf(spectrogram_row[i]);
+        }
+      }
+      */
+    }
+  }
+  return kTfLiteOk;
+}
+
+}  // namespace audio_spectrogram
+
+TfLiteRegistration* Register_AUDIO_SPECTROGRAM() {
+  static TfLiteRegistration r = {
+      audio_spectrogram::Init, 
+      /*free=*/nullptr,
+      //audio_spectrogram::Free,
+      audio_spectrogram::Prepare,
+      audio_spectrogram::Eval<audio_spectrogram::kReference>,
+      /*profiling_string=*/nullptr,
+      /*builtin_code=*/0,
+      /*custom_name=*/nullptr,
+      /*version=*/0};
+  return &r;
+}
+
+//}  // namespace custom
+}  // namespace micro
+}  // namespace ops
+}  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/mfcc.cc b/third_party/tflite-micro/tensorflow/lite/micro/kernels/mfcc.cc
new file mode 100644
index 00000000..8c65d739
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/mfcc.cc
@@ -0,0 +1,257 @@
+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+#include "tensorflow/lite/kernels/internal/mfcc.h"
+
+#include <stddef.h>
+#include <stdint.h>
+
+
+#include "flatbuffers/flexbuffers.h"  // from @flatbuffers
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/kernels/internal/compatibility.h"
+#include "tensorflow/lite/kernels/internal/mfcc_dct.h"
+#include "tensorflow/lite/kernels/internal/mfcc_mel_filterbank.h"
+//#include "tensorflow/lite/kernels/internal/optimized/optimized_ops.h"
+//#include "tensorflow/lite/kernels/internal/reference/reference_ops.h"
+//#include "tensorflow/lite/kernels/internal/tensor.h"
+#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
+#include "tensorflow/lite/kernels/kernel_util.h"
+#include "tensorflow/lite/micro/kernels/kernel_util.h"
+#include "tensorflow/lite/micro/memory_helpers.h"
+
+//#include <embARC_debug.h>
+#define DBG_APP_PRINT_LEVEL 0
+
+
+#include "stdio.h"
+#include <cstring>
+
+float mfcc_input[513]; //save stack memory
+float mfcc_output[30];
+
+namespace tflite {
+namespace ops {
+namespace micro {
+namespace mfcc {
+
+enum KernelType {
+  kReference,
+};
+
+typedef struct {
+  internal::Mfcc mfcc;
+} TfLiteMfccParams;
+
+constexpr int kInputTensorWav = 0;
+constexpr int kInputTensorRate = 1;
+constexpr int kOutputTensor = 0;
+
+/*
+char* uint32_to_dec_cstring(char buf[11], uint32_t n) {
+  for (int i{9}; i >= 0; --i) {
+    // printf("iterating\n");
+    buf[i] = '0' + n % 10;
+    n /= 10;
+  }
+  buf[10] = '\0';
+  return buf;
+}
+*/
+
+/*
+char* int32_to_dec_cstring(char buf[11], int32_t n) {
+  bool is_n = n < 0;
+  if(is_n) n = n * -1;
+  for (int i{9}; i >= 0; --i) {
+    // printf("iterating\n");
+    buf[i] = '0' + n % 10;
+    n /= 10;
+  }
+  buf[10] = '\0';
+  if(is_n) buf[0] = '-';
+  return buf;
+}
+*/
+
+/*
+  fflush(stdout);
+  char num[11];
+  int32_to_dec_cstring(num, (int)(mfcc_input[i]*100000));
+  strcat(num," ");
+  fwrite(num, sizeof(char), sizeof(num), stderr);
+
+  fflush(stdout);
+*/
+//dbg_printf(DBG_APP_PRINT_LEVEL, "[%s] %s:%d\n", __FILE__, __func__, __LINE__);
+
+
+void* Init(TfLiteContext* context, const char* buffer, size_t length) {
+  
+  // map flexbuffer and get custom_data_type
+  const uint8_t* buffer_t = reinterpret_cast<const uint8_t*>(buffer);
+  const flexbuffers::Map& m = flexbuffers::GetRoot(buffer_t, length).AsMap();
+
+  // allocate
+  TFLITE_DCHECK(context->AllocatePersistentBuffer != nullptr);
+  void *ptr = context->AllocatePersistentBuffer(context, sizeof(TfLiteMfccParams));
+
+  // assign values
+  auto *params = reinterpret_cast<TfLiteMfccParams*>(ptr);
+  params->mfcc.set_upper_frequency_limit(m["upper_frequency_limit"].AsInt64());
+  params->mfcc.set_lower_frequency_limit(m["lower_frequency_limit"].AsInt64());
+  params->mfcc.set_filterbank_channel_count(m["filterbank_channel_count"].AsInt64());
+  params->mfcc.set_dct_coefficient_count(m["dct_coefficient_count"].AsInt64());
+
+  return ptr;
+}
+
+
+
+TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
+
+  MicroContext* micro_context = GetMicroContext(context);
+  auto* params = reinterpret_cast<TfLiteMfccParams*>(node->user_data);
+
+  TF_LITE_ENSURE_EQ(context, NumInputs(node), 2);
+  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
+
+  //const TfLiteTensor* input_wav = GetInput(context, node, kInputTensorWav);
+  //const TfLiteTensor* input_rate = GetInput(context, node, kInputTensorRate);
+  //TfLiteTensor* output = GetOutput(context, node, kOutputTensor);
+
+  TfLiteTensor* input_wav =
+      micro_context->AllocateTempInputTensor(node, kInputTensorWav);
+  TF_LITE_ENSURE(context, input_wav != nullptr);
+  TfLiteTensor* input_rate =
+      micro_context->AllocateTempInputTensor(node, kInputTensorRate);
+  TF_LITE_ENSURE(context, input_rate != nullptr);
+  TfLiteTensor* output =
+      micro_context->AllocateTempOutputTensor(node, kOutputTensor);
+  TF_LITE_ENSURE(context, output != nullptr);
+
+
+  TF_LITE_ENSURE_EQ(context, NumDimensions(input_wav), 3);
+  TF_LITE_ENSURE_EQ(context, NumElements(input_rate), 1);
+
+  TF_LITE_ENSURE_TYPES_EQ(context, output->type, kTfLiteFloat32);
+  TF_LITE_ENSURE_TYPES_EQ(context, input_wav->type, output->type);
+  TF_LITE_ENSURE_TYPES_EQ(context, input_rate->type, kTfLiteInt32);
+  
+  //const int spectrogram_channels = input_wav->dims->data[2];
+
+  params->mfcc.Initialize(input_wav->dims->data[2], 16000);
+
+  /*
+  printf("output->dims->data[0] = %d\n", output->dims->data[0]);
+  printf("output->dims->data[1] = %d\n", output->dims->data[1]);
+  printf("output->dims->data[2] = %d\n", output->dims->data[2]);
+  */
+  micro_context->DeallocateTempTfLiteTensor(input_wav);
+  micro_context->DeallocateTempTfLiteTensor(input_rate);
+  micro_context->DeallocateTempTfLiteTensor(output);
+  return kTfLiteOk;
+}
+
+
+// Input is a single squared-magnitude spectrogram frame. The input spectrum
+// is converted to linear magnitude and weighted into bands using a
+// triangular mel filterbank, and a discrete cosine transform (DCT) of the
+// values is taken. Output is populated with the lowest dct_coefficient_count
+// of these values.
+template <KernelType kernel_type>
+TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
+
+  auto* params = reinterpret_cast<TfLiteMfccParams*>(node->user_data);
+
+  const TfLiteEvalTensor* input_wav = tflite::micro::GetEvalInput(context, node, kInputTensorWav);
+  //const TfLiteEvalTensor* input_rate = tflite::micro::GetEvalInput(context, node, kInputTensorRate);
+  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, kOutputTensor);
+  //const int32_t sample_rate = *tflite::micro::GetTensorData<int>(input_rate);
+
+  const int spectrogram_channels = input_wav->dims->data[2];
+  const int spectrogram_samples = input_wav->dims->data[1];
+  const int audio_channels = input_wav->dims->data[0];
+  //internal::Mfcc mfcc;
+
+  //mfcc.set_upper_frequency_limit(params->upper_frequency_limit);
+  //mfcc.set_lower_frequency_limit(params->lower_frequency_limit);
+  //mfcc.set_filterbank_channel_count(params->filterbank_channel_count);
+  //mfcc.set_dct_coefficient_count(params->dct_coefficient_count);
+  
+  // printf("spectrogram_channels = %d\n", spectrogram_channels); // 513
+  // printf("params->dct_coefficient_count = %d\n", params->dct_coefficient_count); // 30
+  // printf("audio_channels = %d\n", audio_channels); // 1
+  // printf("spectrogram_samples = %d\n", spectrogram_samples); // 49
+
+  //mfcc.Initialize(spectrogram_channels, sample_rate);
+
+  const float* spectrogram_flat = tflite::micro::GetTensorData<float>(input_wav);
+  float* output_flat = tflite::micro::GetTensorData<float>(output);
+ 
+  
+  for (int audio_channel = 0; audio_channel < audio_channels; ++audio_channel) {
+    for (int spectrogram_sample = 0; spectrogram_sample < spectrogram_samples;
+         ++spectrogram_sample) { // [0, 48]
+      const float* sample_data =
+          spectrogram_flat +
+          (audio_channel * spectrogram_samples * spectrogram_channels) +
+          (spectrogram_sample * spectrogram_channels); 
+      
+      // std::vector<double> mfcc_input(sample_data, sample_data + spectrogram_channels);
+      // std::vector<double> mfcc_output;
+      memcpy(mfcc_input, sample_data, spectrogram_channels * sizeof(float));
+      /*
+      for (int i{0}; i < spectrogram_channels; ++i) 
+      {
+        mfcc_input[i] = sample_data[i];   
+      }
+      */
+      
+      
+      params->mfcc.Compute(mfcc_input, spectrogram_channels, mfcc_output);
+
+      //TF_LITE_ENSURE_EQ(context, params->dct_coefficient_count, 30); 
+      int dct_coefficient_count = params->mfcc.get_dct_coefficient_count();
+      float* output_data = output_flat +
+                           (audio_channel * spectrogram_samples *
+                            dct_coefficient_count) +
+                           (spectrogram_sample * dct_coefficient_count);
+
+      memcpy(output_data, mfcc_output, dct_coefficient_count * sizeof(float));
+      /*
+      for (int i = 0; i < params->dct_coefficient_count; ++i) {
+        output_data[i] = mfcc_output[i];
+      }
+      */
+    }
+  }
+
+  return kTfLiteOk;
+}
+
+}  // namespace mfcc
+
+TfLiteRegistration* Register_MFCC() {
+  static TfLiteRegistration r = {mfcc::Init, 
+                                  nullptr,
+                                 //mfcc::Free, 
+                                 mfcc::Prepare,
+                                 mfcc::Eval<mfcc::kReference>};
+  return &r;
+}
+
+}  // namespace custom
+}  // namespace ops
+}  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/micro_ops.h b/third_party/tflite-micro/tensorflow/lite/micro/kernels/micro_ops.h
index df2a8d2c..d750958f 100644
--- a/third_party/tflite-micro/tensorflow/lite/micro/kernels/micro_ops.h
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/micro_ops.h
@@ -127,7 +127,10 @@ TfLiteRegistration Register_STRIDED_SLICE();
 TfLiteRegistration Register_UNPACK();
 TfLiteRegistration Register_L2_NORMALIZATION();
 TfLiteRegistration Register_TANH();
-
+//TfLiteRegistration* Register_ETHOSU();
+TfLiteRegistration* Register_AUDIO_SPECTROGRAM();
+TfLiteRegistration* Register_MFCC();
+TfLiteRegistration Register_SUM();
 }  // namespace micro
 }  // namespace ops
 }  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/micro_ops.h.orig b/third_party/tflite-micro/tensorflow/lite/micro/kernels/micro_ops.h.orig
new file mode 100644
index 00000000..df2a8d2c
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/micro_ops.h.orig
@@ -0,0 +1,135 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+#ifndef TENSORFLOW_LITE_MICRO_KERNELS_MICRO_OPS_H_
+#define TENSORFLOW_LITE_MICRO_KERNELS_MICRO_OPS_H_
+
+#include "tensorflow/lite/c/common.h"
+
+// Forward declaration of all micro op kernel registration methods. These
+// registrations are included with the standard `BuiltinOpResolver`.
+//
+// This header is particularly useful in cases where only a subset of ops are
+// needed. In such cases, the client can selectively add only the registrations
+// their model requires, using a custom `(Micro)MutableOpResolver`. Selective
+// registration in turn allows the linker to strip unused kernels.
+
+namespace tflite {
+
+// TFLM is incrementally moving towards a flat tflite namespace
+// (https://abseil.io/tips/130). Any new ops (or cleanup of existing ops should
+// have their Register function declarations in the tflite namespace.
+
+TfLiteRegistration Register_ADD();
+TfLiteRegistration Register_ADD_N();
+TfLiteRegistration Register_ARG_MAX();
+TfLiteRegistration Register_ARG_MIN();
+TfLiteRegistration Register_ASSIGN_VARIABLE();
+TfLiteRegistration Register_AVERAGE_POOL_2D();
+TfLiteRegistration Register_BATCH_TO_SPACE_ND();
+TfLiteRegistration Register_BROADCAST_ARGS();
+TfLiteRegistration Register_BROADCAST_TO();
+TfLiteRegistration Register_CALL_ONCE();
+TfLiteRegistration Register_CAST();
+// TODO(b/160234179): Change custom OPs to also return by value.
+TfLiteRegistration* Register_CIRCULAR_BUFFER();
+TfLiteRegistration Register_CUMSUM();
+TfLiteRegistration Register_DEPTH_TO_SPACE();
+TfLiteRegistration Register_DEPTHWISE_CONV_2D();
+TfLiteRegistration Register_DEQUANTIZE();
+TfLiteRegistration Register_DIV();
+TfLiteRegistration Register_ELU();
+TfLiteRegistration Register_EXP();
+TfLiteRegistration Register_EXPAND_DIMS();
+TfLiteRegistration Register_FILL();
+TfLiteRegistration Register_FLOOR_DIV();
+TfLiteRegistration Register_FLOOR_MOD();
+TfLiteRegistration Register_GATHER();
+TfLiteRegistration Register_GATHER_ND();
+TfLiteRegistration Register_HARD_SWISH();
+TfLiteRegistration Register_IF();
+TfLiteRegistration Register_L2_POOL_2D();
+TfLiteRegistration Register_LEAKY_RELU();
+TfLiteRegistration Register_LOG_SOFTMAX();
+TfLiteRegistration Register_LOGICAL_AND();
+TfLiteRegistration Register_LOGICAL_OR();
+TfLiteRegistration Register_LOGISTIC();
+TfLiteRegistration Register_MAX_POOL_2D();
+TfLiteRegistration Register_MIRROR_PAD();
+TfLiteRegistration Register_NEG();
+TfLiteRegistration Register_PRELU();
+TfLiteRegistration Register_MUL();
+TfLiteRegistration Register_PAD();
+TfLiteRegistration Register_PADV2();
+TfLiteRegistration Register_QUANTIZE();
+TfLiteRegistration Register_READ_VARIABLE();
+TfLiteRegistration Register_RELU();
+TfLiteRegistration Register_RELU6();
+TfLiteRegistration Register_RESIZE_BILINEAR();
+TfLiteRegistration Register_SELECT_V2();
+TfLiteRegistration Register_SHAPE();
+TfLiteRegistration Register_SLICE();
+TfLiteRegistration Register_SPACE_TO_BATCH_ND();
+TfLiteRegistration Register_SPACE_TO_DEPTH();
+TfLiteRegistration Register_SQUARED_DIFFERENCE();
+TfLiteRegistration Register_SQUEEZE();
+TfLiteRegistration Register_SUB();
+TfLiteRegistration Register_SUM();
+TfLiteRegistration Register_SVDF();
+TfLiteRegistration Register_TRANSPOSE();
+TfLiteRegistration Register_TRANSPOSE_CONV();
+// TODO(b/230666079): resolve conflict with xtensa implementation
+TfLiteRegistration Register_UNIDIRECTIONAL_SEQUENCE_LSTM();
+TfLiteRegistration Register_VAR_HANDLE();
+TfLiteRegistration Register_WHILE();
+TfLiteRegistration Register_ZEROS_LIKE();
+
+namespace ops {
+namespace micro {
+
+TfLiteRegistration Register_ABS();
+TfLiteRegistration Register_CEIL();
+TfLiteRegistration Register_CONCATENATION();
+TfLiteRegistration Register_COS();
+TfLiteRegistration Register_EQUAL();
+TfLiteRegistration Register_FLOOR();
+TfLiteRegistration Register_GREATER();
+TfLiteRegistration Register_GREATER_EQUAL();
+TfLiteRegistration Register_LESS();
+TfLiteRegistration Register_LESS_EQUAL();
+TfLiteRegistration Register_LOG();
+TfLiteRegistration Register_LOGICAL_NOT();
+TfLiteRegistration Register_MAXIMUM();
+TfLiteRegistration Register_MINIMUM();
+TfLiteRegistration Register_NOT_EQUAL();
+TfLiteRegistration Register_PACK();
+TfLiteRegistration Register_RESHAPE();
+TfLiteRegistration Register_RESIZE_NEAREST_NEIGHBOR();
+TfLiteRegistration Register_ROUND();
+TfLiteRegistration Register_RSQRT();
+TfLiteRegistration Register_SIN();
+TfLiteRegistration Register_SPLIT();
+TfLiteRegistration Register_SPLIT_V();
+TfLiteRegistration Register_SQRT();
+TfLiteRegistration Register_SQUARE();
+TfLiteRegistration Register_STRIDED_SLICE();
+TfLiteRegistration Register_UNPACK();
+TfLiteRegistration Register_L2_NORMALIZATION();
+TfLiteRegistration Register_TANH();
+
+}  // namespace micro
+}  // namespace ops
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_MICRO_KERNELS_MICRO_OPS_H_
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/micro_ops.h.rej b/third_party/tflite-micro/tensorflow/lite/micro/kernels/micro_ops.h.rej
new file mode 100644
index 00000000..7387fc9a
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/micro_ops.h.rej
@@ -0,0 +1,14 @@
+--- tensorflow/lite/micro/kernels/micro_ops.h	2021-04-09 13:12:53.000000000 +0800
++++ tensorflow/lite/micro/kernels/micro_ops.h	2023-07-08 21:57:18.178561318 +0800
+@@ -86,7 +86,10 @@
+ TfLiteRegistration Register_UNPACK();
+ TfLiteRegistration Register_L2_NORMALIZATION();
+ TfLiteRegistration Register_TANH();
+-
++//TfLiteRegistration* Register_ETHOSU();
++TfLiteRegistration* Register_AUDIO_SPECTROGRAM();
++TfLiteRegistration* Register_MFCC();
++TfLiteRegistration Register_SUM();
+ }  // namespace micro
+ }  // namespace ops
+ }  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/reduce.cc b/third_party/tflite-micro/tensorflow/lite/micro/kernels/reduce.cc
index b4734f93..e8370a75 100644
--- a/third_party/tflite-micro/tensorflow/lite/micro/kernels/reduce.cc
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/reduce.cc
@@ -64,9 +64,21 @@ TfLiteRegistration Register_MEAN() {
 TfLiteRegistration Register_REDUCE_MAX() {
   return tflite::micro::RegisterOp(InitReduce, PrepareMax, EvalMax);
 }
-
+/*
 TfLiteRegistration Register_SUM() {
   return tflite::micro::RegisterOp(InitReduce, PrepareMeanOrSum, EvalSum);
 }
+*/
+TfLiteRegistration Register_SUM() {
+  return {/*init=*/InitReduce,
+          /*free=*/nullptr,
+          /*prepare=*/PrepareMeanOrSum,
+          /*invoke=*/EvalSum,
+          /*profiling_string=*/nullptr,
+          /*builtin_code=*/0,
+          /*custom_name=*/nullptr,
+          /*version=*/0};
+}
+
 
 }  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/reduce.cc.orig b/third_party/tflite-micro/tensorflow/lite/micro/kernels/reduce.cc.orig
new file mode 100644
index 00000000..b4734f93
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/reduce.cc.orig
@@ -0,0 +1,72 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/kernels/internal/reference/reduce.h"
+
+#include "tensorflow/lite/c/builtin_op_data.h"
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/kernels/internal/quantization_util.h"
+#include "tensorflow/lite/kernels/internal/reference/integer_ops/mean.h"
+#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
+#include "tensorflow/lite/kernels/internal/types.h"
+#include "tensorflow/lite/kernels/kernel_util.h"
+#include "tensorflow/lite/micro/kernels/kernel_util.h"
+#include "tensorflow/lite/micro/kernels/reduce.h"
+#include "tensorflow/lite/micro/micro_utils.h"
+
+namespace tflite {
+
+void* InitReduce(TfLiteContext* context, const char* buffer, size_t length) {
+  return context->AllocatePersistentBuffer(context, sizeof(OpDataReduce));
+}
+
+TfLiteStatus PrepareMax(TfLiteContext* context, TfLiteNode* node) {
+  return PrepareMaxHelper(context, node,
+                          static_cast<OpDataReduce*>(node->user_data));
+}
+
+TfLiteStatus PrepareMeanOrSum(TfLiteContext* context, TfLiteNode* node) {
+  return PrepareMeanOrSumHelper(context, node,
+                                static_cast<OpDataReduce*>(node->user_data));
+}
+
+TfLiteStatus EvalMean(TfLiteContext* context, TfLiteNode* node) {
+  return EvalMeanHelper(context, node,
+                        static_cast<OpDataReduce*>(node->user_data));
+}
+
+TfLiteStatus EvalMax(TfLiteContext* context, TfLiteNode* node) {
+  OpDataReduce* op_data = static_cast<OpDataReduce*>(node->user_data);
+  return EvalMaxHelper(context, node, op_data);
+}
+
+TfLiteStatus EvalSum(TfLiteContext* context, TfLiteNode* node) {
+  return EvalSumHelper(context, node,
+                       static_cast<OpDataReduce*>(node->user_data));
+}
+
+TfLiteRegistration Register_MEAN() {
+  return tflite::micro::RegisterOp(InitReduce, PrepareMeanOrSum, EvalMean);
+}
+
+TfLiteRegistration Register_REDUCE_MAX() {
+  return tflite::micro::RegisterOp(InitReduce, PrepareMax, EvalMax);
+}
+
+TfLiteRegistration Register_SUM() {
+  return tflite::micro::RegisterOp(InitReduce, PrepareMeanOrSum, EvalSum);
+}
+
+}  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/kernels/reduce.cc.rej b/third_party/tflite-micro/tensorflow/lite/micro/kernels/reduce.cc.rej
new file mode 100644
index 00000000..ef8e1c22
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/kernels/reduce.cc.rej
@@ -0,0 +1,97 @@
+--- tensorflow/lite/micro/kernels/reduce.cc	2021-04-09 13:12:53.000000000 +0800
++++ tensorflow/lite/micro/kernels/reduce.cc	2023-06-14 23:33:52.669187981 +0800
+@@ -315,6 +315,76 @@
+   return kTfLiteOk;
+ }
+ 
++TfLiteStatus EvalSum(TfLiteContext* context, TfLiteNode* node) {
++  const TfLiteEvalTensor* input = tflite::micro::GetEvalInput(context, node, 0);
++  const TfLiteEvalTensor* axis = tflite::micro::GetEvalInput(context, node, 1);
++  TfLiteEvalTensor* output = tflite::micro::GetEvalOutput(context, node, 0);
++  TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);
++  TfLiteReducerParams* params =
++      static_cast<TfLiteReducerParams*>(node->builtin_data);
++  OpData* op_data = reinterpret_cast<OpData*>(node->user_data);
++  // Interpret an axis tensor with null dimensions as a scalar.
++  int num_axis = static_cast<int>(ElementCount(*axis->dims));
++  int temp_index[kMaxNumberOfAxis];
++  int resolved_axis[kMaxNumberOfReducedAxis];
++
++  switch (input->type) {
++    case kTfLiteFloat32: {
++      TF_LITE_ENSURE(
++          context,
++          reference_ops::ReduceGeneric<float>(
++              tflite::micro::GetTensorData<float>(input), input->dims->data,
++              input->dims->size, tflite::micro::GetTensorData<float>(output),
++              output->dims->data, output->dims->size,
++              tflite::micro::GetTensorData<int>(axis), num_axis,
++              params->keep_dims, temp_index, resolved_axis, /*init_value=*/0.f,
++              [](const float current, const float in) -> float {
++                return in + current;
++              }));
++    } break;
++    case kTfLiteInt8: {
++      int32_t* temp_sum = static_cast<int32_t*>(
++          context->GetScratchBuffer(context, op_data->temp_buffer_idx));
++      TF_LITE_ENSURE(
++            context,
++            reference_ops::QuantizedMeanOrSum(
++                tflite::micro::GetTensorData<int8_t>(input), op_data->input_zp,
++                op_data->input_scale, input->dims->data, input->dims->size,
++                tflite::micro::GetTensorData<int8_t>(output),
++                op_data->output_zp, op_data->output_scale, output->dims->data,
++                output->dims->size, tflite::micro::GetTensorData<int>(axis),
++                num_axis, params->keep_dims, temp_index, resolved_axis,
++                temp_sum, true));
++    } break;
++    case kTfLiteInt16: {
++      int32_t* temp_sum = static_cast<int32_t*>(
++          context->GetScratchBuffer(context, op_data->temp_buffer_idx));
++      TF_LITE_ENSURE(
++            context,
++            reference_ops::QuantizedMeanOrSum(
++                tflite::micro::GetTensorData<int16_t>(input), op_data->input_zp,
++                op_data->input_scale, input->dims->data, input->dims->size,
++                tflite::micro::GetTensorData<int16_t>(output),
++                op_data->output_zp, op_data->output_scale, output->dims->data,
++                output->dims->size, tflite::micro::GetTensorData<int>(axis),
++                num_axis, params->keep_dims, temp_index, resolved_axis,
++                temp_sum, true));
++    } break;
++    default:
++      TF_LITE_KERNEL_LOG(context,"Only float32, int8, and int16 types are supported.");
++      return kTfLiteError;
++  }
++  /*
++  for(int i{0}; i < output->dims->data[0]; i++)
++  {
++    for(int j{0}; j < output->dims->data[1]; j++)
++    {
++      printf("%d ", (int)(*(output + i * 128 + j) * 100000));
++    }
++  }
++  */
++  return kTfLiteOk;
++}
+ }  // namespace reduce
+ 
+ TfLiteRegistration Register_MEAN() {
+@@ -339,6 +409,17 @@
+           /*version=*/0};
+ }
+ 
++TfLiteRegistration Register_SUM() {
++  return {/*init=*/reduce::InitReduce,
++          /*free=*/nullptr,
++          /*prepare=*/reduce::PrepareMeanOrSum,
++          /*invoke=*/reduce::EvalSum,
++          /*profiling_string=*/nullptr,
++          /*builtin_code=*/0,
++          /*custom_name=*/nullptr,
++          /*version=*/0};
++}
++
+ }  // namespace micro
+ }  // namespace ops
+ }  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/micro_interpreter.cc b/third_party/tflite-micro/tensorflow/lite/micro/micro_interpreter.cc
index 8bbfbb45..44e1854f 100644
--- a/third_party/tflite-micro/tensorflow/lite/micro/micro_interpreter.cc
+++ b/third_party/tflite-micro/tensorflow/lite/micro/micro_interpreter.cc
@@ -100,7 +100,7 @@ TfLiteStatus MicroInterpreter::PrepareNodeAndRegistrationDataFromFlatbuffer() {
       const auto* op = subgraph->operators()->Get(i);
       const size_t index = op->opcode_index();
       if (index >= opcodes->size()) {
-        MicroPrintf("Missing registration for opcode_index %d\n", index);
+        printf("Missing registration for opcode_index %d\n", index);
         return kTfLiteError;
       }
       const auto* opcode = opcodes->Get(index);
@@ -110,7 +110,7 @@ TfLiteStatus MicroInterpreter::PrepareNodeAndRegistrationDataFromFlatbuffer() {
                                           .node_and_registrations[i]
                                           .registration));
       if (status != kTfLiteOk) {
-        MicroPrintf("Failed to get registration from op code %s\n ",
+        printf("Failed to get registration from op code %s\n ",
                     EnumNameBuiltinOperator(GetBuiltinCode(opcode)));
         return status;
       }
@@ -118,7 +118,7 @@ TfLiteStatus MicroInterpreter::PrepareNodeAndRegistrationDataFromFlatbuffer() {
                                      .node_and_registrations[i]
                                      .registration;
       if (registration == nullptr) {
-        MicroPrintf("Skipping op for opcode_index %d\n", index);
+        printf("Skipping op for opcode_index %d\n", index);
         return kTfLiteError;
       }
       BuiltinOperator op_type =
@@ -137,7 +137,7 @@ TfLiteStatus MicroInterpreter::PrepareNodeAndRegistrationDataFromFlatbuffer() {
         }
       } else {
         if (op->custom_options() != nullptr) {
-          MicroPrintf(
+          printf(
               "Unsupported behavior: found builtin operator %s with custom "
               "options.\n",
               EnumNameBuiltinOperator(op_type));
@@ -147,7 +147,7 @@ TfLiteStatus MicroInterpreter::PrepareNodeAndRegistrationDataFromFlatbuffer() {
         TfLiteBridgeBuiltinParseFunction parser =
             op_resolver_.GetOpDataParser(op_type);
         if (parser == nullptr) {
-          MicroPrintf("Did not find a parser for %s",
+          printf("Did not find a parser for %s",
                       EnumNameBuiltinOperator(op_type));
 
           return kTfLiteError;
@@ -181,9 +181,8 @@ TfLiteStatus MicroInterpreter::PrepareNodeAndRegistrationDataFromFlatbuffer() {
 
 TfLiteStatus MicroInterpreter::AllocateTensors() {
   SubgraphAllocations* allocations = allocator_.StartModelAllocation(model_);
-
   if (allocations == nullptr) {
-    MicroPrintf("Failed starting model allocation.\n");
+    printf("Failed starting model allocation.\n");
     initialization_status_ = kTfLiteError;
     return kTfLiteError;
   }
@@ -214,9 +213,15 @@ TfLiteStatus MicroInterpreter::AllocateTensors() {
   context_.RequestScratchBufferInArena = nullptr;
   context_.GetScratchBuffer = MicroContextGetScratchBuffer;
 
+  TfLiteStatus ts = allocator_.FinishModelAllocation(
+                                   model_, graph_.GetAllocations(),
+                                   &scratch_buffer_handles_);
+  TF_LITE_ENSURE_OK(&context, ts);
+#if 0
   TF_LITE_ENSURE_OK(&context_, allocator_.FinishModelAllocation(
                                    model_, graph_.GetAllocations(),
                                    &scratch_buffer_handles_));
+#endif
 
   micro_context_.SetScratchBufferHandles(scratch_buffer_handles_);
 
@@ -226,7 +231,7 @@ TfLiteStatus MicroInterpreter::AllocateTensors() {
       reinterpret_cast<TfLiteTensor**>(allocator_.AllocatePersistentBuffer(
           sizeof(TfLiteTensor*) * inputs_size()));
   if (input_tensors_ == nullptr) {
-    MicroPrintf(
+    printf(
         "Failed to allocate memory for context->input_tensors_, "
         "%d bytes required",
         sizeof(TfLiteTensor*) * inputs_size());
@@ -237,7 +242,7 @@ TfLiteStatus MicroInterpreter::AllocateTensors() {
     input_tensors_[i] = allocator_.AllocatePersistentTfLiteTensor(
         model_, graph_.GetAllocations(), inputs().Get(i), 0);
     if (input_tensors_[i] == nullptr) {
-      MicroPrintf("Failed to initialize input tensor %d", i);
+      printf("Failed to initialize input tensor %d", i);
       return kTfLiteError;
     }
   }
@@ -248,7 +253,7 @@ TfLiteStatus MicroInterpreter::AllocateTensors() {
       reinterpret_cast<TfLiteTensor**>(allocator_.AllocatePersistentBuffer(
           sizeof(TfLiteTensor*) * outputs_size()));
   if (output_tensors_ == nullptr) {
-    MicroPrintf(
+    printf(
         "Failed to allocate memory for context->output_tensors_, "
         "%d bytes required",
         sizeof(TfLiteTensor*) * outputs_size());
@@ -259,7 +264,7 @@ TfLiteStatus MicroInterpreter::AllocateTensors() {
     output_tensors_[i] = allocator_.AllocatePersistentTfLiteTensor(
         model_, graph_.GetAllocations(), outputs().Get(i), 0);
     if (output_tensors_[i] == nullptr) {
-      MicroPrintf("Failed to initialize output tensor %d", i);
+      printf("Failed to initialize output tensor %d", i);
       return kTfLiteError;
     }
   }
@@ -272,7 +277,7 @@ TfLiteStatus MicroInterpreter::AllocateTensors() {
 
 TfLiteStatus MicroInterpreter::Invoke() {
   if (initialization_status_ != kTfLiteOk) {
-    MicroPrintf("Invoke() called after initialization failed\n");
+    printf("Invoke() called after initialization failed\n");
     return kTfLiteError;
   }
 
@@ -287,7 +292,7 @@ TfLiteStatus MicroInterpreter::Invoke() {
 TfLiteTensor* MicroInterpreter::input(size_t index) {
   const size_t length = inputs_size();
   if (index >= length) {
-    MicroPrintf("Input index %d out of range (length is %d)", index, length);
+    printf("Input index %d out of range (length is %d)", index, length);
     return nullptr;
   }
   return input_tensors_[index];
@@ -296,7 +301,7 @@ TfLiteTensor* MicroInterpreter::input(size_t index) {
 TfLiteTensor* MicroInterpreter::output(size_t index) {
   const size_t length = outputs_size();
   if (index >= length) {
-    MicroPrintf("Output index %d out of range (length is %d)", index, length);
+    printf("Output index %d out of range (length is %d)", index, length);
     return nullptr;
   }
   return output_tensors_[index];
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/micro_mutable_op_resolver.h b/third_party/tflite-micro/tensorflow/lite/micro/micro_mutable_op_resolver.h
index a4d50c83..e0f9d697 100644
--- a/third_party/tflite-micro/tensorflow/lite/micro/micro_mutable_op_resolver.h
+++ b/third_party/tflite-micro/tensorflow/lite/micro/micro_mutable_op_resolver.h
@@ -98,7 +98,6 @@ class MicroMutableOpResolver : public MicroOpResolver {
       MicroPrintf("is not supported (Op: %s).", name);
       return kTfLiteError;
     }
-
     TfLiteRegistration* new_registration = &registrations_[registrations_len_];
     registrations_len_ += 1;
 
@@ -540,9 +539,11 @@ class MicroMutableOpResolver : public MicroOpResolver {
     return AddBuiltin(BuiltinOperator_SUB, tflite::Register_SUB(), ParseSub);
   }
 
+/*
   TfLiteStatus AddSum() {
     return AddBuiltin(BuiltinOperator_SUM, Register_SUM(), ParseReducer);
   }
+*/
 
   TfLiteStatus AddSvdf(
       const TfLiteRegistration& registration = Register_SVDF()) {
@@ -589,6 +590,24 @@ class MicroMutableOpResolver : public MicroOpResolver {
                       ParseZerosLike);
   }
 
+  /********************** new_add *********************/
+  
+  TfLiteStatus AddAudio_Spectrogram() {
+    return AddCustom("AudioSpectrogram",
+                     tflite::ops::micro::Register_AUDIO_SPECTROGRAM());
+  }
+
+  TfLiteStatus AddMFCC() {
+    return AddCustom("Mfcc",
+                     tflite::ops::micro::Register_MFCC());
+  }
+  
+  TfLiteStatus AddSum() {
+    return AddBuiltin(BuiltinOperator_SUM, 
+                        Register_SUM(), ParseReducer);
+  }
+  /********************** new_add *********************/
+
   unsigned int GetRegistrationLength() { return registrations_len_; }
 
  private:
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/micro_mutable_op_resolver.h.orig b/third_party/tflite-micro/tensorflow/lite/micro/micro_mutable_op_resolver.h.orig
new file mode 100644
index 00000000..d5a78cfc
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/micro_mutable_op_resolver.h.orig
@@ -0,0 +1,658 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+#ifndef TENSORFLOW_LITE_MICRO_MICRO_MUTABLE_OP_RESOLVER_H_
+#define TENSORFLOW_LITE_MICRO_MICRO_MUTABLE_OP_RESOLVER_H_
+
+#include <cstdio>
+#include <cstring>
+
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/core/api/flatbuffer_conversions.h"
+#include "tensorflow/lite/kernels/internal/compatibility.h"
+#include "tensorflow/lite/kernels/op_macros.h"
+#include "tensorflow/lite/micro/compatibility.h"
+#include "tensorflow/lite/micro/kernels/add.h"
+#include "tensorflow/lite/micro/kernels/conv.h"
+#include "tensorflow/lite/micro/kernels/depthwise_conv.h"
+#include "tensorflow/lite/micro/kernels/ethosu.h"
+#include "tensorflow/lite/micro/kernels/fully_connected.h"
+#include "tensorflow/lite/micro/kernels/micro_ops.h"
+#include "tensorflow/lite/micro/kernels/pooling.h"
+#include "tensorflow/lite/micro/kernels/reduce.h"
+#include "tensorflow/lite/micro/kernels/softmax.h"
+#include "tensorflow/lite/micro/micro_log.h"
+#include "tensorflow/lite/micro/micro_op_resolver.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+
+namespace tflite {
+TfLiteRegistration* Register_DETECTION_POSTPROCESS();
+
+template <unsigned int tOpCount>
+class MicroMutableOpResolver : public MicroOpResolver {
+ public:
+  TF_LITE_REMOVE_VIRTUAL_DELETE
+
+  explicit MicroMutableOpResolver() {}
+
+  const TfLiteRegistration* FindOp(tflite::BuiltinOperator op) const override {
+    if (op == BuiltinOperator_CUSTOM) return nullptr;
+
+    for (unsigned int i = 0; i < registrations_len_; ++i) {
+      const TfLiteRegistration& registration = registrations_[i];
+      if (registration.builtin_code == op) {
+        return &registration;
+      }
+    }
+    return nullptr;
+  }
+
+  const TfLiteRegistration* FindOp(const char* op) const override {
+    for (unsigned int i = 0; i < registrations_len_; ++i) {
+      const TfLiteRegistration& registration = registrations_[i];
+      if ((registration.builtin_code == BuiltinOperator_CUSTOM) &&
+          (strcmp(registration.custom_name, op) == 0)) {
+        return &registration;
+      }
+    }
+    return nullptr;
+  }
+
+  TfLiteBridgeBuiltinParseFunction GetOpDataParser(
+      BuiltinOperator op) const override {
+    TFLITE_DCHECK(num_buitin_ops_ <= tOpCount);
+    for (unsigned int i = 0; i < num_buitin_ops_; ++i) {
+      if (builtin_codes_[i] == op) return builtin_parsers_[i];
+    }
+    return nullptr;
+  }
+
+  // Registers a Custom Operator with the MicroOpResolver.
+  //
+  // Only the first call for a given name will be successful. i.e. if this
+  // function is called again for a previously added Custom Operator, the
+  // MicroOpResolver will be unchanged and this function will return
+  // kTfLiteError.
+  TfLiteStatus AddCustom(const char* name, TfLiteRegistration* registration) {
+    if (registrations_len_ >= tOpCount) {
+      MicroPrintf(
+          "Couldn't register custom op '%s', resolver size is too"
+          "small (%d)",
+          name, tOpCount);
+      return kTfLiteError;
+    }
+
+    if (FindOp(name) != nullptr) {
+      MicroPrintf("Calling AddCustom for the same op more than once ");
+      MicroPrintf("is not supported (Op: %s).", name);
+      return kTfLiteError;
+    }
+
+    TfLiteRegistration* new_registration = &registrations_[registrations_len_];
+    registrations_len_ += 1;
+
+    *new_registration = *registration;
+    new_registration->builtin_code = BuiltinOperator_CUSTOM;
+    new_registration->custom_name = name;
+    return kTfLiteOk;
+  }
+
+  // The Add* functions below add the various Builtin operators to the
+  // MicroMutableOpResolver object.
+
+  TfLiteStatus AddAbs() {
+    return AddBuiltin(BuiltinOperator_ABS, tflite::ops::micro::Register_ABS(),
+                      ParseAbs);
+  }
+
+  TfLiteStatus AddAdd(const TfLiteRegistration& registration = Register_ADD()) {
+    return AddBuiltin(BuiltinOperator_ADD, registration, ParseAdd);
+  }
+
+  TfLiteStatus AddAddN() {
+    return AddBuiltin(BuiltinOperator_ADD_N, tflite::Register_ADD_N(),
+                      ParseAddN);
+  }
+
+  TfLiteStatus AddArgMax() {
+    return AddBuiltin(BuiltinOperator_ARG_MAX, Register_ARG_MAX(), ParseArgMax);
+  }
+
+  TfLiteStatus AddArgMin() {
+    return AddBuiltin(BuiltinOperator_ARG_MIN, Register_ARG_MIN(), ParseArgMin);
+  }
+
+  TfLiteStatus AddAssignVariable() {
+    return AddBuiltin(BuiltinOperator_ASSIGN_VARIABLE,
+                      tflite::Register_ASSIGN_VARIABLE(), ParseAssignVariable);
+  }
+
+  TfLiteStatus AddAveragePool2D(
+      const TfLiteRegistration& registration = Register_AVERAGE_POOL_2D()) {
+    return AddBuiltin(BuiltinOperator_AVERAGE_POOL_2D, registration, ParsePool);
+  }
+
+  TfLiteStatus AddBatchToSpaceNd() {
+    return AddBuiltin(BuiltinOperator_BATCH_TO_SPACE_ND,
+                      Register_BATCH_TO_SPACE_ND(), ParseBatchToSpaceNd);
+  }
+
+  TfLiteStatus AddBroadcastArgs() {
+    return AddBuiltin(BuiltinOperator_BROADCAST_ARGS, Register_BROADCAST_ARGS(),
+                      ParseBroadcastArgs);
+  }
+
+  TfLiteStatus AddBroadcastTo() {
+    return AddBuiltin(BuiltinOperator_BROADCAST_TO, Register_BROADCAST_TO(),
+                      ParseBroadcastTo);
+  }
+
+  TfLiteStatus AddCallOnce() {
+    return AddBuiltin(BuiltinOperator_CALL_ONCE, Register_CALL_ONCE(),
+                      ParseCallOnce);
+  }
+
+  TfLiteStatus AddCast() {
+    return AddBuiltin(BuiltinOperator_CAST, Register_CAST(), ParseCast);
+  }
+
+  TfLiteStatus AddCeil() {
+    return AddBuiltin(BuiltinOperator_CEIL, tflite::ops::micro::Register_CEIL(),
+                      ParseCeil);
+  }
+
+  TfLiteStatus AddCircularBuffer() {
+    return AddCustom("CIRCULAR_BUFFER", tflite::Register_CIRCULAR_BUFFER());
+  }
+
+  TfLiteStatus AddConcatenation() {
+    return AddBuiltin(BuiltinOperator_CONCATENATION,
+                      tflite::ops::micro::Register_CONCATENATION(),
+                      ParseConcatenation);
+  }
+
+  TfLiteStatus AddConv2D(
+      const TfLiteRegistration& registration = Register_CONV_2D()) {
+    return AddBuiltin(BuiltinOperator_CONV_2D, registration, ParseConv2D);
+  }
+
+  TfLiteStatus AddCos() {
+    return AddBuiltin(BuiltinOperator_COS, tflite::ops::micro::Register_COS(),
+                      ParseCos);
+  }
+
+  TfLiteStatus AddCumSum() {
+    return AddBuiltin(BuiltinOperator_CUMSUM, tflite::Register_CUMSUM(),
+                      ParseCumsum);
+  }
+
+  TfLiteStatus AddDepthToSpace() {
+    return AddBuiltin(BuiltinOperator_DEPTH_TO_SPACE,
+                      tflite::Register_DEPTH_TO_SPACE(), ParseDepthToSpace);
+  }
+
+  TfLiteStatus AddDepthwiseConv2D(
+      const TfLiteRegistration& registration = Register_DEPTHWISE_CONV_2D()) {
+    return AddBuiltin(BuiltinOperator_DEPTHWISE_CONV_2D, registration,
+                      ParseDepthwiseConv2D);
+  }
+
+  TfLiteStatus AddDequantize() {
+    return AddBuiltin(BuiltinOperator_DEQUANTIZE, tflite::Register_DEQUANTIZE(),
+                      ParseDequantize);
+  }
+
+  TfLiteStatus AddDetectionPostprocess() {
+    return AddCustom("TFLite_Detection_PostProcess",
+                     tflite::Register_DETECTION_POSTPROCESS());
+  }
+
+  TfLiteStatus AddDiv() {
+    return AddBuiltin(BuiltinOperator_DIV, tflite::Register_DIV(), ParseDiv);
+  }
+
+  TfLiteStatus AddElu() {
+    return AddBuiltin(BuiltinOperator_ELU, tflite::Register_ELU(), ParseElu);
+  }
+
+  TfLiteStatus AddEqual() {
+    return AddBuiltin(BuiltinOperator_EQUAL,
+                      tflite::ops::micro::Register_EQUAL(), ParseEqual);
+  }
+
+  TfLiteStatus AddEthosU() {
+    TfLiteRegistration* registration = tflite::Register_ETHOSU();
+    if (registration) {
+      return AddCustom(tflite::GetString_ETHOSU(), registration);
+    }
+    return kTfLiteOk;
+  }
+
+  TfLiteStatus AddExp() {
+    return AddBuiltin(BuiltinOperator_EXP, Register_EXP(), ParseExp);
+  }
+
+  TfLiteStatus AddExpandDims() {
+    return AddBuiltin(BuiltinOperator_EXPAND_DIMS, Register_EXPAND_DIMS(),
+                      ParseExpandDims);
+  }
+
+  TfLiteStatus AddFill() {
+    return AddBuiltin(BuiltinOperator_FILL, tflite::Register_FILL(), ParseFill);
+  }
+
+  TfLiteStatus AddFloor() {
+    return AddBuiltin(BuiltinOperator_FLOOR,
+                      tflite::ops::micro::Register_FLOOR(), ParseFloor);
+  }
+
+  TfLiteStatus AddFloorDiv() {
+    return AddBuiltin(BuiltinOperator_FLOOR_DIV, tflite::Register_FLOOR_DIV(),
+                      ParseFloorDiv);
+  }
+
+  TfLiteStatus AddFloorMod() {
+    return AddBuiltin(BuiltinOperator_FLOOR_MOD, tflite::Register_FLOOR_MOD(),
+                      ParseFloorMod);
+  }
+
+  TfLiteStatus AddFullyConnected(
+      const TfLiteRegistration& registration = Register_FULLY_CONNECTED()) {
+    return AddBuiltin(BuiltinOperator_FULLY_CONNECTED, registration,
+                      ParseFullyConnected);
+  }
+
+  TfLiteStatus AddGather() {
+    return AddBuiltin(BuiltinOperator_GATHER, tflite::Register_GATHER(),
+                      ParseGather);
+  }
+
+  TfLiteStatus AddGatherNd() {
+    return AddBuiltin(BuiltinOperator_GATHER_ND, tflite::Register_GATHER_ND(),
+                      ParseGatherNd);
+  }
+
+  TfLiteStatus AddGreater() {
+    return AddBuiltin(BuiltinOperator_GREATER,
+                      tflite::ops::micro::Register_GREATER(), ParseGreater);
+  }
+
+  TfLiteStatus AddGreaterEqual() {
+    return AddBuiltin(BuiltinOperator_GREATER_EQUAL,
+                      tflite::ops::micro::Register_GREATER_EQUAL(),
+                      ParseGreaterEqual);
+  }
+
+  TfLiteStatus AddHardSwish() {
+    return AddBuiltin(BuiltinOperator_HARD_SWISH, tflite::Register_HARD_SWISH(),
+                      ParseHardSwish);
+  }
+
+  TfLiteStatus AddIf() {
+    return AddBuiltin(BuiltinOperator_IF, tflite::Register_IF(), ParseIf);
+  }
+
+  TfLiteStatus AddL2Normalization() {
+    return AddBuiltin(BuiltinOperator_L2_NORMALIZATION,
+                      tflite::ops::micro::Register_L2_NORMALIZATION(),
+                      ParseL2Normalization);
+  }
+
+  TfLiteStatus AddL2Pool2D() {
+    return AddBuiltin(BuiltinOperator_L2_POOL_2D, tflite::Register_L2_POOL_2D(),
+                      ParsePool);
+  }
+
+  TfLiteStatus AddLeakyRelu() {
+    return AddBuiltin(BuiltinOperator_LEAKY_RELU, tflite::Register_LEAKY_RELU(),
+                      ParseLeakyRelu);
+  }
+
+  TfLiteStatus AddLess() {
+    return AddBuiltin(BuiltinOperator_LESS, tflite::ops::micro::Register_LESS(),
+                      ParseLess);
+  }
+
+  TfLiteStatus AddLessEqual() {
+    return AddBuiltin(BuiltinOperator_LESS_EQUAL,
+                      tflite::ops::micro::Register_LESS_EQUAL(),
+                      ParseLessEqual);
+  }
+
+  TfLiteStatus AddLog() {
+    return AddBuiltin(BuiltinOperator_LOG, tflite::ops::micro::Register_LOG(),
+                      ParseLog);
+  }
+
+  TfLiteStatus AddLogicalAnd() {
+    return AddBuiltin(BuiltinOperator_LOGICAL_AND,
+                      tflite::Register_LOGICAL_AND(), ParseLogicalAnd);
+  }
+
+  TfLiteStatus AddLogicalNot() {
+    return AddBuiltin(BuiltinOperator_LOGICAL_NOT,
+                      tflite::ops::micro::Register_LOGICAL_NOT(),
+                      ParseLogicalNot);
+  }
+
+  TfLiteStatus AddLogicalOr() {
+    return AddBuiltin(BuiltinOperator_LOGICAL_OR, tflite::Register_LOGICAL_OR(),
+                      ParseLogicalOr);
+  }
+
+  TfLiteStatus AddLogistic() {
+    return AddBuiltin(BuiltinOperator_LOGISTIC, tflite::Register_LOGISTIC(),
+                      ParseLogistic);
+  }
+
+  TfLiteStatus AddLogSoftmax() {
+    return AddBuiltin(BuiltinOperator_LOG_SOFTMAX,
+                      tflite::Register_LOG_SOFTMAX(), ParseLogSoftmax);
+  }
+
+  TfLiteStatus AddMaximum() {
+    return AddBuiltin(BuiltinOperator_MAXIMUM,
+                      tflite::ops::micro::Register_MAXIMUM(), ParseMaximum);
+  }
+
+  TfLiteStatus AddMaxPool2D(
+      const TfLiteRegistration& registration = Register_MAX_POOL_2D()) {
+    return AddBuiltin(BuiltinOperator_MAX_POOL_2D, registration, ParsePool);
+  }
+
+  TfLiteStatus AddMirrorPad() {
+    return AddBuiltin(BuiltinOperator_MIRROR_PAD, tflite::Register_MIRROR_PAD(),
+                      ParseMirrorPad);
+  }
+
+  TfLiteStatus AddMean() {
+    return AddBuiltin(BuiltinOperator_MEAN, Register_MEAN(), ParseReducer);
+  }
+
+  TfLiteStatus AddMinimum() {
+    return AddBuiltin(BuiltinOperator_MINIMUM,
+                      tflite::ops::micro::Register_MINIMUM(), ParseMinimum);
+  }
+
+  TfLiteStatus AddMul(const TfLiteRegistration& registration = Register_MUL()) {
+    return AddBuiltin(BuiltinOperator_MUL, registration, ParseMul);
+  }
+
+  TfLiteStatus AddNeg() {
+    return AddBuiltin(BuiltinOperator_NEG, Register_NEG(), ParseNeg);
+  }
+
+  TfLiteStatus AddNotEqual() {
+    return AddBuiltin(BuiltinOperator_NOT_EQUAL,
+                      tflite::ops::micro::Register_NOT_EQUAL(), ParseNotEqual);
+  }
+
+  TfLiteStatus AddPack() {
+    return AddBuiltin(BuiltinOperator_PACK, tflite::ops::micro::Register_PACK(),
+                      ParsePack);
+  }
+  /********************** new_add *********************/
+  
+  TfLiteStatus AddAudio_Spectrogram() {
+    return AddCustom("AudioSpectrogram",
+                     tflite::ops::micro::Register_AUDIO_SPECTROGRAM());
+  }
+
+  TfLiteStatus AddMFCC() {
+    return AddCustom("Mfcc",
+                     tflite::ops::micro::Register_MFCC());
+  }
+  
+  TfLiteStatus AddSum() {
+    return AddBuiltin(BuiltinOperator_SUM, 
+                        tflite::ops::micro::Register_SUM(), ParseReducer);
+  }
+  /********************** new_add *********************/
+
+  TfLiteStatus AddPad(const TfLiteRegistration& registration = Register_PAD()) {
+    return AddBuiltin(BuiltinOperator_PAD, registration, ParsePad);
+  }
+
+  TfLiteStatus AddPadV2() {
+    return AddBuiltin(BuiltinOperator_PADV2, Register_PADV2(), ParsePadV2);
+  }
+
+  TfLiteStatus AddPrelu() {
+    return AddBuiltin(BuiltinOperator_PRELU, tflite::Register_PRELU(),
+                      ParsePrelu);
+  }
+
+  TfLiteStatus AddQuantize() {
+    return AddBuiltin(BuiltinOperator_QUANTIZE, Register_QUANTIZE(),
+                      ParseQuantize);
+  }
+
+  TfLiteStatus AddReadVariable() {
+    return AddBuiltin(BuiltinOperator_READ_VARIABLE,
+                      tflite::Register_READ_VARIABLE(), ParseReadVariable);
+  }
+
+  TfLiteStatus AddReduceMax() {
+    return AddBuiltin(BuiltinOperator_REDUCE_MAX, Register_REDUCE_MAX(),
+                      ParseReducer);
+  }
+
+  TfLiteStatus AddRelu() {
+    return AddBuiltin(BuiltinOperator_RELU, tflite::Register_RELU(), ParseRelu);
+  }
+
+  TfLiteStatus AddRelu6() {
+    return AddBuiltin(BuiltinOperator_RELU6, tflite::Register_RELU6(),
+                      ParseRelu6);
+  }
+
+  TfLiteStatus AddReshape() {
+    return AddBuiltin(BuiltinOperator_RESHAPE,
+                      tflite::ops::micro::Register_RESHAPE(), ParseReshape);
+  }
+
+  TfLiteStatus AddResizeBilinear() {
+    return AddBuiltin(BuiltinOperator_RESIZE_BILINEAR,
+                      Register_RESIZE_BILINEAR(), ParseResizeBilinear);
+  }
+
+  TfLiteStatus AddResizeNearestNeighbor() {
+    return AddBuiltin(BuiltinOperator_RESIZE_NEAREST_NEIGHBOR,
+                      tflite::ops::micro::Register_RESIZE_NEAREST_NEIGHBOR(),
+                      ParseResizeNearestNeighbor);
+  }
+
+  TfLiteStatus AddRound() {
+    return AddBuiltin(BuiltinOperator_ROUND,
+                      tflite::ops::micro::Register_ROUND(), ParseRound);
+  }
+
+  TfLiteStatus AddRsqrt() {
+    return AddBuiltin(BuiltinOperator_RSQRT,
+                      tflite::ops::micro::Register_RSQRT(), ParseRsqrt);
+  }
+
+  TfLiteStatus AddSelectV2() {
+    return AddBuiltin(BuiltinOperator_SELECT_V2, Register_SELECT_V2(),
+                      ParseSelectV2);
+  }
+
+  TfLiteStatus AddShape() {
+    return AddBuiltin(BuiltinOperator_SHAPE, Register_SHAPE(), ParseShape);
+  }
+
+  TfLiteStatus AddSin() {
+    return AddBuiltin(BuiltinOperator_SIN, tflite::ops::micro::Register_SIN(),
+                      ParseSin);
+  }
+
+  TfLiteStatus AddSlice() {
+    return AddBuiltin(BuiltinOperator_SLICE, Register_SLICE(), ParseSlice);
+  }
+
+  TfLiteStatus AddSoftmax(
+      const TfLiteRegistration& registration = Register_SOFTMAX()) {
+    return AddBuiltin(BuiltinOperator_SOFTMAX, registration, ParseSoftmax);
+  }
+
+  TfLiteStatus AddSpaceToBatchNd() {
+    return AddBuiltin(BuiltinOperator_SPACE_TO_BATCH_ND,
+                      Register_SPACE_TO_BATCH_ND(), ParseSpaceToBatchNd);
+  }
+
+  TfLiteStatus AddSpaceToDepth() {
+    return AddBuiltin(BuiltinOperator_SPACE_TO_DEPTH, Register_SPACE_TO_DEPTH(),
+                      ParseSpaceToDepth);
+  }
+
+  TfLiteStatus AddSplit() {
+    return AddBuiltin(BuiltinOperator_SPLIT,
+                      tflite::ops::micro::Register_SPLIT(), ParseSplit);
+  }
+
+  TfLiteStatus AddSplitV() {
+    return AddBuiltin(BuiltinOperator_SPLIT_V,
+                      tflite::ops::micro::Register_SPLIT_V(), ParseSplitV);
+  }
+
+  TfLiteStatus AddSqueeze() {
+    return AddBuiltin(BuiltinOperator_SQUEEZE, Register_SQUEEZE(),
+                      ParseSqueeze);
+  }
+
+  TfLiteStatus AddSqrt() {
+    return AddBuiltin(BuiltinOperator_SQRT, tflite::ops::micro::Register_SQRT(),
+                      ParseSqrt);
+  }
+
+  TfLiteStatus AddSquare() {
+    return AddBuiltin(BuiltinOperator_SQUARE,
+                      tflite::ops::micro::Register_SQUARE(), ParseSquare);
+  }
+
+  TfLiteStatus AddSquaredDifference() {
+    return AddBuiltin(BuiltinOperator_SQUARED_DIFFERENCE,
+                      tflite::Register_SQUARED_DIFFERENCE(),
+                      ParseSquaredDifference);
+  }
+
+  TfLiteStatus AddStridedSlice() {
+    return AddBuiltin(BuiltinOperator_STRIDED_SLICE,
+                      tflite::ops::micro::Register_STRIDED_SLICE(),
+                      ParseStridedSlice);
+  }
+
+  TfLiteStatus AddSub() {
+    return AddBuiltin(BuiltinOperator_SUB, tflite::Register_SUB(), ParseSub);
+  }
+
+  TfLiteStatus AddSum() {
+    return AddBuiltin(BuiltinOperator_SUM, Register_SUM(), ParseReducer);
+  }
+
+  TfLiteStatus AddSvdf(
+      const TfLiteRegistration& registration = Register_SVDF()) {
+    return AddBuiltin(BuiltinOperator_SVDF, registration, ParseSvdf);
+  }
+
+  TfLiteStatus AddTanh() {
+    return AddBuiltin(BuiltinOperator_TANH, tflite::ops::micro::Register_TANH(),
+                      ParseTanh);
+  }
+
+  TfLiteStatus AddTransposeConv() {
+    return AddBuiltin(BuiltinOperator_TRANSPOSE_CONV,
+                      tflite::Register_TRANSPOSE_CONV(), ParseTransposeConv);
+  }
+
+  TfLiteStatus AddTranspose() {
+    return AddBuiltin(BuiltinOperator_TRANSPOSE, Register_TRANSPOSE(),
+                      ParseTranspose);
+  }
+
+  TfLiteStatus AddUnpack() {
+    return AddBuiltin(BuiltinOperator_UNPACK,
+                      tflite::ops::micro::Register_UNPACK(), ParseUnpack);
+  }
+
+  TfLiteStatus AddUnidirectionalSequenceLSTM() {
+    return AddBuiltin(BuiltinOperator_UNIDIRECTIONAL_SEQUENCE_LSTM,
+                      Register_UNIDIRECTIONAL_SEQUENCE_LSTM(),
+                      ParseUnidirectionalSequenceLSTM);
+  }
+
+  TfLiteStatus AddVarHandle() {
+    return AddBuiltin(BuiltinOperator_VAR_HANDLE, Register_VAR_HANDLE(),
+                      ParseVarHandle);
+  }
+
+  TfLiteStatus AddWhile() {
+    return AddBuiltin(BuiltinOperator_WHILE, Register_WHILE(), ParseWhile);
+  }
+
+  TfLiteStatus AddZerosLike() {
+    return AddBuiltin(BuiltinOperator_ZEROS_LIKE, Register_ZEROS_LIKE(),
+                      ParseZerosLike);
+  }
+
+  unsigned int GetRegistrationLength() { return registrations_len_; }
+
+ private:
+  TfLiteStatus AddBuiltin(tflite::BuiltinOperator op,
+                          const TfLiteRegistration& registration,
+                          TfLiteBridgeBuiltinParseFunction parser) {
+    if (op == BuiltinOperator_CUSTOM) {
+      MicroPrintf("Invalid parameter BuiltinOperator_CUSTOM to the ");
+      MicroPrintf("AddBuiltin function.");
+      return kTfLiteError;
+    }
+
+    if (FindOp(op) != nullptr) {
+      MicroPrintf("Calling AddBuiltin with the same op more than ");
+      MicroPrintf("once is not supported (Op: #%d).", op);
+      return kTfLiteError;
+    }
+
+    if (registrations_len_ >= tOpCount) {
+      MicroPrintf("Couldn't register builtin op #%d, resolver size ", op);
+      MicroPrintf("is too small (%d).", tOpCount);
+      return kTfLiteError;
+    }
+
+    registrations_[registrations_len_] = registration;
+    // Strictly speaking, the builtin_code is not necessary for TFLM but filling
+    // it in regardless.
+    registrations_[registrations_len_].builtin_code = op;
+    registrations_len_++;
+
+    builtin_codes_[num_buitin_ops_] = op;
+    builtin_parsers_[num_buitin_ops_] = parser;
+    num_buitin_ops_++;
+
+    return kTfLiteOk;
+  }
+
+  TfLiteRegistration registrations_[tOpCount];
+  unsigned int registrations_len_ = 0;
+
+  // Arrays (and counter) to store the builtin codes and their corresponding
+  // parse functions as these are registered with the Op Resolver.
+  BuiltinOperator builtin_codes_[tOpCount];
+  TfLiteBridgeBuiltinParseFunction builtin_parsers_[tOpCount];
+  unsigned int num_buitin_ops_ = 0;
+};
+
+};  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_MICRO_MICRO_MUTABLE_OP_RESOLVER_H_
diff --git a/third_party/tflite-micro/tensorflow/lite/micro/testing/micro_test.h b/third_party/tflite-micro/tensorflow/lite/micro/testing/micro_test.h
new file mode 100644
index 00000000..0a7abe03
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/micro/testing/micro_test.h
@@ -0,0 +1,251 @@
+// An ultra-lightweight testing framework designed for use with microcontroller
+// applications. This is designed to be usable even
+// when no standard C or C++ libraries are available, and without any dynamic
+// memory allocation or reliance on global constructors.
+//
+// To build a test, you use syntax similar to gunit, but with some extra
+// decoration to create a hidden 'main' function containing each of the tests to
+// be run. Your code should look something like:
+// ----------------------------------------------------------------------------
+// #include "path/to/this/header"
+//
+// TF_LITE_MICRO_TESTS_BEGIN
+//
+// TF_LITE_MICRO_TEST(SomeTest) {
+//   TF_LITE_LOG_EXPECT_EQ(true, true);
+// }
+//
+// TF_LITE_MICRO_TESTS_END
+// ----------------------------------------------------------------------------
+// If you compile this for your platform, you'll get a normal binary that you
+// should be able to run. Executing it will output logging information like this
+// to stderr:
+// ----------------------------------------------------------------------------
+// Testing SomeTest
+// 1/1 tests passed
+// ~~~ALL TESTS PASSED~~~
+// ----------------------------------------------------------------------------
+// This is designed to be human-readable, so you can just run tests manually,
+// but the string "~~~ALL TESTS PASSED~~~" should only appear if all of the
+// tests do pass. This makes it possible to integrate with automated test
+// systems by scanning the output logs and looking for that magic value.
+//
+// This framework is intended to be a rudimentary alternative to no testing at
+// all on systems that struggle to run more conventional approaches, so use with
+// caution!
+
+#ifndef TENSORFLOW_LITE_MICRO_TESTING_MICRO_TEST_H_
+#define TENSORFLOW_LITE_MICRO_TESTING_MICRO_TEST_H_
+#include <limits>
+#include <type_traits>
+
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/micro/micro_log.h"
+#include "tensorflow/lite/micro/system_setup.h"
+
+namespace micro_test {
+extern int tests_passed;
+extern int tests_failed;
+extern bool is_test_complete;
+extern bool did_test_fail;
+}  // namespace micro_test
+
+namespace tflite {
+
+// This additional helper function is used (instead of directly calling
+// tflite::InitializeTarget from the TF_LITE_MICRO_TESTS_BEGIN macro) to avoid
+// adding a dependency from every bazel test target to micro:system_setp (which
+// is the target that implements InitializeTarget().
+//
+// The underlying issue here is that the use of the macros results in
+// dependencies that can be containted within the micro/testing:micro_test
+// target bleeding on to all the tests.
+inline void InitializeTest() { InitializeTarget(); }
+}  // namespace tflite
+
+#define TF_LITE_MICRO_TESTS_BEGIN   \
+  namespace micro_test {            \
+  int tests_passed;                 \
+  int tests_failed;                 \
+  bool is_test_complete;            \
+  bool did_test_fail;               \
+  }                                 \
+                                    \
+  int main(int argc, char** argv) { \
+    micro_test::tests_passed = 0;   \
+    micro_test::tests_failed = 0;   \
+    tflite::InitializeTest();
+
+#define TF_LITE_MICRO_TESTS_END                                       \
+  MicroPrintf("%d/%d tests passed", micro_test::tests_passed,         \
+              (micro_test::tests_failed + micro_test::tests_passed)); \
+  if (micro_test::tests_failed == 0) {                                \
+    MicroPrintf("~~~ALL TESTS PASSED~~~\n");                          \
+    return kTfLiteOk;                                                 \
+  } else {                                                            \
+    MicroPrintf("~~~SOME TESTS FAILED~~~\n");                         \
+    return kTfLiteError;                                              \
+  }                                                                   \
+  }
+
+// TODO(petewarden): I'm going to hell for what I'm doing to this poor for loop.
+#define TF_LITE_MICRO_TEST(name)                                           \
+  MicroPrintf("Testing " #name);                                           \
+  for (micro_test::is_test_complete = false,                               \
+      micro_test::did_test_fail = false;                                   \
+       !micro_test::is_test_complete; micro_test::is_test_complete = true, \
+      micro_test::tests_passed += (micro_test::did_test_fail) ? 0 : 1,     \
+      micro_test::tests_failed += (micro_test::did_test_fail) ? 1 : 0)
+
+#define TF_LITE_MICRO_EXPECT(x)                               \
+  do {                                                        \
+    if (!(x)) {                                               \
+      MicroPrintf(#x " failed at %s:%d", __FILE__, __LINE__); \
+      micro_test::did_test_fail = true;                       \
+    }                                                         \
+  } while (false)
+
+#define TF_LITE_MICRO_EXPECT_EQ(x, y)                                     \
+  do {                                                                    \
+    auto vx = x;                                                          \
+    auto vy = y;                                                          \
+    bool isFloatingX = (std::is_floating_point<decltype(vx)>::value);     \
+    bool isFloatingY = (std::is_floating_point<decltype(vy)>::value);     \
+    if (isFloatingX && isFloatingY) {                                     \
+      auto delta = ((vx) > (vy)) ? ((vx) - (vy)) : ((vy) - (vx));         \
+      if (delta > std::numeric_limits<decltype(delta)>::epsilon()) {      \
+        MicroPrintf(#x " == " #y " failed at %s:%d (%f vs %f)", __FILE__, \
+                    __LINE__, static_cast<double>(vx),                    \
+                    static_cast<double>(vy));                             \
+        micro_test::did_test_fail = true;                                 \
+      }                                                                   \
+    } else if ((vx) != (vy)) {                                            \
+      MicroPrintf(#x " == " #y " failed at %s:%d (%d vs %d)", __FILE__,   \
+                  __LINE__, static_cast<int>(vx), static_cast<int>(vy));  \
+      if (isFloatingX || isFloatingY) {                                   \
+        MicroPrintf("-----------WARNING-----------");                     \
+        MicroPrintf("Only one of the values is floating point value.");   \
+      }                                                                   \
+      micro_test::did_test_fail = true;                                   \
+    }                                                                     \
+  } while (false)
+
+#define TF_LITE_MICRO_EXPECT_NE(x, y)                                     \
+  do {                                                                    \
+    auto vx = x;                                                          \
+    auto vy = y;                                                          \
+    bool isFloatingX = (std::is_floating_point<decltype(vx)>::value);     \
+    bool isFloatingY = (std::is_floating_point<decltype(vy)>::value);     \
+    if (isFloatingX && isFloatingY) {                                     \
+      auto delta = ((vx) > (vy)) ? ((vx) - (vy)) : ((vy) - (vx));         \
+      if (delta <= std::numeric_limits<decltype(delta)>::epsilon()) {     \
+        MicroPrintf(#x " != " #y " failed at %s:%d", __FILE__, __LINE__); \
+        micro_test::did_test_fail = true;                                 \
+      }                                                                   \
+    } else if ((vx) == (vy)) {                                            \
+      MicroPrintf(#x " != " #y " failed at %s:%d", __FILE__, __LINE__);   \
+      if (isFloatingX || isFloatingY) {                                   \
+        MicroPrintf("-----------WARNING-----------");                     \
+        MicroPrintf("Only one of the values is floating point value.");   \
+      }                                                                   \
+      micro_test::did_test_fail = true;                                   \
+    }                                                                     \
+  } while (false)
+
+// TODO(wangtz): Making it more generic once needed.
+#define TF_LITE_MICRO_ARRAY_ELEMENT_EXPECT_NEAR(arr1, idx1, arr2, idx2,       \
+                                                epsilon)                      \
+  do {                                                                        \
+    auto delta = ((arr1)[(idx1)] > (arr2)[(idx2)])                            \
+                     ? ((arr1)[(idx1)] - (arr2)[(idx2)])                      \
+                     : ((arr2)[(idx2)] - (arr1)[(idx1)]);                     \
+    if (delta > epsilon) {                                                    \
+      MicroPrintf(#arr1 "[%d] (%f) near " #arr2 "[%d] (%f) failed at %s:%d",  \
+                  static_cast<int>(idx1), static_cast<float>((arr1)[(idx1)]), \
+                  static_cast<int>(idx2), static_cast<float>((arr2)[(idx2)]), \
+                  __FILE__, __LINE__);                                        \
+      micro_test::did_test_fail = true;                                       \
+    }                                                                         \
+  } while (false)
+
+// The check vx != vy is needed to properly handle the case where both
+// x and y evaluate to infinity. See #46960 for more details.
+#define TF_LITE_MICRO_EXPECT_NEAR(x, y, epsilon)                              \
+  do {                                                                        \
+    auto vx = (x);                                                            \
+    auto vy = (y);                                                            \
+    auto delta = ((vx) > (vy)) ? ((vx) - (vy)) : ((vy) - (vx));               \
+    if (vx != vy && delta > epsilon) {                                        \
+      MicroPrintf(#x " (%f) near " #y " (%f) failed at %s:%d",                \
+                  static_cast<double>(vx), static_cast<double>(vy), __FILE__, \
+                  __LINE__);                                                  \
+      micro_test::did_test_fail = true;                                       \
+    }                                                                         \
+  } while (false)
+
+#define TF_LITE_MICRO_EXPECT_GT(x, y)                                  \
+  do {                                                                 \
+    if ((x) <= (y)) {                                                  \
+      MicroPrintf(#x " > " #y " failed at %s:%d", __FILE__, __LINE__); \
+      micro_test::did_test_fail = true;                                \
+    }                                                                  \
+  } while (false)
+
+#define TF_LITE_MICRO_EXPECT_LT(x, y)                                  \
+  do {                                                                 \
+    if ((x) >= (y)) {                                                  \
+      MicroPrintf(#x " < " #y " failed at %s:%d", __FILE__, __LINE__); \
+      micro_test::did_test_fail = true;                                \
+    }                                                                  \
+  } while (false)
+
+#define TF_LITE_MICRO_EXPECT_GE(x, y)                                   \
+  do {                                                                  \
+    if ((x) < (y)) {                                                    \
+      MicroPrintf(#x " >= " #y " failed at %s:%d", __FILE__, __LINE__); \
+      micro_test::did_test_fail = true;                                 \
+    }                                                                   \
+  } while (false)
+
+#define TF_LITE_MICRO_EXPECT_LE(x, y)                                   \
+  do {                                                                  \
+    if ((x) > (y)) {                                                    \
+      MicroPrintf(#x " <= " #y " failed at %s:%d", __FILE__, __LINE__); \
+      micro_test::did_test_fail = true;                                 \
+    }                                                                   \
+  } while (false)
+
+#define TF_LITE_MICRO_EXPECT_TRUE(x)                                       \
+  do {                                                                     \
+    if (!(x)) {                                                            \
+      MicroPrintf(#x " was not true failed at %s:%d", __FILE__, __LINE__); \
+      micro_test::did_test_fail = true;                                    \
+    }                                                                      \
+  } while (false)
+
+#define TF_LITE_MICRO_EXPECT_FALSE(x)                                       \
+  do {                                                                      \
+    if (x) {                                                                \
+      MicroPrintf(#x " was not false failed at %s:%d", __FILE__, __LINE__); \
+      micro_test::did_test_fail = true;                                     \
+    }                                                                       \
+  } while (false)
+
+#define TF_LITE_MICRO_FAIL(msg)                       \
+  do {                                                \
+    MicroPrintf("FAIL: %s", msg, __FILE__, __LINE__); \
+    micro_test::did_test_fail = true;                 \
+  } while (false)
+
+#define TF_LITE_MICRO_EXPECT_STRING_EQ(string1, string2)                     \
+  do {                                                                       \
+    for (int i = 0; string1[i] != '\0' && string2[i] != '\0'; i++) {         \
+      if (string1[i] != string2[i]) {                                        \
+        MicroPrintf("FAIL: %s did not match %s", string1, string2, __FILE__, \
+                    __LINE__);                                               \
+        micro_test::did_test_fail = true;                                    \
+      }                                                                      \
+    }                                                                        \
+  } while (false)
+
+#endif  // TENSORFLOW_LITE_MICRO_TESTING_MICRO_TEST_H_
diff --git a/third_party/tflite-micro/tensorflow/lite/string_type.h b/third_party/tflite-micro/tensorflow/lite/string_type.h
new file mode 100644
index 00000000..f5a7f833
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/string_type.h
@@ -0,0 +1,27 @@
+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+// Abstract string. We don't want even absl at this level.
+#ifndef TENSORFLOW_LITE_STRING_TYPE_H_
+#define TENSORFLOW_LITE_STRING_TYPE_H_
+
+#include <string>
+
+namespace tflite {
+
+using std::string;
+
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_STRING_TYPE_H_
diff --git a/third_party/tflite-micro/tensorflow/lite/string_util.cc b/third_party/tflite-micro/tensorflow/lite/string_util.cc
new file mode 100644
index 00000000..5f8d6400
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/string_util.cc
@@ -0,0 +1,171 @@
+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/string_util.h"
+
+#include <stddef.h>
+
+#include <cstdlib>
+#include <cstring>
+#include <limits>
+#include <vector>
+
+#include "tensorflow/lite/core/c/c_api_types.h"
+#include "tensorflow/lite/core/c/common.h"
+
+namespace tflite {
+
+TfLiteStatus DynamicBuffer::AddString(const char* str, size_t len) {
+  // If `data_.size() + len` is greater than `SIZE_MAX` then the left hand side
+  // will overflow to something less than max_length_. After checking `len <=
+  // max_length_` we can use this subtraction to check for overflow.
+  if (len > max_length_ || data_.size() >= max_length_ - len)
+    return kTfLiteError;
+  data_.resize(data_.size() + len);
+  memcpy(data_.data() + offset_.back(), str, len);
+  offset_.push_back(offset_.back() + len);
+  return kTfLiteOk;
+}
+
+TfLiteStatus DynamicBuffer::AddString(const StringRef& string_) {
+  return AddString(string_.str, string_.len);
+}
+
+void DynamicBuffer::AddJoinedString(const std::vector<StringRef>& strings,
+                                    char separator) {
+  StringRef ref;
+  ref.str = &separator;
+  ref.len = 1;
+  AddJoinedString(strings, ref);
+}
+
+void DynamicBuffer::AddJoinedString(const std::vector<StringRef>& strings,
+                                    StringRef separator) {
+  // Resize the data buffer.
+  int total_len = (strings.size() - 1) * separator.len;
+  for (StringRef ref : strings) {
+    total_len += ref.len;
+  }
+  data_.resize(data_.size() + total_len);
+
+  char* dst = data_.data() + offset_.back();
+  for (size_t i = 0; i < strings.size(); ++i) {
+    // Fill separator if not first string.
+    if (i != 0) {
+      memcpy(dst, separator.str, separator.len);
+      dst += separator.len;
+    }
+
+    // Fill content of the string.
+    memcpy(dst, strings[i].str, strings[i].len);
+    dst += strings[i].len;
+  }
+  offset_.push_back(offset_.back() + total_len);
+}
+
+int DynamicBuffer::WriteToBuffer(char** buffer) {
+  // Allocate sufficient memory to tensor buffer.
+  int32_t num_strings = offset_.size() - 1;
+  // Total bytes include:
+  //   * size of content (data_.size)
+  //   * offset of each tensor (sizeof(int32_t) * num_strings)
+  //   * length of whole buffer (int32_t)
+  //   * num of strings (int32_t).
+  int32_t bytes = data_.size()                            // size of content
+                  + sizeof(int32_t) * (num_strings + 2);  // size of header
+
+  // Caller will take ownership of buffer.
+  *buffer = reinterpret_cast<char*>(malloc(bytes));
+
+  // Set num of string
+  //
+  // NOTE: The string buffer is accessed here as if it's native endian (instead
+  // of small endian, as documented in the header). This will protentially break
+  // when TFLite is ported to big endian platforms.
+  // TODO(b/165919229): This code will need changing if/when we port to a
+  // big-endian platform.
+  memcpy(*buffer, &num_strings, sizeof(int32_t));
+
+  // Set offset of strings.
+  int32_t start = sizeof(int32_t) * (num_strings + 2);
+  for (size_t i = 0; i < offset_.size(); i++) {
+    // TODO(b/165919229): This code will need changing if/when we port to a
+    // big-endian platform.
+    int32_t offset = start + offset_[i];
+    memcpy(*buffer + sizeof(int32_t) * (i + 1), &offset, sizeof(int32_t));
+  }
+
+  // Copy data of strings.
+  memcpy(*buffer + start, data_.data(), data_.size());
+  return bytes;
+}
+
+#ifndef TF_LITE_STATIC_MEMORY
+void DynamicBuffer::WriteToTensorAsVector(TfLiteTensor* tensor) {
+  auto dims = TfLiteIntArrayCreate(1);
+  dims->data[0] = offset_.size() - 1;  // Store number of strings.
+  WriteToTensor(tensor, dims);
+}
+
+void DynamicBuffer::WriteToTensor(TfLiteTensor* tensor,
+                                  TfLiteIntArray* new_shape) {
+  char* tensor_buffer;
+  int bytes = WriteToBuffer(&tensor_buffer);
+
+  if (new_shape == nullptr) {
+    new_shape = TfLiteIntArrayCopy(tensor->dims);
+  }
+
+  // Set tensor content pointer to tensor_buffer, and release original data.
+  TfLiteTensorReset(tensor->type, tensor->name, new_shape, tensor->params,
+                    tensor_buffer, bytes, kTfLiteDynamic, tensor->allocation,
+                    tensor->is_variable, tensor);
+}
+#endif  // TF_LITE_STATIC_MEMORY
+
+int GetStringCount(const void* raw_buffer) {
+  // The first integers in the raw buffer is the number of strings.
+  //
+  // NOTE: The string buffer is accessed here as if it's native endian (instead
+  // of small endian, as documented in the header). This will protentially break
+  // when TFLite is ported to big endian platforms.
+  // TODO(b/165919229): This code will need changing if/when we port to a
+  // big-endian platform.
+  return *static_cast<const int32_t*>(raw_buffer);
+}
+
+int GetStringCount(const TfLiteTensor* tensor) {
+  // The first integers in the raw buffer is the number of strings.
+  return GetStringCount(tensor->data.raw);
+}
+
+StringRef GetString(const void* raw_buffer, int string_index) {
+  // NOTE: The string buffer is accessed here as if it's native endian (instead
+  // of small endian, as documented in the header). This will protentially break
+  // when TFLite is ported to big endian platforms.
+  // TODO(b/165919229): This code will need changing if/when we port to a
+  // big-endian platform.
+  const int32_t* offset =
+      static_cast<const int32_t*>(raw_buffer) + (string_index + 1);
+  const size_t string_len = (*(offset + 1)) - (*offset);
+  return StringRef{static_cast<const char*>(raw_buffer) + (*offset),
+                   string_len};
+}
+
+StringRef GetString(const TfLiteTensor* tensor, int string_index) {
+  return GetString(tensor->data.raw, string_index);
+}
+
+}  // namespace tflite
diff --git a/third_party/tflite-micro/tensorflow/lite/string_util.h b/third_party/tflite-micro/tensorflow/lite/string_util.h
new file mode 100644
index 00000000..4935c6c8
--- /dev/null
+++ b/third_party/tflite-micro/tensorflow/lite/string_util.h
@@ -0,0 +1,124 @@
+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+// Util methods to read and write String tensors.
+// String tensors are considered to be char tensor with protocol.
+//   [0, 3] 4 bytes: N, num of strings in the tensor in little endian.
+//   [(i+1)*4, (i+1)*4+3] 4 bytes: offset of i-th string in little endian,
+//                                 for i from 0 to N-1.
+//   [(N+1)*4, (N+1)*4+3] 4 bytes: length of the whole char buffer.
+//   [offset(i), offset(i+1) - 1] : content of i-th string.
+// Example of a string tensor:
+// [
+//   2, 0, 0, 0,     # 2 strings.
+//   16, 0, 0, 0,    # 0-th string starts from index 16.
+//   18, 0, 0, 0,    # 1-st string starts from index 18.
+//   18, 0, 0, 0,    # total length of array.
+//   'A', 'B',       # 0-th string [16..17]: "AB"
+// ]                 # 1-th string, empty
+//
+// A typical usage:
+// In op.Eval(context, node):
+//   DynamicBuffer buf;
+//   # Add string "AB" to tensor, string is stored in dynamic buffer.
+//   buf.AddString("AB", 2);
+//   # Write content of DynamicBuffer to tensor in format of string tensor
+//   # described above.
+//   buf.WriteToTensor(tensor, nullptr)
+
+#ifndef TENSORFLOW_LITE_STRING_UTIL_H_
+#define TENSORFLOW_LITE_STRING_UTIL_H_
+
+#include <stddef.h>
+#include <stdint.h>
+
+#include <limits>
+#include <vector>
+
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/string_type.h"
+
+namespace tflite {
+
+// Convenient structure to store string pointer and length. Note that
+// methods on DynamicBuffer enforce that the whole buffer (and by extension
+// every contained string) is of max length (2ul << 30) - 1. See
+// string_util.cc for more info.
+typedef struct {
+  const char* str;
+  size_t len;
+} StringRef;
+
+constexpr uint64_t kDefaultMaxLength = std::numeric_limits<int>::max();
+
+// DynamicBuffer holds temporary buffer that will be used to create a dynamic
+// tensor. A typical usage is to initialize a DynamicBuffer object, fill in
+// content and call CreateStringTensor in op.Eval().
+class DynamicBuffer {
+ public:
+  explicit DynamicBuffer(size_t max_length = kDefaultMaxLength)
+      : offset_({0}), max_length_(max_length) {}
+
+  // Add string to dynamic buffer by resizing the buffer and copying the data.
+  TfLiteStatus AddString(const StringRef& string_);
+
+  // Add string to dynamic buffer by resizing the buffer and copying the data.
+  TfLiteStatus AddString(const char* str, size_t len);
+
+  // Join a list of string with separator, and add as a single string to the
+  // buffer.
+  void AddJoinedString(const std::vector<StringRef>& strings, char separator);
+  void AddJoinedString(const std::vector<StringRef>& strings,
+                       StringRef separator);
+
+  // Fill content into a buffer and returns the number of bytes stored.
+  // The function allocates space for the buffer but does NOT take ownership.
+  int WriteToBuffer(char** buffer);
+
+  // Fill content into a string tensor, with the given new_shape. The new shape
+  // must match the number of strings in this object. Caller relinquishes
+  // ownership of new_shape. If 'new_shape' is nullptr, keep the tensor's
+  // existing shape.
+  void WriteToTensor(TfLiteTensor* tensor, TfLiteIntArray* new_shape);
+
+  // Fill content into a string tensor. Set shape to {num_strings}.
+  void WriteToTensorAsVector(TfLiteTensor* tensor);
+
+ private:
+  // Data buffer to store contents of strings, not including headers.
+  std::vector<char> data_;
+  // Offset of the starting index of each string in data buffer.
+  std::vector<size_t> offset_;
+  // Max length in number of characters that we permit the total
+  // buffer containing the concatenation of all added strings to be.
+  // For historical reasons this is limited to 32bit length. At this files
+  // inception, sizes were represented using 32bit which forced an implicit cap
+  // on the size of the buffer. When this was refactored to use size_t (which
+  // could be 64bit) we enforce that the buffer remains at most 32bit length to
+  // avoid a change in behavior.
+  const size_t max_length_;
+};
+
+// Return num of strings in a String tensor.
+int GetStringCount(const void* raw_buffer);
+int GetStringCount(const TfLiteTensor* tensor);
+
+// Get String pointer and length of index-th string in tensor.
+// NOTE: This will not create a copy of string data.
+StringRef GetString(const void* raw_buffer, int string_index);
+StringRef GetString(const TfLiteTensor* tensor, int string_index);
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_STRING_UTIL_H_
diff --git a/third_party/tflite-micro/third_party/fft2d/BUILD b/third_party/tflite-micro/third_party/fft2d/BUILD
new file mode 100644
index 00000000..fe336bac
--- /dev/null
+++ b/third_party/tflite-micro/third_party/fft2d/BUILD
@@ -0,0 +1,39 @@
+# Headers for 2D Fast Fourier Transform package
+# from http://momonga.t.u-tokyo.ac.jp/~ooura/fft2d.html
+# This is a separate package because the original downloaded archive doesn't
+# contain any header files.
+
+package(
+    default_visibility = ["//visibility:public"],
+)
+
+# Unrestricted use; can only distribute original package.
+# See fft/readme.txt
+licenses(["notice"])
+
+exports_files(["LICENSE"])
+
+cc_library(
+    name = "fft2d_headers",
+    srcs = [
+        "fft.h",
+        "fft2d.h",
+    ],
+)
+
+objc_library(
+    name = "fft2d_headersd_ios",
+    srcs = [
+        "fft.h",
+        "fft2d.h",
+    ],
+)
+
+# Export the source code so that it could be compiled for Andoid native apps.
+filegroup(
+    name = "fft2d_headers_srcs",
+    srcs = [
+        "fft.h",
+        "fft2d.h",
+    ],
+)
diff --git a/third_party/tflite-micro/third_party/fft2d/LICENSE b/third_party/tflite-micro/third_party/fft2d/LICENSE
new file mode 100644
index 00000000..2bd85506
--- /dev/null
+++ b/third_party/tflite-micro/third_party/fft2d/LICENSE
@@ -0,0 +1,3 @@
+Copyright(C) 1997,2001 Takuya OOURA (email: ooura@kurims.kyoto-u.ac.jp).
+You may use, copy, modify this code for any purpose and 
+without fee. You may distribute this ORIGINAL package.
diff --git a/third_party/tflite-micro/third_party/fft2d/fft.h b/third_party/tflite-micro/third_party/fft2d/fft.h
new file mode 100644
index 00000000..36d838b7
--- /dev/null
+++ b/third_party/tflite-micro/third_party/fft2d/fft.h
@@ -0,0 +1,36 @@
+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+// Declarations for 1D FFT routines in third_party/fft2d/fft2d.
+
+#ifndef FFT2D_FFT_H__
+#define FFT2D_FFT_H__
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+extern void cdft(int, int, double *, int *, double *);
+extern void rdft(int, int, double *, int *, double *);
+extern void ddct(int, int, double *, int *, double *);
+extern void ddst(int, int, double *, int *, double *);
+extern void dfct(int, double *, double *, int *, double *);
+extern void dfst(int, double *, double *, int *, double *);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif  // FFT2D_FFT_H__
diff --git a/third_party/tflite-micro/third_party/fft2d/fft2d.h b/third_party/tflite-micro/third_party/fft2d/fft2d.h
new file mode 100644
index 00000000..e3311cc1
--- /dev/null
+++ b/third_party/tflite-micro/third_party/fft2d/fft2d.h
@@ -0,0 +1,38 @@
+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+// Declarations for 2D FFT routines in third_party/fft2d/fft2d.
+
+#ifndef FFT2D_FFT_H__
+#define FFT2D_FFT_H__
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+extern void cdft2d(int, int, int, double **, double *, int *, double *);
+extern void rdft2d(int, int, int, double **, double *, int *, double *);
+extern void ddct2d(int, int, int, double **, double *, int *, double *);
+extern void ddst2d(int, int, int, double **, double *, int *, double *);
+extern void rdft2dsort(int, int, int, double **);
+extern void ddct8x8s(int isgn, double **a);
+extern void ddct16x16s(int isgn, double **a);
+
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif  // FFT2D_FFT_H__
diff --git a/third_party/tflite-micro/third_party/fft2d/fft4g.c b/third_party/tflite-micro/third_party/fft2d/fft4g.c
new file mode 100644
index 00000000..3dde4920
--- /dev/null
+++ b/third_party/tflite-micro/third_party/fft2d/fft4g.c
@@ -0,0 +1,1346 @@
+/*
+Fast Fourier/Cosine/Sine Transform
+    dimension   :one
+    data length :power of 2
+    decimation  :frequency
+    radix       :4, 2
+    data        :inplace
+    table       :use
+functions
+    cdft: Complex Discrete Fourier Transform
+    rdft: Real Discrete Fourier Transform
+    ddct: Discrete Cosine Transform
+    ddst: Discrete Sine Transform
+    dfct: Cosine Transform of RDFT (Real Symmetric DFT)
+    dfst: Sine Transform of RDFT (Real Anti-symmetric DFT)
+function prototypes
+    void cdft(int, int, double *, int *, double *);
+    void rdft(int, int, double *, int *, double *);
+    void ddct(int, int, double *, int *, double *);
+    void ddst(int, int, double *, int *, double *);
+    void dfct(int, double *, double *, int *, double *);
+    void dfst(int, double *, double *, int *, double *);
+
+
+-------- Complex DFT (Discrete Fourier Transform) --------
+    [definition]
+        <case1>
+            X[k] = sum_j=0^n-1 x[j]*exp(2*pi*i*j*k/n), 0<=k<n
+        <case2>
+            X[k] = sum_j=0^n-1 x[j]*exp(-2*pi*i*j*k/n), 0<=k<n
+        (notes: sum_j=0^n-1 is a summation from j=0 to n-1)
+    [usage]
+        <case1>
+            ip[0] = 0; // first time only
+            cdft(2*n, 1, a, ip, w);
+        <case2>
+            ip[0] = 0; // first time only
+            cdft(2*n, -1, a, ip, w);
+    [parameters]
+        2*n            :data length (int)
+                        n >= 1, n = power of 2
+        a[0...2*n-1]   :input/output data (double *)
+                        input data
+                            a[2*j] = Re(x[j]), 
+                            a[2*j+1] = Im(x[j]), 0<=j<n
+                        output data
+                            a[2*k] = Re(X[k]), 
+                            a[2*k+1] = Im(X[k]), 0<=k<n
+        ip[0...*]      :work area for bit reversal (int *)
+                        length of ip >= 2+sqrt(n)
+                        strictly, 
+                        length of ip >= 
+                            2+(1<<(int)(log(n+0.5)/log(2))/2).
+                        ip[0],ip[1] are pointers of the cos/sin table.
+        w[0...n/2-1]   :cos/sin table (double *)
+                        w[],ip[] are initialized if ip[0] == 0.
+    [remark]
+        Inverse of 
+            cdft(2*n, -1, a, ip, w);
+        is 
+            cdft(2*n, 1, a, ip, w);
+            for (j = 0; j <= 2 * n - 1; j++) {
+                a[j] *= 1.0 / n;
+            }
+        .
+
+
+-------- Real DFT / Inverse of Real DFT --------
+    [definition]
+        <case1> RDFT
+            R[k] = sum_j=0^n-1 a[j]*cos(2*pi*j*k/n), 0<=k<=n/2
+            I[k] = sum_j=0^n-1 a[j]*sin(2*pi*j*k/n), 0<k<n/2
+        <case2> IRDFT (excluding scale)
+            a[k] = (R[0] + R[n/2]*cos(pi*k))/2 + 
+                   sum_j=1^n/2-1 R[j]*cos(2*pi*j*k/n) + 
+                   sum_j=1^n/2-1 I[j]*sin(2*pi*j*k/n), 0<=k<n
+    [usage]
+        <case1>
+            ip[0] = 0; // first time only
+            rdft(n, 1, a, ip, w);
+        <case2>
+            ip[0] = 0; // first time only
+            rdft(n, -1, a, ip, w);
+    [parameters]
+        n              :data length (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        <case1>
+                            output data
+                                a[2*k] = R[k], 0<=k<n/2
+                                a[2*k+1] = I[k], 0<k<n/2
+                                a[1] = R[n/2]
+                        <case2>
+                            input data
+                                a[2*j] = R[j], 0<=j<n/2
+                                a[2*j+1] = I[j], 0<j<n/2
+                                a[1] = R[n/2]
+        ip[0...*]      :work area for bit reversal (int *)
+                        length of ip >= 2+sqrt(n/2)
+                        strictly, 
+                        length of ip >= 
+                            2+(1<<(int)(log(n/2+0.5)/log(2))/2).
+                        ip[0],ip[1] are pointers of the cos/sin table.
+        w[0...n/2-1]   :cos/sin table (double *)
+                        w[],ip[] are initialized if ip[0] == 0.
+    [remark]
+        Inverse of 
+            rdft(n, 1, a, ip, w);
+        is 
+            rdft(n, -1, a, ip, w);
+            for (j = 0; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- DCT (Discrete Cosine Transform) / Inverse of DCT --------
+    [definition]
+        <case1> IDCT (excluding scale)
+            C[k] = sum_j=0^n-1 a[j]*cos(pi*j*(k+1/2)/n), 0<=k<n
+        <case2> DCT
+            C[k] = sum_j=0^n-1 a[j]*cos(pi*(j+1/2)*k/n), 0<=k<n
+    [usage]
+        <case1>
+            ip[0] = 0; // first time only
+            ddct(n, 1, a, ip, w);
+        <case2>
+            ip[0] = 0; // first time only
+            ddct(n, -1, a, ip, w);
+    [parameters]
+        n              :data length (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        output data
+                            a[k] = C[k], 0<=k<n
+        ip[0...*]      :work area for bit reversal (int *)
+                        length of ip >= 2+sqrt(n/2)
+                        strictly, 
+                        length of ip >= 
+                            2+(1<<(int)(log(n/2+0.5)/log(2))/2).
+                        ip[0],ip[1] are pointers of the cos/sin table.
+        w[0...n*5/4-1] :cos/sin table (double *)
+                        w[],ip[] are initialized if ip[0] == 0.
+    [remark]
+        Inverse of 
+            ddct(n, -1, a, ip, w);
+        is 
+            a[0] *= 0.5;
+            ddct(n, 1, a, ip, w);
+            for (j = 0; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- DST (Discrete Sine Transform) / Inverse of DST --------
+    [definition]
+        <case1> IDST (excluding scale)
+            S[k] = sum_j=1^n A[j]*sin(pi*j*(k+1/2)/n), 0<=k<n
+        <case2> DST
+            S[k] = sum_j=0^n-1 a[j]*sin(pi*(j+1/2)*k/n), 0<k<=n
+    [usage]
+        <case1>
+            ip[0] = 0; // first time only
+            ddst(n, 1, a, ip, w);
+        <case2>
+            ip[0] = 0; // first time only
+            ddst(n, -1, a, ip, w);
+    [parameters]
+        n              :data length (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        <case1>
+                            input data
+                                a[j] = A[j], 0<j<n
+                                a[0] = A[n]
+                            output data
+                                a[k] = S[k], 0<=k<n
+                        <case2>
+                            output data
+                                a[k] = S[k], 0<k<n
+                                a[0] = S[n]
+        ip[0...*]      :work area for bit reversal (int *)
+                        length of ip >= 2+sqrt(n/2)
+                        strictly, 
+                        length of ip >= 
+                            2+(1<<(int)(log(n/2+0.5)/log(2))/2).
+                        ip[0],ip[1] are pointers of the cos/sin table.
+        w[0...n*5/4-1] :cos/sin table (double *)
+                        w[],ip[] are initialized if ip[0] == 0.
+    [remark]
+        Inverse of 
+            ddst(n, -1, a, ip, w);
+        is 
+            a[0] *= 0.5;
+            ddst(n, 1, a, ip, w);
+            for (j = 0; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- Cosine Transform of RDFT (Real Symmetric DFT) --------
+    [definition]
+        C[k] = sum_j=0^n a[j]*cos(pi*j*k/n), 0<=k<=n
+    [usage]
+        ip[0] = 0; // first time only
+        dfct(n, a, t, ip, w);
+    [parameters]
+        n              :data length - 1 (int)
+                        n >= 2, n = power of 2
+        a[0...n]       :input/output data (double *)
+                        output data
+                            a[k] = C[k], 0<=k<=n
+        t[0...n/2]     :work area (double *)
+        ip[0...*]      :work area for bit reversal (int *)
+                        length of ip >= 2+sqrt(n/4)
+                        strictly, 
+                        length of ip >= 
+                            2+(1<<(int)(log(n/4+0.5)/log(2))/2).
+                        ip[0],ip[1] are pointers of the cos/sin table.
+        w[0...n*5/8-1] :cos/sin table (double *)
+                        w[],ip[] are initialized if ip[0] == 0.
+    [remark]
+        Inverse of 
+            a[0] *= 0.5;
+            a[n] *= 0.5;
+            dfct(n, a, t, ip, w);
+        is 
+            a[0] *= 0.5;
+            a[n] *= 0.5;
+            dfct(n, a, t, ip, w);
+            for (j = 0; j <= n; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- Sine Transform of RDFT (Real Anti-symmetric DFT) --------
+    [definition]
+        S[k] = sum_j=1^n-1 a[j]*sin(pi*j*k/n), 0<k<n
+    [usage]
+        ip[0] = 0; // first time only
+        dfst(n, a, t, ip, w);
+    [parameters]
+        n              :data length + 1 (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        output data
+                            a[k] = S[k], 0<k<n
+                        (a[0] is used for work area)
+        t[0...n/2-1]   :work area (double *)
+        ip[0...*]      :work area for bit reversal (int *)
+                        length of ip >= 2+sqrt(n/4)
+                        strictly, 
+                        length of ip >= 
+                            2+(1<<(int)(log(n/4+0.5)/log(2))/2).
+                        ip[0],ip[1] are pointers of the cos/sin table.
+        w[0...n*5/8-1] :cos/sin table (double *)
+                        w[],ip[] are initialized if ip[0] == 0.
+    [remark]
+        Inverse of 
+            dfst(n, a, t, ip, w);
+        is 
+            dfst(n, a, t, ip, w);
+            for (j = 1; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+Appendix :
+    The cos/sin table is recalculated when the larger table required.
+    w[] and ip[] are compatible with all routines.
+*/
+
+#include <stdio.h>
+void cdft(int n, int isgn, double *a, int *ip, double *w)
+{
+    void makewt(int nw, int *ip, double *w);
+    void bitrv2(int n, int *ip, double *a);
+    void bitrv2conj(int n, int *ip, double *a);
+    void cftfsub(int n, double *a, double *w);
+    void cftbsub(int n, double *a, double *w);
+    
+    if (n > (ip[0] << 2)) {
+        makewt(n >> 2, ip, w);
+    }
+    if (n > 4) {
+        if (isgn >= 0) {
+            bitrv2(n, ip + 2, a);
+            cftfsub(n, a, w);
+        } else {
+            bitrv2conj(n, ip + 2, a);
+            cftbsub(n, a, w);
+        }
+    } else if (n == 4) {
+        cftfsub(n, a, w);
+    }
+}
+
+
+void rdft(int n, int isgn, double *a, int *ip, double *w)
+{
+    //printf("in rdft\n");
+    void makewt(int nw, int *ip, double *w);
+    void makect(int nc, int *ip, double *c);
+    void bitrv2(int n, int *ip, double *a);
+    void cftfsub(int n, double *a, double *w);
+    void cftbsub(int n, double *a, double *w);
+    void rftfsub(int n, double *a, int nc, double *c);
+    void rftbsub(int n, double *a, int nc, double *c);
+    int nw, nc;
+    double xi;
+    
+    nw = ip[0];
+    if (n > (nw << 2)) {
+        nw = n >> 2;
+        makewt(nw, ip, w);
+    }
+    nc = ip[1];
+    if (n > (nc << 2)) {
+        nc = n >> 2;
+        makect(nc, ip, w + nw);
+    }
+    if (isgn >= 0) {
+        if (n > 4) {
+            bitrv2(n, ip + 2, a);
+            cftfsub(n, a, w);
+            rftfsub(n, a, nc, w + nw);
+        } else if (n == 4) {
+            cftfsub(n, a, w);
+        }
+        xi = a[0] - a[1];
+        a[0] += a[1];
+        a[1] = xi;
+    } else {
+        a[1] = 0.5 * (a[0] - a[1]);
+        a[0] -= a[1];
+        if (n > 4) {
+            rftbsub(n, a, nc, w + nw);
+            bitrv2(n, ip + 2, a);
+            cftbsub(n, a, w);
+        } else if (n == 4) {
+            cftfsub(n, a, w);
+        }
+    }
+}
+
+
+void ddct(int n, int isgn, double *a, int *ip, double *w)
+{
+    void makewt(int nw, int *ip, double *w);
+    void makect(int nc, int *ip, double *c);
+    void bitrv2(int n, int *ip, double *a);
+    void cftfsub(int n, double *a, double *w);
+    void cftbsub(int n, double *a, double *w);
+    void rftfsub(int n, double *a, int nc, double *c);
+    void rftbsub(int n, double *a, int nc, double *c);
+    void dctsub(int n, double *a, int nc, double *c);
+    int j, nw, nc;
+    double xr;
+    
+    nw = ip[0];
+    if (n > (nw << 2)) {
+        nw = n >> 2;
+        makewt(nw, ip, w);
+    }
+    nc = ip[1];
+    if (n > nc) {
+        nc = n;
+        makect(nc, ip, w + nw);
+    }
+    if (isgn < 0) {
+        xr = a[n - 1];
+        for (j = n - 2; j >= 2; j -= 2) {
+            a[j + 1] = a[j] - a[j - 1];
+            a[j] += a[j - 1];
+        }
+        a[1] = a[0] - xr;
+        a[0] += xr;
+        if (n > 4) {
+            rftbsub(n, a, nc, w + nw);
+            bitrv2(n, ip + 2, a);
+            cftbsub(n, a, w);
+        } else if (n == 4) {
+            cftfsub(n, a, w);
+        }
+    }
+    dctsub(n, a, nc, w + nw);
+    if (isgn >= 0) {
+        if (n > 4) {
+            bitrv2(n, ip + 2, a);
+            cftfsub(n, a, w);
+            rftfsub(n, a, nc, w + nw);
+        } else if (n == 4) {
+            cftfsub(n, a, w);
+        }
+        xr = a[0] - a[1];
+        a[0] += a[1];
+        for (j = 2; j < n; j += 2) {
+            a[j - 1] = a[j] - a[j + 1];
+            a[j] += a[j + 1];
+        }
+        a[n - 1] = xr;
+    }
+}
+
+
+void ddst(int n, int isgn, double *a, int *ip, double *w)
+{
+    void makewt(int nw, int *ip, double *w);
+    void makect(int nc, int *ip, double *c);
+    void bitrv2(int n, int *ip, double *a);
+    void cftfsub(int n, double *a, double *w);
+    void cftbsub(int n, double *a, double *w);
+    void rftfsub(int n, double *a, int nc, double *c);
+    void rftbsub(int n, double *a, int nc, double *c);
+    void dstsub(int n, double *a, int nc, double *c);
+    int j, nw, nc;
+    double xr;
+    
+    nw = ip[0];
+    if (n > (nw << 2)) {
+        nw = n >> 2;
+        makewt(nw, ip, w);
+    }
+    nc = ip[1];
+    if (n > nc) {
+        nc = n;
+        makect(nc, ip, w + nw);
+    }
+    if (isgn < 0) {
+        xr = a[n - 1];
+        for (j = n - 2; j >= 2; j -= 2) {
+            a[j + 1] = -a[j] - a[j - 1];
+            a[j] -= a[j - 1];
+        }
+        a[1] = a[0] + xr;
+        a[0] -= xr;
+        if (n > 4) {
+            rftbsub(n, a, nc, w + nw);
+            bitrv2(n, ip + 2, a);
+            cftbsub(n, a, w);
+        } else if (n == 4) {
+            cftfsub(n, a, w);
+        }
+    }
+    dstsub(n, a, nc, w + nw);
+    if (isgn >= 0) {
+        if (n > 4) {
+            bitrv2(n, ip + 2, a);
+            cftfsub(n, a, w);
+            rftfsub(n, a, nc, w + nw);
+        } else if (n == 4) {
+            cftfsub(n, a, w);
+        }
+        xr = a[0] - a[1];
+        a[0] += a[1];
+        for (j = 2; j < n; j += 2) {
+            a[j - 1] = -a[j] - a[j + 1];
+            a[j] -= a[j + 1];
+        }
+        a[n - 1] = -xr;
+    }
+}
+
+
+void dfct(int n, double *a, double *t, int *ip, double *w)
+{
+    void makewt(int nw, int *ip, double *w);
+    void makect(int nc, int *ip, double *c);
+    void bitrv2(int n, int *ip, double *a);
+    void cftfsub(int n, double *a, double *w);
+    void rftfsub(int n, double *a, int nc, double *c);
+    void dctsub(int n, double *a, int nc, double *c);
+    int j, k, l, m, mh, nw, nc;
+    double xr, xi, yr, yi;
+    
+    nw = ip[0];
+    if (n > (nw << 3)) {
+        nw = n >> 3;
+        makewt(nw, ip, w);
+    }
+    nc = ip[1];
+    if (n > (nc << 1)) {
+        nc = n >> 1;
+        makect(nc, ip, w + nw);
+    }
+    m = n >> 1;
+    yi = a[m];
+    xi = a[0] + a[n];
+    a[0] -= a[n];
+    t[0] = xi - yi;
+    t[m] = xi + yi;
+    if (n > 2) {
+        mh = m >> 1;
+        for (j = 1; j < mh; j++) {
+            k = m - j;
+            xr = a[j] - a[n - j];
+            xi = a[j] + a[n - j];
+            yr = a[k] - a[n - k];
+            yi = a[k] + a[n - k];
+            a[j] = xr;
+            a[k] = yr;
+            t[j] = xi - yi;
+            t[k] = xi + yi;
+        }
+        t[mh] = a[mh] + a[n - mh];
+        a[mh] -= a[n - mh];
+        dctsub(m, a, nc, w + nw);
+        if (m > 4) {
+            bitrv2(m, ip + 2, a);
+            cftfsub(m, a, w);
+            rftfsub(m, a, nc, w + nw);
+        } else if (m == 4) {
+            cftfsub(m, a, w);
+        }
+        a[n - 1] = a[0] - a[1];
+        a[1] = a[0] + a[1];
+        for (j = m - 2; j >= 2; j -= 2) {
+            a[2 * j + 1] = a[j] + a[j + 1];
+            a[2 * j - 1] = a[j] - a[j + 1];
+        }
+        l = 2;
+        m = mh;
+        while (m >= 2) {
+            dctsub(m, t, nc, w + nw);
+            if (m > 4) {
+                bitrv2(m, ip + 2, t);
+                cftfsub(m, t, w);
+                rftfsub(m, t, nc, w + nw);
+            } else if (m == 4) {
+                cftfsub(m, t, w);
+            }
+            a[n - l] = t[0] - t[1];
+            a[l] = t[0] + t[1];
+            k = 0;
+            for (j = 2; j < m; j += 2) {
+                k += l << 2;
+                a[k - l] = t[j] - t[j + 1];
+                a[k + l] = t[j] + t[j + 1];
+            }
+            l <<= 1;
+            mh = m >> 1;
+            for (j = 0; j < mh; j++) {
+                k = m - j;
+                t[j] = t[m + k] - t[m + j];
+                t[k] = t[m + k] + t[m + j];
+            }
+            t[mh] = t[m + mh];
+            m = mh;
+        }
+        a[l] = t[0];
+        a[n] = t[2] - t[1];
+        a[0] = t[2] + t[1];
+    } else {
+        a[1] = a[0];
+        a[2] = t[0];
+        a[0] = t[1];
+    }
+}
+
+
+void dfst(int n, double *a, double *t, int *ip, double *w)
+{
+    void makewt(int nw, int *ip, double *w);
+    void makect(int nc, int *ip, double *c);
+    void bitrv2(int n, int *ip, double *a);
+    void cftfsub(int n, double *a, double *w);
+    void rftfsub(int n, double *a, int nc, double *c);
+    void dstsub(int n, double *a, int nc, double *c);
+    int j, k, l, m, mh, nw, nc;
+    double xr, xi, yr, yi;
+    
+    nw = ip[0];
+    if (n > (nw << 3)) {
+        nw = n >> 3;
+        makewt(nw, ip, w);
+    }
+    nc = ip[1];
+    if (n > (nc << 1)) {
+        nc = n >> 1;
+        makect(nc, ip, w + nw);
+    }
+    if (n > 2) {
+        m = n >> 1;
+        mh = m >> 1;
+        for (j = 1; j < mh; j++) {
+            k = m - j;
+            xr = a[j] + a[n - j];
+            xi = a[j] - a[n - j];
+            yr = a[k] + a[n - k];
+            yi = a[k] - a[n - k];
+            a[j] = xr;
+            a[k] = yr;
+            t[j] = xi + yi;
+            t[k] = xi - yi;
+        }
+        t[0] = a[mh] - a[n - mh];
+        a[mh] += a[n - mh];
+        a[0] = a[m];
+        dstsub(m, a, nc, w + nw);
+        if (m > 4) {
+            bitrv2(m, ip + 2, a);
+            cftfsub(m, a, w);
+            rftfsub(m, a, nc, w + nw);
+        } else if (m == 4) {
+            cftfsub(m, a, w);
+        }
+        a[n - 1] = a[1] - a[0];
+        a[1] = a[0] + a[1];
+        for (j = m - 2; j >= 2; j -= 2) {
+            a[2 * j + 1] = a[j] - a[j + 1];
+            a[2 * j - 1] = -a[j] - a[j + 1];
+        }
+        l = 2;
+        m = mh;
+        while (m >= 2) {
+            dstsub(m, t, nc, w + nw);
+            if (m > 4) {
+                bitrv2(m, ip + 2, t);
+                cftfsub(m, t, w);
+                rftfsub(m, t, nc, w + nw);
+            } else if (m == 4) {
+                cftfsub(m, t, w);
+            }
+            a[n - l] = t[1] - t[0];
+            a[l] = t[0] + t[1];
+            k = 0;
+            for (j = 2; j < m; j += 2) {
+                k += l << 2;
+                a[k - l] = -t[j] - t[j + 1];
+                a[k + l] = t[j] - t[j + 1];
+            }
+            l <<= 1;
+            mh = m >> 1;
+            for (j = 1; j < mh; j++) {
+                k = m - j;
+                t[j] = t[m + k] + t[m + j];
+                t[k] = t[m + k] - t[m + j];
+            }
+            t[0] = t[m + mh];
+            m = mh;
+        }
+        a[l] = t[0];
+    }
+    a[0] = 0;
+}
+
+
+/* -------- initializing routines -------- */
+
+
+#include <math.h>
+
+void makewt(int nw, int *ip, double *w)
+{
+    void bitrv2(int n, int *ip, double *a);
+    int j, nwh;
+    double delta, x, y;
+    
+    ip[0] = nw;
+    ip[1] = 1;
+    if (nw > 2) {
+        nwh = nw >> 1;
+        delta = atan(1.0) / nwh;
+        w[0] = 1;
+        w[1] = 0;
+        w[nwh] = cos(delta * nwh);
+        w[nwh + 1] = w[nwh];
+        if (nwh > 2) {
+            for (j = 2; j < nwh; j += 2) {
+                x = cos(delta * j);
+                y = sin(delta * j);
+                w[j] = x;
+                w[j + 1] = y;
+                w[nw - j] = y;
+                w[nw - j + 1] = x;
+            }
+            bitrv2(nw, ip + 2, w);
+        }
+    }
+}
+
+
+void makect(int nc, int *ip, double *c)
+{
+    int j, nch;
+    double delta;
+    
+    ip[1] = nc;
+    if (nc > 1) {
+        nch = nc >> 1;
+        delta = atan(1.0) / nch;
+        c[0] = cos(delta * nch);
+        c[nch] = 0.5 * c[0];
+        for (j = 1; j < nch; j++) {
+            c[j] = 0.5 * cos(delta * j);
+            c[nc - j] = 0.5 * sin(delta * j);
+        }
+    }
+}
+
+
+/* -------- child routines -------- */
+
+
+void bitrv2(int n, int *ip, double *a)
+{
+    int j, j1, k, k1, l, m, m2;
+    double xr, xi, yr, yi;
+    
+    ip[0] = 0;
+    l = n;
+    m = 1;
+    while ((m << 3) < l) {
+        l >>= 1;
+        for (j = 0; j < m; j++) {
+            ip[m + j] = ip[j] + l;
+        }
+        m <<= 1;
+    }
+    m2 = 2 * m;
+    if ((m << 3) == l) {
+        for (k = 0; k < m; k++) {
+            for (j = 0; j < k; j++) {
+                j1 = 2 * j + ip[k];
+                k1 = 2 * k + ip[j];
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m2;
+                k1 += 2 * m2;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m2;
+                k1 -= m2;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m2;
+                k1 += 2 * m2;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+            }
+            j1 = 2 * k + m2 + ip[k];
+            k1 = j1 + m2;
+            xr = a[j1];
+            xi = a[j1 + 1];
+            yr = a[k1];
+            yi = a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+        }
+    } else {
+        for (k = 1; k < m; k++) {
+            for (j = 0; j < k; j++) {
+                j1 = 2 * j + ip[k];
+                k1 = 2 * k + ip[j];
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m2;
+                k1 += m2;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+            }
+        }
+    }
+}
+
+
+void bitrv2conj(int n, int *ip, double *a)
+{
+    int j, j1, k, k1, l, m, m2;
+    double xr, xi, yr, yi;
+    
+    ip[0] = 0;
+    l = n;
+    m = 1;
+    while ((m << 3) < l) {
+        l >>= 1;
+        for (j = 0; j < m; j++) {
+            ip[m + j] = ip[j] + l;
+        }
+        m <<= 1;
+    }
+    m2 = 2 * m;
+    if ((m << 3) == l) {
+        for (k = 0; k < m; k++) {
+            for (j = 0; j < k; j++) {
+                j1 = 2 * j + ip[k];
+                k1 = 2 * k + ip[j];
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m2;
+                k1 += 2 * m2;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m2;
+                k1 -= m2;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m2;
+                k1 += 2 * m2;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+            }
+            k1 = 2 * k + ip[k];
+            a[k1 + 1] = -a[k1 + 1];
+            j1 = k1 + m2;
+            k1 = j1 + m2;
+            xr = a[j1];
+            xi = -a[j1 + 1];
+            yr = a[k1];
+            yi = -a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            k1 += m2;
+            a[k1 + 1] = -a[k1 + 1];
+        }
+    } else {
+        a[1] = -a[1];
+        a[m2 + 1] = -a[m2 + 1];
+        for (k = 1; k < m; k++) {
+            for (j = 0; j < k; j++) {
+                j1 = 2 * j + ip[k];
+                k1 = 2 * k + ip[j];
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m2;
+                k1 += m2;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+            }
+            k1 = 2 * k + ip[k];
+            a[k1 + 1] = -a[k1 + 1];
+            a[k1 + m2 + 1] = -a[k1 + m2 + 1];
+        }
+    }
+}
+
+
+void cftfsub(int n, double *a, double *w)
+{
+    void cft1st(int n, double *a, double *w);
+    void cftmdl(int n, int l, double *a, double *w);
+    int j, j1, j2, j3, l;
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i;
+    
+    l = 2;
+    if (n > 8) {
+        cft1st(n, a, w);
+        l = 8;
+        while ((l << 2) < n) {
+            cftmdl(n, l, a, w);
+            l <<= 2;
+        }
+    }
+    if ((l << 2) == n) {
+        for (j = 0; j < l; j += 2) {
+            j1 = j + l;
+            j2 = j1 + l;
+            j3 = j2 + l;
+            x0r = a[j] + a[j1];
+            x0i = a[j + 1] + a[j1 + 1];
+            x1r = a[j] - a[j1];
+            x1i = a[j + 1] - a[j1 + 1];
+            x2r = a[j2] + a[j3];
+            x2i = a[j2 + 1] + a[j3 + 1];
+            x3r = a[j2] - a[j3];
+            x3i = a[j2 + 1] - a[j3 + 1];
+            a[j] = x0r + x2r;
+            a[j + 1] = x0i + x2i;
+            a[j2] = x0r - x2r;
+            a[j2 + 1] = x0i - x2i;
+            a[j1] = x1r - x3i;
+            a[j1 + 1] = x1i + x3r;
+            a[j3] = x1r + x3i;
+            a[j3 + 1] = x1i - x3r;
+        }
+    } else {
+        for (j = 0; j < l; j += 2) {
+            j1 = j + l;
+            x0r = a[j] - a[j1];
+            x0i = a[j + 1] - a[j1 + 1];
+            a[j] += a[j1];
+            a[j + 1] += a[j1 + 1];
+            a[j1] = x0r;
+            a[j1 + 1] = x0i;
+        }
+    }
+}
+
+
+void cftbsub(int n, double *a, double *w)
+{
+    void cft1st(int n, double *a, double *w);
+    void cftmdl(int n, int l, double *a, double *w);
+    int j, j1, j2, j3, l;
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i;
+    
+    l = 2;
+    if (n > 8) {
+        cft1st(n, a, w);
+        l = 8;
+        while ((l << 2) < n) {
+            cftmdl(n, l, a, w);
+            l <<= 2;
+        }
+    }
+    if ((l << 2) == n) {
+        for (j = 0; j < l; j += 2) {
+            j1 = j + l;
+            j2 = j1 + l;
+            j3 = j2 + l;
+            x0r = a[j] + a[j1];
+            x0i = -a[j + 1] - a[j1 + 1];
+            x1r = a[j] - a[j1];
+            x1i = -a[j + 1] + a[j1 + 1];
+            x2r = a[j2] + a[j3];
+            x2i = a[j2 + 1] + a[j3 + 1];
+            x3r = a[j2] - a[j3];
+            x3i = a[j2 + 1] - a[j3 + 1];
+            a[j] = x0r + x2r;
+            a[j + 1] = x0i - x2i;
+            a[j2] = x0r - x2r;
+            a[j2 + 1] = x0i + x2i;
+            a[j1] = x1r - x3i;
+            a[j1 + 1] = x1i - x3r;
+            a[j3] = x1r + x3i;
+            a[j3 + 1] = x1i + x3r;
+        }
+    } else {
+        for (j = 0; j < l; j += 2) {
+            j1 = j + l;
+            x0r = a[j] - a[j1];
+            x0i = -a[j + 1] + a[j1 + 1];
+            a[j] += a[j1];
+            a[j + 1] = -a[j + 1] - a[j1 + 1];
+            a[j1] = x0r;
+            a[j1 + 1] = x0i;
+        }
+    }
+}
+
+
+void cft1st(int n, double *a, double *w)
+{
+    int j, k1, k2;
+    double wk1r, wk1i, wk2r, wk2i, wk3r, wk3i;
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i;
+    
+    x0r = a[0] + a[2];
+    x0i = a[1] + a[3];
+    x1r = a[0] - a[2];
+    x1i = a[1] - a[3];
+    x2r = a[4] + a[6];
+    x2i = a[5] + a[7];
+    x3r = a[4] - a[6];
+    x3i = a[5] - a[7];
+    a[0] = x0r + x2r;
+    a[1] = x0i + x2i;
+    a[4] = x0r - x2r;
+    a[5] = x0i - x2i;
+    a[2] = x1r - x3i;
+    a[3] = x1i + x3r;
+    a[6] = x1r + x3i;
+    a[7] = x1i - x3r;
+    wk1r = w[2];
+    x0r = a[8] + a[10];
+    x0i = a[9] + a[11];
+    x1r = a[8] - a[10];
+    x1i = a[9] - a[11];
+    x2r = a[12] + a[14];
+    x2i = a[13] + a[15];
+    x3r = a[12] - a[14];
+    x3i = a[13] - a[15];
+    a[8] = x0r + x2r;
+    a[9] = x0i + x2i;
+    a[12] = x2i - x0i;
+    a[13] = x0r - x2r;
+    x0r = x1r - x3i;
+    x0i = x1i + x3r;
+    a[10] = wk1r * (x0r - x0i);
+    a[11] = wk1r * (x0r + x0i);
+    x0r = x3i + x1r;
+    x0i = x3r - x1i;
+    a[14] = wk1r * (x0i - x0r);
+    a[15] = wk1r * (x0i + x0r);
+    k1 = 0;
+    for (j = 16; j < n; j += 16) {
+        k1 += 2;
+        k2 = 2 * k1;
+        wk2r = w[k1];
+        wk2i = w[k1 + 1];
+        wk1r = w[k2];
+        wk1i = w[k2 + 1];
+        wk3r = wk1r - 2 * wk2i * wk1i;
+        wk3i = 2 * wk2i * wk1r - wk1i;
+        x0r = a[j] + a[j + 2];
+        x0i = a[j + 1] + a[j + 3];
+        x1r = a[j] - a[j + 2];
+        x1i = a[j + 1] - a[j + 3];
+        x2r = a[j + 4] + a[j + 6];
+        x2i = a[j + 5] + a[j + 7];
+        x3r = a[j + 4] - a[j + 6];
+        x3i = a[j + 5] - a[j + 7];
+        a[j] = x0r + x2r;
+        a[j + 1] = x0i + x2i;
+        x0r -= x2r;
+        x0i -= x2i;
+        a[j + 4] = wk2r * x0r - wk2i * x0i;
+        a[j + 5] = wk2r * x0i + wk2i * x0r;
+        x0r = x1r - x3i;
+        x0i = x1i + x3r;
+        a[j + 2] = wk1r * x0r - wk1i * x0i;
+        a[j + 3] = wk1r * x0i + wk1i * x0r;
+        x0r = x1r + x3i;
+        x0i = x1i - x3r;
+        a[j + 6] = wk3r * x0r - wk3i * x0i;
+        a[j + 7] = wk3r * x0i + wk3i * x0r;
+        wk1r = w[k2 + 2];
+        wk1i = w[k2 + 3];
+        wk3r = wk1r - 2 * wk2r * wk1i;
+        wk3i = 2 * wk2r * wk1r - wk1i;
+        x0r = a[j + 8] + a[j + 10];
+        x0i = a[j + 9] + a[j + 11];
+        x1r = a[j + 8] - a[j + 10];
+        x1i = a[j + 9] - a[j + 11];
+        x2r = a[j + 12] + a[j + 14];
+        x2i = a[j + 13] + a[j + 15];
+        x3r = a[j + 12] - a[j + 14];
+        x3i = a[j + 13] - a[j + 15];
+        a[j + 8] = x0r + x2r;
+        a[j + 9] = x0i + x2i;
+        x0r -= x2r;
+        x0i -= x2i;
+        a[j + 12] = -wk2i * x0r - wk2r * x0i;
+        a[j + 13] = -wk2i * x0i + wk2r * x0r;
+        x0r = x1r - x3i;
+        x0i = x1i + x3r;
+        a[j + 10] = wk1r * x0r - wk1i * x0i;
+        a[j + 11] = wk1r * x0i + wk1i * x0r;
+        x0r = x1r + x3i;
+        x0i = x1i - x3r;
+        a[j + 14] = wk3r * x0r - wk3i * x0i;
+        a[j + 15] = wk3r * x0i + wk3i * x0r;
+    }
+}
+
+
+void cftmdl(int n, int l, double *a, double *w)
+{
+    int j, j1, j2, j3, k, k1, k2, m, m2;
+    double wk1r, wk1i, wk2r, wk2i, wk3r, wk3i;
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i;
+    
+    m = l << 2;
+    for (j = 0; j < l; j += 2) {
+        j1 = j + l;
+        j2 = j1 + l;
+        j3 = j2 + l;
+        x0r = a[j] + a[j1];
+        x0i = a[j + 1] + a[j1 + 1];
+        x1r = a[j] - a[j1];
+        x1i = a[j + 1] - a[j1 + 1];
+        x2r = a[j2] + a[j3];
+        x2i = a[j2 + 1] + a[j3 + 1];
+        x3r = a[j2] - a[j3];
+        x3i = a[j2 + 1] - a[j3 + 1];
+        a[j] = x0r + x2r;
+        a[j + 1] = x0i + x2i;
+        a[j2] = x0r - x2r;
+        a[j2 + 1] = x0i - x2i;
+        a[j1] = x1r - x3i;
+        a[j1 + 1] = x1i + x3r;
+        a[j3] = x1r + x3i;
+        a[j3 + 1] = x1i - x3r;
+    }
+    wk1r = w[2];
+    for (j = m; j < l + m; j += 2) {
+        j1 = j + l;
+        j2 = j1 + l;
+        j3 = j2 + l;
+        x0r = a[j] + a[j1];
+        x0i = a[j + 1] + a[j1 + 1];
+        x1r = a[j] - a[j1];
+        x1i = a[j + 1] - a[j1 + 1];
+        x2r = a[j2] + a[j3];
+        x2i = a[j2 + 1] + a[j3 + 1];
+        x3r = a[j2] - a[j3];
+        x3i = a[j2 + 1] - a[j3 + 1];
+        a[j] = x0r + x2r;
+        a[j + 1] = x0i + x2i;
+        a[j2] = x2i - x0i;
+        a[j2 + 1] = x0r - x2r;
+        x0r = x1r - x3i;
+        x0i = x1i + x3r;
+        a[j1] = wk1r * (x0r - x0i);
+        a[j1 + 1] = wk1r * (x0r + x0i);
+        x0r = x3i + x1r;
+        x0i = x3r - x1i;
+        a[j3] = wk1r * (x0i - x0r);
+        a[j3 + 1] = wk1r * (x0i + x0r);
+    }
+    k1 = 0;
+    m2 = 2 * m;
+    for (k = m2; k < n; k += m2) {
+        k1 += 2;
+        k2 = 2 * k1;
+        wk2r = w[k1];
+        wk2i = w[k1 + 1];
+        wk1r = w[k2];
+        wk1i = w[k2 + 1];
+        wk3r = wk1r - 2 * wk2i * wk1i;
+        wk3i = 2 * wk2i * wk1r - wk1i;
+        for (j = k; j < l + k; j += 2) {
+            j1 = j + l;
+            j2 = j1 + l;
+            j3 = j2 + l;
+            x0r = a[j] + a[j1];
+            x0i = a[j + 1] + a[j1 + 1];
+            x1r = a[j] - a[j1];
+            x1i = a[j + 1] - a[j1 + 1];
+            x2r = a[j2] + a[j3];
+            x2i = a[j2 + 1] + a[j3 + 1];
+            x3r = a[j2] - a[j3];
+            x3i = a[j2 + 1] - a[j3 + 1];
+            a[j] = x0r + x2r;
+            a[j + 1] = x0i + x2i;
+            x0r -= x2r;
+            x0i -= x2i;
+            a[j2] = wk2r * x0r - wk2i * x0i;
+            a[j2 + 1] = wk2r * x0i + wk2i * x0r;
+            x0r = x1r - x3i;
+            x0i = x1i + x3r;
+            a[j1] = wk1r * x0r - wk1i * x0i;
+            a[j1 + 1] = wk1r * x0i + wk1i * x0r;
+            x0r = x1r + x3i;
+            x0i = x1i - x3r;
+            a[j3] = wk3r * x0r - wk3i * x0i;
+            a[j3 + 1] = wk3r * x0i + wk3i * x0r;
+        }
+        wk1r = w[k2 + 2];
+        wk1i = w[k2 + 3];
+        wk3r = wk1r - 2 * wk2r * wk1i;
+        wk3i = 2 * wk2r * wk1r - wk1i;
+        for (j = k + m; j < l + (k + m); j += 2) {
+            j1 = j + l;
+            j2 = j1 + l;
+            j3 = j2 + l;
+            x0r = a[j] + a[j1];
+            x0i = a[j + 1] + a[j1 + 1];
+            x1r = a[j] - a[j1];
+            x1i = a[j + 1] - a[j1 + 1];
+            x2r = a[j2] + a[j3];
+            x2i = a[j2 + 1] + a[j3 + 1];
+            x3r = a[j2] - a[j3];
+            x3i = a[j2 + 1] - a[j3 + 1];
+            a[j] = x0r + x2r;
+            a[j + 1] = x0i + x2i;
+            x0r -= x2r;
+            x0i -= x2i;
+            a[j2] = -wk2i * x0r - wk2r * x0i;
+            a[j2 + 1] = -wk2i * x0i + wk2r * x0r;
+            x0r = x1r - x3i;
+            x0i = x1i + x3r;
+            a[j1] = wk1r * x0r - wk1i * x0i;
+            a[j1 + 1] = wk1r * x0i + wk1i * x0r;
+            x0r = x1r + x3i;
+            x0i = x1i - x3r;
+            a[j3] = wk3r * x0r - wk3i * x0i;
+            a[j3 + 1] = wk3r * x0i + wk3i * x0r;
+        }
+    }
+}
+
+
+void rftfsub(int n, double *a, int nc, double *c)
+{
+    int j, k, kk, ks, m;
+    double wkr, wki, xr, xi, yr, yi;
+    
+    m = n >> 1;
+    ks = 2 * nc / m;
+    kk = 0;
+    for (j = 2; j < m; j += 2) {
+        k = n - j;
+        kk += ks;
+        wkr = 0.5 - c[nc - kk];
+        wki = c[kk];
+        xr = a[j] - a[k];
+        xi = a[j + 1] + a[k + 1];
+        yr = wkr * xr - wki * xi;
+        yi = wkr * xi + wki * xr;
+        a[j] -= yr;
+        a[j + 1] -= yi;
+        a[k] += yr;
+        a[k + 1] -= yi;
+    }
+}
+
+
+void rftbsub(int n, double *a, int nc, double *c)
+{
+    int j, k, kk, ks, m;
+    double wkr, wki, xr, xi, yr, yi;
+    
+    a[1] = -a[1];
+    m = n >> 1;
+    ks = 2 * nc / m;
+    kk = 0;
+    for (j = 2; j < m; j += 2) {
+        k = n - j;
+        kk += ks;
+        wkr = 0.5 - c[nc - kk];
+        wki = c[kk];
+        xr = a[j] - a[k];
+        xi = a[j + 1] + a[k + 1];
+        yr = wkr * xr + wki * xi;
+        yi = wkr * xi - wki * xr;
+        a[j] -= yr;
+        a[j + 1] = yi - a[j + 1];
+        a[k] += yr;
+        a[k + 1] = yi - a[k + 1];
+    }
+    a[m + 1] = -a[m + 1];
+}
+
+
+void dctsub(int n, double *a, int nc, double *c)
+{
+    int j, k, kk, ks, m;
+    double wkr, wki, xr;
+    
+    m = n >> 1;
+    ks = nc / n;
+    kk = 0;
+    for (j = 1; j < m; j++) {
+        k = n - j;
+        kk += ks;
+        wkr = c[kk] - c[nc - kk];
+        wki = c[kk] + c[nc - kk];
+        xr = wki * a[j] - wkr * a[k];
+        a[j] = wkr * a[j] + wki * a[k];
+        a[k] = xr;
+    }
+    a[m] *= c[0];
+}
+
+
+void dstsub(int n, double *a, int nc, double *c)
+{
+    int j, k, kk, ks, m;
+    double wkr, wki, xr;
+    
+    m = n >> 1;
+    ks = nc / n;
+    kk = 0;
+    for (j = 1; j < m; j++) {
+        k = n - j;
+        kk += ks;
+        wkr = c[kk] - c[nc - kk];
+        wki = c[kk] + c[nc - kk];
+        xr = wki * a[k] - wkr * a[j];
+        a[k] = wkr * a[k] + wki * a[j];
+        a[j] = xr;
+    }
+    a[m] *= c[0];
+}
+
diff --git a/third_party/tflite-micro/third_party/fft2d/fft4g_h.c b/third_party/tflite-micro/third_party/fft2d/fft4g_h.c
new file mode 100644
index 00000000..1064285d
--- /dev/null
+++ b/third_party/tflite-micro/third_party/fft2d/fft4g_h.c
@@ -0,0 +1,1368 @@
+/*
+Fast Fourier/Cosine/Sine Transform
+    dimension   :one
+    data length :power of 2
+    decimation  :frequency
+    radix       :4, 2
+    data        :inplace
+    table       :not use
+functions
+    cdft: Complex Discrete Fourier Transform
+    rdft: Real Discrete Fourier Transform
+    ddct: Discrete Cosine Transform
+    ddst: Discrete Sine Transform
+    dfct: Cosine Transform of RDFT (Real Symmetric DFT)
+    dfst: Sine Transform of RDFT (Real Anti-symmetric DFT)
+function prototypes
+    void cdft(int, int, double *);
+    void rdft(int, int, double *);
+    void ddct(int, int, double *);
+    void ddst(int, int, double *);
+    void dfct(int, double *);
+    void dfst(int, double *);
+
+
+-------- Complex DFT (Discrete Fourier Transform) --------
+    [definition]
+        <case1>
+            X[k] = sum_j=0^n-1 x[j]*exp(2*pi*i*j*k/n), 0<=k<n
+        <case2>
+            X[k] = sum_j=0^n-1 x[j]*exp(-2*pi*i*j*k/n), 0<=k<n
+        (notes: sum_j=0^n-1 is a summation from j=0 to n-1)
+    [usage]
+        <case1>
+            cdft(2*n, 1, a);
+        <case2>
+            cdft(2*n, -1, a);
+    [parameters]
+        2*n            :data length (int)
+                        n >= 1, n = power of 2
+        a[0...2*n-1]   :input/output data (double *)
+                        input data
+                            a[2*j] = Re(x[j]), 
+                            a[2*j+1] = Im(x[j]), 0<=j<n
+                        output data
+                            a[2*k] = Re(X[k]), 
+                            a[2*k+1] = Im(X[k]), 0<=k<n
+    [remark]
+        Inverse of 
+            cdft(2*n, -1, a);
+        is 
+            cdft(2*n, 1, a);
+            for (j = 0; j <= 2 * n - 1; j++) {
+                a[j] *= 1.0 / n;
+            }
+        .
+
+
+-------- Real DFT / Inverse of Real DFT --------
+    [definition]
+        <case1> RDFT
+            R[k] = sum_j=0^n-1 a[j]*cos(2*pi*j*k/n), 0<=k<=n/2
+            I[k] = sum_j=0^n-1 a[j]*sin(2*pi*j*k/n), 0<k<n/2
+        <case2> IRDFT (excluding scale)
+            a[k] = (R[0] + R[n/2]*cos(pi*k))/2 + 
+                   sum_j=1^n/2-1 R[j]*cos(2*pi*j*k/n) + 
+                   sum_j=1^n/2-1 I[j]*sin(2*pi*j*k/n), 0<=k<n
+    [usage]
+        <case1>
+            rdft(n, 1, a);
+        <case2>
+            rdft(n, -1, a);
+    [parameters]
+        n              :data length (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        <case1>
+                            output data
+                                a[2*k] = R[k], 0<=k<n/2
+                                a[2*k+1] = I[k], 0<k<n/2
+                                a[1] = R[n/2]
+                        <case2>
+                            input data
+                                a[2*j] = R[j], 0<=j<n/2
+                                a[2*j+1] = I[j], 0<j<n/2
+                                a[1] = R[n/2]
+    [remark]
+        Inverse of 
+            rdft(n, 1, a);
+        is 
+            rdft(n, -1, a);
+            for (j = 0; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- DCT (Discrete Cosine Transform) / Inverse of DCT --------
+    [definition]
+        <case1> IDCT (excluding scale)
+            C[k] = sum_j=0^n-1 a[j]*cos(pi*j*(k+1/2)/n), 0<=k<n
+        <case2> DCT
+            C[k] = sum_j=0^n-1 a[j]*cos(pi*(j+1/2)*k/n), 0<=k<n
+    [usage]
+        <case1>
+            ddct(n, 1, a);
+        <case2>
+            ddct(n, -1, a);
+    [parameters]
+        n              :data length (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        output data
+                            a[k] = C[k], 0<=k<n
+    [remark]
+        Inverse of 
+            ddct(n, -1, a);
+        is 
+            a[0] *= 0.5;
+            ddct(n, 1, a);
+            for (j = 0; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- DST (Discrete Sine Transform) / Inverse of DST --------
+    [definition]
+        <case1> IDST (excluding scale)
+            S[k] = sum_j=1^n A[j]*sin(pi*j*(k+1/2)/n), 0<=k<n
+        <case2> DST
+            S[k] = sum_j=0^n-1 a[j]*sin(pi*(j+1/2)*k/n), 0<k<=n
+    [usage]
+        <case1>
+            ddst(n, 1, a);
+        <case2>
+            ddst(n, -1, a);
+    [parameters]
+        n              :data length (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        <case1>
+                            input data
+                                a[j] = A[j], 0<j<n
+                                a[0] = A[n]
+                            output data
+                                a[k] = S[k], 0<=k<n
+                        <case2>
+                            output data
+                                a[k] = S[k], 0<k<n
+                                a[0] = S[n]
+    [remark]
+        Inverse of 
+            ddst(n, -1, a);
+        is 
+            a[0] *= 0.5;
+            ddst(n, 1, a);
+            for (j = 0; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- Cosine Transform of RDFT (Real Symmetric DFT) --------
+    [definition]
+        C[k] = sum_j=0^n a[j]*cos(pi*j*k/n), 0<=k<=n
+    [usage]
+        dfct(n, a);
+    [parameters]
+        n              :data length - 1 (int)
+                        n >= 2, n = power of 2
+        a[0...n]       :input/output data (double *)
+                        output data
+                            a[k] = C[k], 0<=k<=n
+    [remark]
+        Inverse of 
+            a[0] *= 0.5;
+            a[n] *= 0.5;
+            dfct(n, a);
+        is 
+            a[0] *= 0.5;
+            a[n] *= 0.5;
+            dfct(n, a);
+            for (j = 0; j <= n; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- Sine Transform of RDFT (Real Anti-symmetric DFT) --------
+    [definition]
+        S[k] = sum_j=1^n-1 a[j]*sin(pi*j*k/n), 0<k<n
+    [usage]
+        dfst(n, a);
+    [parameters]
+        n              :data length + 1 (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        output data
+                            a[k] = S[k], 0<k<n
+                        (a[0] is used for work area)
+    [remark]
+        Inverse of 
+            dfst(n, a);
+        is 
+            dfst(n, a);
+            for (j = 1; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+*/
+
+
+void cdft(int n, int isgn, double *a)
+{
+    void bitrv2(int n, double *a);
+    void bitrv2conj(int n, double *a);
+    void cftfsub(int n, double *a);
+    void cftbsub(int n, double *a);
+    
+    if (n > 4) {
+        if (isgn >= 0) {
+            bitrv2(n, a);
+            cftfsub(n, a);
+        } else {
+            bitrv2conj(n, a);
+            cftbsub(n, a);
+        }
+    } else if (n == 4) {
+        cftfsub(n, a);
+    }
+}
+
+
+void rdft(int n, int isgn, double *a)
+{
+    void bitrv2(int n, double *a);
+    void cftfsub(int n, double *a);
+    void cftbsub(int n, double *a);
+    void rftfsub(int n, double *a);
+    void rftbsub(int n, double *a);
+    double xi;
+    
+    if (isgn >= 0) {
+        if (n > 4) {
+            bitrv2(n, a);
+            cftfsub(n, a);
+            rftfsub(n, a);
+        } else if (n == 4) {
+            cftfsub(n, a);
+        }
+        xi = a[0] - a[1];
+        a[0] += a[1];
+        a[1] = xi;
+    } else {
+        a[1] = 0.5 * (a[0] - a[1]);
+        a[0] -= a[1];
+        if (n > 4) {
+            rftbsub(n, a);
+            bitrv2(n, a);
+            cftbsub(n, a);
+        } else if (n == 4) {
+            cftfsub(n, a);
+        }
+    }
+}
+
+
+void ddct(int n, int isgn, double *a)
+{
+    void bitrv2(int n, double *a);
+    void cftfsub(int n, double *a);
+    void cftbsub(int n, double *a);
+    void rftfsub(int n, double *a);
+    void rftbsub(int n, double *a);
+    void dctsub(int n, double *a);
+    void dctsub4(int n, double *a);
+    int j;
+    double xr;
+    
+    if (isgn < 0) {
+        xr = a[n - 1];
+        for (j = n - 2; j >= 2; j -= 2) {
+            a[j + 1] = a[j] - a[j - 1];
+            a[j] += a[j - 1];
+        }
+        a[1] = a[0] - xr;
+        a[0] += xr;
+        if (n > 4) {
+            rftbsub(n, a);
+            bitrv2(n, a);
+            cftbsub(n, a);
+        } else if (n == 4) {
+            cftfsub(n, a);
+        }
+    }
+    if (n > 4) {
+        dctsub(n, a);
+    } else {
+        dctsub4(n, a);
+    }
+    if (isgn >= 0) {
+        if (n > 4) {
+            bitrv2(n, a);
+            cftfsub(n, a);
+            rftfsub(n, a);
+        } else if (n == 4) {
+            cftfsub(n, a);
+        }
+        xr = a[0] - a[1];
+        a[0] += a[1];
+        for (j = 2; j < n; j += 2) {
+            a[j - 1] = a[j] - a[j + 1];
+            a[j] += a[j + 1];
+        }
+        a[n - 1] = xr;
+    }
+}
+
+
+void ddst(int n, int isgn, double *a)
+{
+    void bitrv2(int n, double *a);
+    void cftfsub(int n, double *a);
+    void cftbsub(int n, double *a);
+    void rftfsub(int n, double *a);
+    void rftbsub(int n, double *a);
+    void dstsub(int n, double *a);
+    void dstsub4(int n, double *a);
+    int j;
+    double xr;
+    
+    if (isgn < 0) {
+        xr = a[n - 1];
+        for (j = n - 2; j >= 2; j -= 2) {
+            a[j + 1] = -a[j] - a[j - 1];
+            a[j] -= a[j - 1];
+        }
+        a[1] = a[0] + xr;
+        a[0] -= xr;
+        if (n > 4) {
+            rftbsub(n, a);
+            bitrv2(n, a);
+            cftbsub(n, a);
+        } else if (n == 4) {
+            cftfsub(n, a);
+        }
+    }
+    if (n > 4) {
+        dstsub(n, a);
+    } else {
+        dstsub4(n, a);
+    }
+    if (isgn >= 0) {
+        if (n > 4) {
+            bitrv2(n, a);
+            cftfsub(n, a);
+            rftfsub(n, a);
+        } else if (n == 4) {
+            cftfsub(n, a);
+        }
+        xr = a[0] - a[1];
+        a[0] += a[1];
+        for (j = 2; j < n; j += 2) {
+            a[j - 1] = -a[j] - a[j + 1];
+            a[j] -= a[j + 1];
+        }
+        a[n - 1] = -xr;
+    }
+}
+
+
+void dfct(int n, double *a)
+{
+    void ddct(int n, int isgn, double *a);
+    void bitrv1(int n, double *a);
+    int j, k, m, mh;
+    double xr, xi, yr, yi, an;
+    
+    m = n >> 1;
+    for (j = 0; j < m; j++) {
+        k = n - j;
+        xr = a[j] + a[k];
+        a[j] -= a[k];
+        a[k] = xr;
+    }
+    an = a[n];
+    while (m >= 2) {
+        ddct(m, 1, a);
+        bitrv1(m, a);
+        mh = m >> 1;
+        xi = a[m];
+        a[m] = a[0];
+        a[0] = an - xi;
+        an += xi;
+        for (j = 1; j < mh; j++) {
+            k = m - j;
+            xr = a[m + k];
+            xi = a[m + j];
+            yr = a[j];
+            yi = a[k];
+            a[m + j] = yr;
+            a[m + k] = yi;
+            a[j] = xr - xi;
+            a[k] = xr + xi;
+        }
+        xr = a[mh];
+        a[mh] = a[m + mh];
+        a[m + mh] = xr;
+        m = mh;
+    }
+    xi = a[1];
+    a[1] = a[0];
+    a[0] = an + xi;
+    a[n] = an - xi;
+    bitrv1(n, a);
+}
+
+
+void dfst(int n, double *a)
+{
+    void ddst(int n, int isgn, double *a);
+    void bitrv1(int n, double *a);
+    int j, k, m, mh;
+    double xr, xi, yr, yi;
+    
+    m = n >> 1;
+    for (j = 1; j < m; j++) {
+        k = n - j;
+        xr = a[j] - a[k];
+        a[j] += a[k];
+        a[k] = xr;
+    }
+    a[0] = a[m];
+    while (m >= 2) {
+        ddst(m, 1, a);
+        bitrv1(m, a);
+        mh = m >> 1;
+        for (j = 1; j < mh; j++) {
+            k = m - j;
+            xr = a[m + k];
+            xi = a[m + j];
+            yr = a[j];
+            yi = a[k];
+            a[m + j] = yr;
+            a[m + k] = yi;
+            a[j] = xr + xi;
+            a[k] = xr - xi;
+        }
+        a[m] = a[0];
+        a[0] = a[m + mh];
+        a[m + mh] = a[mh];
+        m = mh;
+    }
+    a[1] = a[0];
+    a[0] = 0;
+    bitrv1(n, a);
+}
+
+
+/* -------- child routines -------- */
+
+
+#include <math.h>
+#ifndef M_PI_2
+#define M_PI_2      1.570796326794896619231321691639751442098584699687
+#endif
+#ifndef WR5000  /* cos(M_PI_2*0.5000) */
+#define WR5000      0.707106781186547524400844362104849039284835937688
+#endif
+#ifndef WR2500  /* cos(M_PI_2*0.2500) */
+#define WR2500      0.923879532511286756128183189396788286822416625863
+#endif
+#ifndef WI2500  /* sin(M_PI_2*0.2500) */
+#define WI2500      0.382683432365089771728459984030398866761344562485
+#endif
+
+
+#ifndef RDFT_LOOP_DIV  /* control of the RDFT's speed & tolerance */
+#define RDFT_LOOP_DIV 64
+#endif
+
+#ifndef DCST_LOOP_DIV  /* control of the DCT,DST's speed & tolerance */
+#define DCST_LOOP_DIV 64
+#endif
+
+
+void bitrv2(int n, double *a)
+{
+    int j0, k0, j1, k1, l, m, i, j, k;
+    double xr, xi, yr, yi;
+    
+    l = n >> 2;
+    m = 2;
+    while (m < l) {
+        l >>= 1;
+        m <<= 1;
+    }
+    if (m == l) {
+        j0 = 0;
+        for (k0 = 0; k0 < m; k0 += 2) {
+            k = k0;
+            for (j = j0; j < j0 + k0; j += 2) {
+                xr = a[j];
+                xi = a[j + 1];
+                yr = a[k];
+                yi = a[k + 1];
+                a[j] = yr;
+                a[j + 1] = yi;
+                a[k] = xr;
+                a[k + 1] = xi;
+                j1 = j + m;
+                k1 = k + 2 * m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m;
+                k1 -= m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m;
+                k1 += 2 * m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                for (i = n >> 1; i > (k ^= i); i >>= 1);
+            }
+            j1 = j0 + k0 + m;
+            k1 = j1 + m;
+            xr = a[j1];
+            xi = a[j1 + 1];
+            yr = a[k1];
+            yi = a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            for (i = n >> 1; i > (j0 ^= i); i >>= 1);
+        }
+    } else {
+        j0 = 0;
+        for (k0 = 2; k0 < m; k0 += 2) {
+            for (i = n >> 1; i > (j0 ^= i); i >>= 1);
+            k = k0;
+            for (j = j0; j < j0 + k0; j += 2) {
+                xr = a[j];
+                xi = a[j + 1];
+                yr = a[k];
+                yi = a[k + 1];
+                a[j] = yr;
+                a[j + 1] = yi;
+                a[k] = xr;
+                a[k + 1] = xi;
+                j1 = j + m;
+                k1 = k + m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                for (i = n >> 1; i > (k ^= i); i >>= 1);
+            }
+        }
+    }
+}
+
+
+void bitrv2conj(int n, double *a)
+{
+    int j0, k0, j1, k1, l, m, i, j, k;
+    double xr, xi, yr, yi;
+    
+    l = n >> 2;
+    m = 2;
+    while (m < l) {
+        l >>= 1;
+        m <<= 1;
+    }
+    if (m == l) {
+        j0 = 0;
+        for (k0 = 0; k0 < m; k0 += 2) {
+            k = k0;
+            for (j = j0; j < j0 + k0; j += 2) {
+                xr = a[j];
+                xi = -a[j + 1];
+                yr = a[k];
+                yi = -a[k + 1];
+                a[j] = yr;
+                a[j + 1] = yi;
+                a[k] = xr;
+                a[k + 1] = xi;
+                j1 = j + m;
+                k1 = k + 2 * m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m;
+                k1 -= m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m;
+                k1 += 2 * m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                for (i = n >> 1; i > (k ^= i); i >>= 1);
+            }
+            k1 = j0 + k0;
+            a[k1 + 1] = -a[k1 + 1];
+            j1 = k1 + m;
+            k1 = j1 + m;
+            xr = a[j1];
+            xi = -a[j1 + 1];
+            yr = a[k1];
+            yi = -a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            k1 += m;
+            a[k1 + 1] = -a[k1 + 1];
+            for (i = n >> 1; i > (j0 ^= i); i >>= 1);
+        }
+    } else {
+        a[1] = -a[1];
+        a[m + 1] = -a[m + 1];
+        j0 = 0;
+        for (k0 = 2; k0 < m; k0 += 2) {
+            for (i = n >> 1; i > (j0 ^= i); i >>= 1);
+            k = k0;
+            for (j = j0; j < j0 + k0; j += 2) {
+                xr = a[j];
+                xi = -a[j + 1];
+                yr = a[k];
+                yi = -a[k + 1];
+                a[j] = yr;
+                a[j + 1] = yi;
+                a[k] = xr;
+                a[k + 1] = xi;
+                j1 = j + m;
+                k1 = k + m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                for (i = n >> 1; i > (k ^= i); i >>= 1);
+            }
+            k1 = j0 + k0;
+            a[k1 + 1] = -a[k1 + 1];
+            a[k1 + m + 1] = -a[k1 + m + 1];
+        }
+    }
+}
+
+
+void bitrv1(int n, double *a)
+{
+    int j0, k0, j1, k1, l, m, i, j, k;
+    double x;
+    
+    l = n >> 2;
+    m = 1;
+    while (m < l) {
+        l >>= 1;
+        m <<= 1;
+    }
+    if (m == l) {
+        j0 = 0;
+        for (k0 = 0; k0 < m; k0++) {
+            k = k0;
+            for (j = j0; j < j0 + k0; j++) {
+                x = a[j];
+                a[j] = a[k];
+                a[k] = x;
+                j1 = j + m;
+                k1 = k + 2 * m;
+                x = a[j1];
+                a[j1] = a[k1];
+                a[k1] = x;
+                j1 += m;
+                k1 -= m;
+                x = a[j1];
+                a[j1] = a[k1];
+                a[k1] = x;
+                j1 += m;
+                k1 += 2 * m;
+                x = a[j1];
+                a[j1] = a[k1];
+                a[k1] = x;
+                for (i = n >> 1; i > (k ^= i); i >>= 1);
+            }
+            j1 = j0 + k0 + m;
+            k1 = j1 + m;
+            x = a[j1];
+            a[j1] = a[k1];
+            a[k1] = x;
+            for (i = n >> 1; i > (j0 ^= i); i >>= 1);
+        }
+    } else {
+        j0 = 0;
+        for (k0 = 1; k0 < m; k0++) {
+            for (i = n >> 1; i > (j0 ^= i); i >>= 1);
+            k = k0;
+            for (j = j0; j < j0 + k0; j++) {
+                x = a[j];
+                a[j] = a[k];
+                a[k] = x;
+                j1 = j + m;
+                k1 = k + m;
+                x = a[j1];
+                a[j1] = a[k1];
+                a[k1] = x;
+                for (i = n >> 1; i > (k ^= i); i >>= 1);
+            }
+        }
+    }
+}
+
+
+void cftfsub(int n, double *a)
+{
+    void cft1st(int n, double *a);
+    void cftmdl(int n, int l, double *a);
+    int j, j1, j2, j3, l;
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i;
+    
+    l = 2;
+    if (n > 8) {
+        cft1st(n, a);
+        l = 8;
+        while ((l << 2) < n) {
+            cftmdl(n, l, a);
+            l <<= 2;
+        }
+    }
+    if ((l << 2) == n) {
+        for (j = 0; j < l; j += 2) {
+            j1 = j + l;
+            j2 = j1 + l;
+            j3 = j2 + l;
+            x0r = a[j] + a[j1];
+            x0i = a[j + 1] + a[j1 + 1];
+            x1r = a[j] - a[j1];
+            x1i = a[j + 1] - a[j1 + 1];
+            x2r = a[j2] + a[j3];
+            x2i = a[j2 + 1] + a[j3 + 1];
+            x3r = a[j2] - a[j3];
+            x3i = a[j2 + 1] - a[j3 + 1];
+            a[j] = x0r + x2r;
+            a[j + 1] = x0i + x2i;
+            a[j2] = x0r - x2r;
+            a[j2 + 1] = x0i - x2i;
+            a[j1] = x1r - x3i;
+            a[j1 + 1] = x1i + x3r;
+            a[j3] = x1r + x3i;
+            a[j3 + 1] = x1i - x3r;
+        }
+    } else {
+        for (j = 0; j < l; j += 2) {
+            j1 = j + l;
+            x0r = a[j] - a[j1];
+            x0i = a[j + 1] - a[j1 + 1];
+            a[j] += a[j1];
+            a[j + 1] += a[j1 + 1];
+            a[j1] = x0r;
+            a[j1 + 1] = x0i;
+        }
+    }
+}
+
+
+void cftbsub(int n, double *a)
+{
+    void cft1st(int n, double *a);
+    void cftmdl(int n, int l, double *a);
+    int j, j1, j2, j3, l;
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i;
+    
+    l = 2;
+    if (n > 8) {
+        cft1st(n, a);
+        l = 8;
+        while ((l << 2) < n) {
+            cftmdl(n, l, a);
+            l <<= 2;
+        }
+    }
+    if ((l << 2) == n) {
+        for (j = 0; j < l; j += 2) {
+            j1 = j + l;
+            j2 = j1 + l;
+            j3 = j2 + l;
+            x0r = a[j] + a[j1];
+            x0i = -a[j + 1] - a[j1 + 1];
+            x1r = a[j] - a[j1];
+            x1i = -a[j + 1] + a[j1 + 1];
+            x2r = a[j2] + a[j3];
+            x2i = a[j2 + 1] + a[j3 + 1];
+            x3r = a[j2] - a[j3];
+            x3i = a[j2 + 1] - a[j3 + 1];
+            a[j] = x0r + x2r;
+            a[j + 1] = x0i - x2i;
+            a[j2] = x0r - x2r;
+            a[j2 + 1] = x0i + x2i;
+            a[j1] = x1r - x3i;
+            a[j1 + 1] = x1i - x3r;
+            a[j3] = x1r + x3i;
+            a[j3 + 1] = x1i + x3r;
+        }
+    } else {
+        for (j = 0; j < l; j += 2) {
+            j1 = j + l;
+            x0r = a[j] - a[j1];
+            x0i = -a[j + 1] + a[j1 + 1];
+            a[j] += a[j1];
+            a[j + 1] = -a[j + 1] - a[j1 + 1];
+            a[j1] = x0r;
+            a[j1 + 1] = x0i;
+        }
+    }
+}
+
+
+void cft1st(int n, double *a)
+{
+    int j, kj, kr;
+    double ew, wn4r, wk1r, wk1i, wk2r, wk2i, wk3r, wk3i;
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i;
+    
+    x0r = a[0] + a[2];
+    x0i = a[1] + a[3];
+    x1r = a[0] - a[2];
+    x1i = a[1] - a[3];
+    x2r = a[4] + a[6];
+    x2i = a[5] + a[7];
+    x3r = a[4] - a[6];
+    x3i = a[5] - a[7];
+    a[0] = x0r + x2r;
+    a[1] = x0i + x2i;
+    a[4] = x0r - x2r;
+    a[5] = x0i - x2i;
+    a[2] = x1r - x3i;
+    a[3] = x1i + x3r;
+    a[6] = x1r + x3i;
+    a[7] = x1i - x3r;
+    wn4r = WR5000;
+    x0r = a[8] + a[10];
+    x0i = a[9] + a[11];
+    x1r = a[8] - a[10];
+    x1i = a[9] - a[11];
+    x2r = a[12] + a[14];
+    x2i = a[13] + a[15];
+    x3r = a[12] - a[14];
+    x3i = a[13] - a[15];
+    a[8] = x0r + x2r;
+    a[9] = x0i + x2i;
+    a[12] = x2i - x0i;
+    a[13] = x0r - x2r;
+    x0r = x1r - x3i;
+    x0i = x1i + x3r;
+    a[10] = wn4r * (x0r - x0i);
+    a[11] = wn4r * (x0r + x0i);
+    x0r = x3i + x1r;
+    x0i = x3r - x1i;
+    a[14] = wn4r * (x0i - x0r);
+    a[15] = wn4r * (x0i + x0r);
+    ew = M_PI_2 / n;
+    kr = 0;
+    for (j = 16; j < n; j += 16) {
+        for (kj = n >> 2; kj > (kr ^= kj); kj >>= 1);
+        wk1r = cos(ew * kr);
+        wk1i = sin(ew * kr);
+        wk2r = 1 - 2 * wk1i * wk1i;
+        wk2i = 2 * wk1i * wk1r;
+        wk3r = wk1r - 2 * wk2i * wk1i;
+        wk3i = 2 * wk2i * wk1r - wk1i;
+        x0r = a[j] + a[j + 2];
+        x0i = a[j + 1] + a[j + 3];
+        x1r = a[j] - a[j + 2];
+        x1i = a[j + 1] - a[j + 3];
+        x2r = a[j + 4] + a[j + 6];
+        x2i = a[j + 5] + a[j + 7];
+        x3r = a[j + 4] - a[j + 6];
+        x3i = a[j + 5] - a[j + 7];
+        a[j] = x0r + x2r;
+        a[j + 1] = x0i + x2i;
+        x0r -= x2r;
+        x0i -= x2i;
+        a[j + 4] = wk2r * x0r - wk2i * x0i;
+        a[j + 5] = wk2r * x0i + wk2i * x0r;
+        x0r = x1r - x3i;
+        x0i = x1i + x3r;
+        a[j + 2] = wk1r * x0r - wk1i * x0i;
+        a[j + 3] = wk1r * x0i + wk1i * x0r;
+        x0r = x1r + x3i;
+        x0i = x1i - x3r;
+        a[j + 6] = wk3r * x0r - wk3i * x0i;
+        a[j + 7] = wk3r * x0i + wk3i * x0r;
+        x0r = wn4r * (wk1r - wk1i);
+        wk1i = wn4r * (wk1r + wk1i);
+        wk1r = x0r;
+        wk3r = wk1r - 2 * wk2r * wk1i;
+        wk3i = 2 * wk2r * wk1r - wk1i;
+        x0r = a[j + 8] + a[j + 10];
+        x0i = a[j + 9] + a[j + 11];
+        x1r = a[j + 8] - a[j + 10];
+        x1i = a[j + 9] - a[j + 11];
+        x2r = a[j + 12] + a[j + 14];
+        x2i = a[j + 13] + a[j + 15];
+        x3r = a[j + 12] - a[j + 14];
+        x3i = a[j + 13] - a[j + 15];
+        a[j + 8] = x0r + x2r;
+        a[j + 9] = x0i + x2i;
+        x0r -= x2r;
+        x0i -= x2i;
+        a[j + 12] = -wk2i * x0r - wk2r * x0i;
+        a[j + 13] = -wk2i * x0i + wk2r * x0r;
+        x0r = x1r - x3i;
+        x0i = x1i + x3r;
+        a[j + 10] = wk1r * x0r - wk1i * x0i;
+        a[j + 11] = wk1r * x0i + wk1i * x0r;
+        x0r = x1r + x3i;
+        x0i = x1i - x3r;
+        a[j + 14] = wk3r * x0r - wk3i * x0i;
+        a[j + 15] = wk3r * x0i + wk3i * x0r;
+    }
+}
+
+
+void cftmdl(int n, int l, double *a)
+{
+    int j, j1, j2, j3, k, kj, kr, m, m2;
+    double ew, wn4r, wk1r, wk1i, wk2r, wk2i, wk3r, wk3i;
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i;
+    
+    m = l << 2;
+    for (j = 0; j < l; j += 2) {
+        j1 = j + l;
+        j2 = j1 + l;
+        j3 = j2 + l;
+        x0r = a[j] + a[j1];
+        x0i = a[j + 1] + a[j1 + 1];
+        x1r = a[j] - a[j1];
+        x1i = a[j + 1] - a[j1 + 1];
+        x2r = a[j2] + a[j3];
+        x2i = a[j2 + 1] + a[j3 + 1];
+        x3r = a[j2] - a[j3];
+        x3i = a[j2 + 1] - a[j3 + 1];
+        a[j] = x0r + x2r;
+        a[j + 1] = x0i + x2i;
+        a[j2] = x0r - x2r;
+        a[j2 + 1] = x0i - x2i;
+        a[j1] = x1r - x3i;
+        a[j1 + 1] = x1i + x3r;
+        a[j3] = x1r + x3i;
+        a[j3 + 1] = x1i - x3r;
+    }
+    wn4r = WR5000;
+    for (j = m; j < l + m; j += 2) {
+        j1 = j + l;
+        j2 = j1 + l;
+        j3 = j2 + l;
+        x0r = a[j] + a[j1];
+        x0i = a[j + 1] + a[j1 + 1];
+        x1r = a[j] - a[j1];
+        x1i = a[j + 1] - a[j1 + 1];
+        x2r = a[j2] + a[j3];
+        x2i = a[j2 + 1] + a[j3 + 1];
+        x3r = a[j2] - a[j3];
+        x3i = a[j2 + 1] - a[j3 + 1];
+        a[j] = x0r + x2r;
+        a[j + 1] = x0i + x2i;
+        a[j2] = x2i - x0i;
+        a[j2 + 1] = x0r - x2r;
+        x0r = x1r - x3i;
+        x0i = x1i + x3r;
+        a[j1] = wn4r * (x0r - x0i);
+        a[j1 + 1] = wn4r * (x0r + x0i);
+        x0r = x3i + x1r;
+        x0i = x3r - x1i;
+        a[j3] = wn4r * (x0i - x0r);
+        a[j3 + 1] = wn4r * (x0i + x0r);
+    }
+    ew = M_PI_2 / n;
+    kr = 0;
+    m2 = 2 * m;
+    for (k = m2; k < n; k += m2) {
+        for (kj = n >> 2; kj > (kr ^= kj); kj >>= 1);
+        wk1r = cos(ew * kr);
+        wk1i = sin(ew * kr);
+        wk2r = 1 - 2 * wk1i * wk1i;
+        wk2i = 2 * wk1i * wk1r;
+        wk3r = wk1r - 2 * wk2i * wk1i;
+        wk3i = 2 * wk2i * wk1r - wk1i;
+        for (j = k; j < l + k; j += 2) {
+            j1 = j + l;
+            j2 = j1 + l;
+            j3 = j2 + l;
+            x0r = a[j] + a[j1];
+            x0i = a[j + 1] + a[j1 + 1];
+            x1r = a[j] - a[j1];
+            x1i = a[j + 1] - a[j1 + 1];
+            x2r = a[j2] + a[j3];
+            x2i = a[j2 + 1] + a[j3 + 1];
+            x3r = a[j2] - a[j3];
+            x3i = a[j2 + 1] - a[j3 + 1];
+            a[j] = x0r + x2r;
+            a[j + 1] = x0i + x2i;
+            x0r -= x2r;
+            x0i -= x2i;
+            a[j2] = wk2r * x0r - wk2i * x0i;
+            a[j2 + 1] = wk2r * x0i + wk2i * x0r;
+            x0r = x1r - x3i;
+            x0i = x1i + x3r;
+            a[j1] = wk1r * x0r - wk1i * x0i;
+            a[j1 + 1] = wk1r * x0i + wk1i * x0r;
+            x0r = x1r + x3i;
+            x0i = x1i - x3r;
+            a[j3] = wk3r * x0r - wk3i * x0i;
+            a[j3 + 1] = wk3r * x0i + wk3i * x0r;
+        }
+        x0r = wn4r * (wk1r - wk1i);
+        wk1i = wn4r * (wk1r + wk1i);
+        wk1r = x0r;
+        wk3r = wk1r - 2 * wk2r * wk1i;
+        wk3i = 2 * wk2r * wk1r - wk1i;
+        for (j = k + m; j < l + (k + m); j += 2) {
+            j1 = j + l;
+            j2 = j1 + l;
+            j3 = j2 + l;
+            x0r = a[j] + a[j1];
+            x0i = a[j + 1] + a[j1 + 1];
+            x1r = a[j] - a[j1];
+            x1i = a[j + 1] - a[j1 + 1];
+            x2r = a[j2] + a[j3];
+            x2i = a[j2 + 1] + a[j3 + 1];
+            x3r = a[j2] - a[j3];
+            x3i = a[j2 + 1] - a[j3 + 1];
+            a[j] = x0r + x2r;
+            a[j + 1] = x0i + x2i;
+            x0r -= x2r;
+            x0i -= x2i;
+            a[j2] = -wk2i * x0r - wk2r * x0i;
+            a[j2 + 1] = -wk2i * x0i + wk2r * x0r;
+            x0r = x1r - x3i;
+            x0i = x1i + x3r;
+            a[j1] = wk1r * x0r - wk1i * x0i;
+            a[j1 + 1] = wk1r * x0i + wk1i * x0r;
+            x0r = x1r + x3i;
+            x0i = x1i - x3r;
+            a[j3] = wk3r * x0r - wk3i * x0i;
+            a[j3 + 1] = wk3r * x0i + wk3i * x0r;
+        }
+    }
+}
+
+
+void rftfsub(int n, double *a)
+{
+    int i, i0, j, k;
+    double ec, w1r, w1i, wkr, wki, wdr, wdi, ss, xr, xi, yr, yi;
+    
+    ec = 2 * M_PI_2 / n;
+    wkr = 0;
+    wki = 0;
+    wdi = cos(ec);
+    wdr = sin(ec);
+    wdi *= wdr;
+    wdr *= wdr;
+    w1r = 1 - 2 * wdr;
+    w1i = 2 * wdi;
+    ss = 2 * w1i;
+    i = n >> 1;
+    for (;;) {
+        i0 = i - 4 * RDFT_LOOP_DIV;
+        if (i0 < 4) {
+            i0 = 4;
+        }
+        for (j = i - 4; j >= i0; j -= 4) {
+            k = n - j;
+            xr = a[j + 2] - a[k - 2];
+            xi = a[j + 3] + a[k - 1];
+            yr = wdr * xr - wdi * xi;
+            yi = wdr * xi + wdi * xr;
+            a[j + 2] -= yr;
+            a[j + 3] -= yi;
+            a[k - 2] += yr;
+            a[k - 1] -= yi;
+            wkr += ss * wdi;
+            wki += ss * (0.5 - wdr);
+            xr = a[j] - a[k];
+            xi = a[j + 1] + a[k + 1];
+            yr = wkr * xr - wki * xi;
+            yi = wkr * xi + wki * xr;
+            a[j] -= yr;
+            a[j + 1] -= yi;
+            a[k] += yr;
+            a[k + 1] -= yi;
+            wdr += ss * wki;
+            wdi += ss * (0.5 - wkr);
+        }
+        if (i0 == 4) {
+            break;
+        }
+        wkr = 0.5 * sin(ec * i0);
+        wki = 0.5 * cos(ec * i0);
+        wdr = 0.5 - (wkr * w1r - wki * w1i);
+        wdi = wkr * w1i + wki * w1r;
+        wkr = 0.5 - wkr;
+        i = i0;
+    }
+    xr = a[2] - a[n - 2];
+    xi = a[3] + a[n - 1];
+    yr = wdr * xr - wdi * xi;
+    yi = wdr * xi + wdi * xr;
+    a[2] -= yr;
+    a[3] -= yi;
+    a[n - 2] += yr;
+    a[n - 1] -= yi;
+}
+
+
+void rftbsub(int n, double *a)
+{
+    int i, i0, j, k;
+    double ec, w1r, w1i, wkr, wki, wdr, wdi, ss, xr, xi, yr, yi;
+    
+    ec = 2 * M_PI_2 / n;
+    wkr = 0;
+    wki = 0;
+    wdi = cos(ec);
+    wdr = sin(ec);
+    wdi *= wdr;
+    wdr *= wdr;
+    w1r = 1 - 2 * wdr;
+    w1i = 2 * wdi;
+    ss = 2 * w1i;
+    i = n >> 1;
+    a[i + 1] = -a[i + 1];
+    for (;;) {
+        i0 = i - 4 * RDFT_LOOP_DIV;
+        if (i0 < 4) {
+            i0 = 4;
+        }
+        for (j = i - 4; j >= i0; j -= 4) {
+            k = n - j;
+            xr = a[j + 2] - a[k - 2];
+            xi = a[j + 3] + a[k - 1];
+            yr = wdr * xr + wdi * xi;
+            yi = wdr * xi - wdi * xr;
+            a[j + 2] -= yr;
+            a[j + 3] = yi - a[j + 3];
+            a[k - 2] += yr;
+            a[k - 1] = yi - a[k - 1];
+            wkr += ss * wdi;
+            wki += ss * (0.5 - wdr);
+            xr = a[j] - a[k];
+            xi = a[j + 1] + a[k + 1];
+            yr = wkr * xr + wki * xi;
+            yi = wkr * xi - wki * xr;
+            a[j] -= yr;
+            a[j + 1] = yi - a[j + 1];
+            a[k] += yr;
+            a[k + 1] = yi - a[k + 1];
+            wdr += ss * wki;
+            wdi += ss * (0.5 - wkr);
+        }
+        if (i0 == 4) {
+            break;
+        }
+        wkr = 0.5 * sin(ec * i0);
+        wki = 0.5 * cos(ec * i0);
+        wdr = 0.5 - (wkr * w1r - wki * w1i);
+        wdi = wkr * w1i + wki * w1r;
+        wkr = 0.5 - wkr;
+        i = i0;
+    }
+    xr = a[2] - a[n - 2];
+    xi = a[3] + a[n - 1];
+    yr = wdr * xr + wdi * xi;
+    yi = wdr * xi - wdi * xr;
+    a[2] -= yr;
+    a[3] = yi - a[3];
+    a[n - 2] += yr;
+    a[n - 1] = yi - a[n - 1];
+    a[1] = -a[1];
+}
+
+
+void dctsub(int n, double *a)
+{
+    int i, i0, j, k, m;
+    double ec, w1r, w1i, wkr, wki, wdr, wdi, ss, xr, xi, yr, yi;
+    
+    ec = M_PI_2 / n;
+    wkr = 0.5;
+    wki = 0.5;
+    w1r = cos(ec);
+    w1i = sin(ec);
+    wdr = 0.5 * (w1r - w1i);
+    wdi = 0.5 * (w1r + w1i);
+    ss = 2 * w1i;
+    m = n >> 1;
+    i = 0;
+    for (;;) {
+        i0 = i + 2 * DCST_LOOP_DIV;
+        if (i0 > m - 2) {
+            i0 = m - 2;
+        }
+        for (j = i + 2; j <= i0; j += 2) {
+            k = n - j;
+            xr = wdi * a[j - 1] - wdr * a[k + 1];
+            xi = wdr * a[j - 1] + wdi * a[k + 1];
+            wkr -= ss * wdi;
+            wki += ss * wdr;
+            yr = wki * a[j] - wkr * a[k];
+            yi = wkr * a[j] + wki * a[k];
+            wdr -= ss * wki;
+            wdi += ss * wkr;
+            a[k + 1] = xr;
+            a[k] = yr;
+            a[j - 1] = xi;
+            a[j] = yi;
+        }
+        if (i0 == m - 2) {
+            break;
+        }
+        wdr = cos(ec * i0);
+        wdi = sin(ec * i0);
+        wkr = 0.5 * (wdr - wdi);
+        wki = 0.5 * (wdr + wdi);
+        wdr = wkr * w1r - wki * w1i;
+        wdi = wkr * w1i + wki * w1r;
+        i = i0;
+    }
+    xr = wdi * a[m - 1] - wdr * a[m + 1];
+    a[m - 1] = wdr * a[m - 1] + wdi * a[m + 1];
+    a[m + 1] = xr;
+    a[m] *= wki + ss * wdr;
+}
+
+
+void dstsub(int n, double *a)
+{
+    int i, i0, j, k, m;
+    double ec, w1r, w1i, wkr, wki, wdr, wdi, ss, xr, xi, yr, yi;
+    
+    ec = M_PI_2 / n;
+    wkr = 0.5;
+    wki = 0.5;
+    w1r = cos(ec);
+    w1i = sin(ec);
+    wdr = 0.5 * (w1r - w1i);
+    wdi = 0.5 * (w1r + w1i);
+    ss = 2 * w1i;
+    m = n >> 1;
+    i = 0;
+    for (;;) {
+        i0 = i + 2 * DCST_LOOP_DIV;
+        if (i0 > m - 2) {
+            i0 = m - 2;
+        }
+        for (j = i + 2; j <= i0; j += 2) {
+            k = n - j;
+            xr = wdi * a[k + 1] - wdr * a[j - 1];
+            xi = wdr * a[k + 1] + wdi * a[j - 1];
+            wkr -= ss * wdi;
+            wki += ss * wdr;
+            yr = wki * a[k] - wkr * a[j];
+            yi = wkr * a[k] + wki * a[j];
+            wdr -= ss * wki;
+            wdi += ss * wkr;
+            a[j - 1] = xr;
+            a[j] = yr;
+            a[k + 1] = xi;
+            a[k] = yi;
+        }
+        if (i0 == m - 2) {
+            break;
+        }
+        wdr = cos(ec * i0);
+        wdi = sin(ec * i0);
+        wkr = 0.5 * (wdr - wdi);
+        wki = 0.5 * (wdr + wdi);
+        wdr = wkr * w1r - wki * w1i;
+        wdi = wkr * w1i + wki * w1r;
+        i = i0;
+    }
+    xr = wdi * a[m + 1] - wdr * a[m - 1];
+    a[m + 1] = wdr * a[m + 1] + wdi * a[m - 1];
+    a[m - 1] = xr;
+    a[m] *= wki + ss * wdr;
+}
+
+
+void dctsub4(int n, double *a)
+{
+    int m;
+    double wki, wdr, wdi, xr;
+    
+    wki = WR5000;
+    m = n >> 1;
+    if (m == 2) {
+        wdr = wki * WI2500;
+        wdi = wki * WR2500;
+        xr = wdi * a[1] - wdr * a[3];
+        a[1] = wdr * a[1] + wdi * a[3];
+        a[3] = xr;
+    }
+    a[m] *= wki;
+}
+
+
+void dstsub4(int n, double *a)
+{
+    int m;
+    double wki, wdr, wdi, xr;
+    
+    wki = WR5000;
+    m = n >> 1;
+    if (m == 2) {
+        wdr = wki * WI2500;
+        wdi = wki * WR2500;
+        xr = wdi * a[3] - wdr * a[1];
+        a[3] = wdr * a[3] + wdi * a[1];
+        a[1] = xr;
+    }
+    a[m] *= wki;
+}
+
diff --git a/third_party/tflite-micro/third_party/fft2d/fft8g.c b/third_party/tflite-micro/third_party/fft2d/fft8g.c
new file mode 100644
index 00000000..99fcad58
--- /dev/null
+++ b/third_party/tflite-micro/third_party/fft2d/fft8g.c
@@ -0,0 +1,1642 @@
+/*
+Fast Fourier/Cosine/Sine Transform
+    dimension   :one
+    data length :power of 2
+    decimation  :frequency
+    radix       :8, 4, 2
+    data        :inplace
+    table       :use
+functions
+    cdft: Complex Discrete Fourier Transform
+    rdft: Real Discrete Fourier Transform
+    ddct: Discrete Cosine Transform
+    ddst: Discrete Sine Transform
+    dfct: Cosine Transform of RDFT (Real Symmetric DFT)
+    dfst: Sine Transform of RDFT (Real Anti-symmetric DFT)
+function prototypes
+    void cdft(int, int, double *, int *, double *);
+    void rdft(int, int, double *, int *, double *);
+    void ddct(int, int, double *, int *, double *);
+    void ddst(int, int, double *, int *, double *);
+    void dfct(int, double *, double *, int *, double *);
+    void dfst(int, double *, double *, int *, double *);
+
+
+-------- Complex DFT (Discrete Fourier Transform) --------
+    [definition]
+        <case1>
+            X[k] = sum_j=0^n-1 x[j]*exp(2*pi*i*j*k/n), 0<=k<n
+        <case2>
+            X[k] = sum_j=0^n-1 x[j]*exp(-2*pi*i*j*k/n), 0<=k<n
+        (notes: sum_j=0^n-1 is a summation from j=0 to n-1)
+    [usage]
+        <case1>
+            ip[0] = 0; // first time only
+            cdft(2*n, 1, a, ip, w);
+        <case2>
+            ip[0] = 0; // first time only
+            cdft(2*n, -1, a, ip, w);
+    [parameters]
+        2*n            :data length (int)
+                        n >= 1, n = power of 2
+        a[0...2*n-1]   :input/output data (double *)
+                        input data
+                            a[2*j] = Re(x[j]), 
+                            a[2*j+1] = Im(x[j]), 0<=j<n
+                        output data
+                            a[2*k] = Re(X[k]), 
+                            a[2*k+1] = Im(X[k]), 0<=k<n
+        ip[0...*]      :work area for bit reversal (int *)
+                        length of ip >= 2+sqrt(n)
+                        strictly, 
+                        length of ip >= 
+                            2+(1<<(int)(log(n+0.5)/log(2))/2).
+                        ip[0],ip[1] are pointers of the cos/sin table.
+        w[0...n/2-1]   :cos/sin table (double *)
+                        w[],ip[] are initialized if ip[0] == 0.
+    [remark]
+        Inverse of 
+            cdft(2*n, -1, a, ip, w);
+        is 
+            cdft(2*n, 1, a, ip, w);
+            for (j = 0; j <= 2 * n - 1; j++) {
+                a[j] *= 1.0 / n;
+            }
+        .
+
+
+-------- Real DFT / Inverse of Real DFT --------
+    [definition]
+        <case1> RDFT
+            R[k] = sum_j=0^n-1 a[j]*cos(2*pi*j*k/n), 0<=k<=n/2
+            I[k] = sum_j=0^n-1 a[j]*sin(2*pi*j*k/n), 0<k<n/2
+        <case2> IRDFT (excluding scale)
+            a[k] = (R[0] + R[n/2]*cos(pi*k))/2 + 
+                   sum_j=1^n/2-1 R[j]*cos(2*pi*j*k/n) + 
+                   sum_j=1^n/2-1 I[j]*sin(2*pi*j*k/n), 0<=k<n
+    [usage]
+        <case1>
+            ip[0] = 0; // first time only
+            rdft(n, 1, a, ip, w);
+        <case2>
+            ip[0] = 0; // first time only
+            rdft(n, -1, a, ip, w);
+    [parameters]
+        n              :data length (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        <case1>
+                            output data
+                                a[2*k] = R[k], 0<=k<n/2
+                                a[2*k+1] = I[k], 0<k<n/2
+                                a[1] = R[n/2]
+                        <case2>
+                            input data
+                                a[2*j] = R[j], 0<=j<n/2
+                                a[2*j+1] = I[j], 0<j<n/2
+                                a[1] = R[n/2]
+        ip[0...*]      :work area for bit reversal (int *)
+                        length of ip >= 2+sqrt(n/2)
+                        strictly, 
+                        length of ip >= 
+                            2+(1<<(int)(log(n/2+0.5)/log(2))/2).
+                        ip[0],ip[1] are pointers of the cos/sin table.
+        w[0...n/2-1]   :cos/sin table (double *)
+                        w[],ip[] are initialized if ip[0] == 0.
+    [remark]
+        Inverse of 
+            rdft(n, 1, a, ip, w);
+        is 
+            rdft(n, -1, a, ip, w);
+            for (j = 0; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- DCT (Discrete Cosine Transform) / Inverse of DCT --------
+    [definition]
+        <case1> IDCT (excluding scale)
+            C[k] = sum_j=0^n-1 a[j]*cos(pi*j*(k+1/2)/n), 0<=k<n
+        <case2> DCT
+            C[k] = sum_j=0^n-1 a[j]*cos(pi*(j+1/2)*k/n), 0<=k<n
+    [usage]
+        <case1>
+            ip[0] = 0; // first time only
+            ddct(n, 1, a, ip, w);
+        <case2>
+            ip[0] = 0; // first time only
+            ddct(n, -1, a, ip, w);
+    [parameters]
+        n              :data length (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        output data
+                            a[k] = C[k], 0<=k<n
+        ip[0...*]      :work area for bit reversal (int *)
+                        length of ip >= 2+sqrt(n/2)
+                        strictly, 
+                        length of ip >= 
+                            2+(1<<(int)(log(n/2+0.5)/log(2))/2).
+                        ip[0],ip[1] are pointers of the cos/sin table.
+        w[0...n*5/4-1] :cos/sin table (double *)
+                        w[],ip[] are initialized if ip[0] == 0.
+    [remark]
+        Inverse of 
+            ddct(n, -1, a, ip, w);
+        is 
+            a[0] *= 0.5;
+            ddct(n, 1, a, ip, w);
+            for (j = 0; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- DST (Discrete Sine Transform) / Inverse of DST --------
+    [definition]
+        <case1> IDST (excluding scale)
+            S[k] = sum_j=1^n A[j]*sin(pi*j*(k+1/2)/n), 0<=k<n
+        <case2> DST
+            S[k] = sum_j=0^n-1 a[j]*sin(pi*(j+1/2)*k/n), 0<k<=n
+    [usage]
+        <case1>
+            ip[0] = 0; // first time only
+            ddst(n, 1, a, ip, w);
+        <case2>
+            ip[0] = 0; // first time only
+            ddst(n, -1, a, ip, w);
+    [parameters]
+        n              :data length (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        <case1>
+                            input data
+                                a[j] = A[j], 0<j<n
+                                a[0] = A[n]
+                            output data
+                                a[k] = S[k], 0<=k<n
+                        <case2>
+                            output data
+                                a[k] = S[k], 0<k<n
+                                a[0] = S[n]
+        ip[0...*]      :work area for bit reversal (int *)
+                        length of ip >= 2+sqrt(n/2)
+                        strictly, 
+                        length of ip >= 
+                            2+(1<<(int)(log(n/2+0.5)/log(2))/2).
+                        ip[0],ip[1] are pointers of the cos/sin table.
+        w[0...n*5/4-1] :cos/sin table (double *)
+                        w[],ip[] are initialized if ip[0] == 0.
+    [remark]
+        Inverse of 
+            ddst(n, -1, a, ip, w);
+        is 
+            a[0] *= 0.5;
+            ddst(n, 1, a, ip, w);
+            for (j = 0; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- Cosine Transform of RDFT (Real Symmetric DFT) --------
+    [definition]
+        C[k] = sum_j=0^n a[j]*cos(pi*j*k/n), 0<=k<=n
+    [usage]
+        ip[0] = 0; // first time only
+        dfct(n, a, t, ip, w);
+    [parameters]
+        n              :data length - 1 (int)
+                        n >= 2, n = power of 2
+        a[0...n]       :input/output data (double *)
+                        output data
+                            a[k] = C[k], 0<=k<=n
+        t[0...n/2]     :work area (double *)
+        ip[0...*]      :work area for bit reversal (int *)
+                        length of ip >= 2+sqrt(n/4)
+                        strictly, 
+                        length of ip >= 
+                            2+(1<<(int)(log(n/4+0.5)/log(2))/2).
+                        ip[0],ip[1] are pointers of the cos/sin table.
+        w[0...n*5/8-1] :cos/sin table (double *)
+                        w[],ip[] are initialized if ip[0] == 0.
+    [remark]
+        Inverse of 
+            a[0] *= 0.5;
+            a[n] *= 0.5;
+            dfct(n, a, t, ip, w);
+        is 
+            a[0] *= 0.5;
+            a[n] *= 0.5;
+            dfct(n, a, t, ip, w);
+            for (j = 0; j <= n; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- Sine Transform of RDFT (Real Anti-symmetric DFT) --------
+    [definition]
+        S[k] = sum_j=1^n-1 a[j]*sin(pi*j*k/n), 0<k<n
+    [usage]
+        ip[0] = 0; // first time only
+        dfst(n, a, t, ip, w);
+    [parameters]
+        n              :data length + 1 (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        output data
+                            a[k] = S[k], 0<k<n
+                        (a[0] is used for work area)
+        t[0...n/2-1]   :work area (double *)
+        ip[0...*]      :work area for bit reversal (int *)
+                        length of ip >= 2+sqrt(n/4)
+                        strictly, 
+                        length of ip >= 
+                            2+(1<<(int)(log(n/4+0.5)/log(2))/2).
+                        ip[0],ip[1] are pointers of the cos/sin table.
+        w[0...n*5/8-1] :cos/sin table (double *)
+                        w[],ip[] are initialized if ip[0] == 0.
+    [remark]
+        Inverse of 
+            dfst(n, a, t, ip, w);
+        is 
+            dfst(n, a, t, ip, w);
+            for (j = 1; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+Appendix :
+    The cos/sin table is recalculated when the larger table required.
+    w[] and ip[] are compatible with all routines.
+*/
+
+
+void cdft(int n, int isgn, double *a, int *ip, double *w)
+{
+    void makewt(int nw, int *ip, double *w);
+    void bitrv2(int n, int *ip, double *a);
+    void bitrv2conj(int n, int *ip, double *a);
+    void cftfsub(int n, double *a, double *w);
+    void cftbsub(int n, double *a, double *w);
+    
+    if (n > (ip[0] << 2)) {
+        makewt(n >> 2, ip, w);
+    }
+    if (n > 4) {
+        if (isgn >= 0) {
+            bitrv2(n, ip + 2, a);
+            cftfsub(n, a, w);
+        } else {
+            bitrv2conj(n, ip + 2, a);
+            cftbsub(n, a, w);
+        }
+    } else if (n == 4) {
+        cftfsub(n, a, w);
+    }
+}
+
+
+void rdft(int n, int isgn, double *a, int *ip, double *w)
+{
+    void makewt(int nw, int *ip, double *w);
+    void makect(int nc, int *ip, double *c);
+    void bitrv2(int n, int *ip, double *a);
+    void cftfsub(int n, double *a, double *w);
+    void cftbsub(int n, double *a, double *w);
+    void rftfsub(int n, double *a, int nc, double *c);
+    void rftbsub(int n, double *a, int nc, double *c);
+    int nw, nc;
+    double xi;
+    
+    nw = ip[0];
+    if (n > (nw << 2)) {
+        nw = n >> 2;
+        makewt(nw, ip, w);
+    }
+    nc = ip[1];
+    if (n > (nc << 2)) {
+        nc = n >> 2;
+        makect(nc, ip, w + nw);
+    }
+    if (isgn >= 0) {
+        if (n > 4) {
+            bitrv2(n, ip + 2, a);
+            cftfsub(n, a, w);
+            rftfsub(n, a, nc, w + nw);
+        } else if (n == 4) {
+            cftfsub(n, a, w);
+        }
+        xi = a[0] - a[1];
+        a[0] += a[1];
+        a[1] = xi;
+    } else {
+        a[1] = 0.5 * (a[0] - a[1]);
+        a[0] -= a[1];
+        if (n > 4) {
+            rftbsub(n, a, nc, w + nw);
+            bitrv2(n, ip + 2, a);
+            cftbsub(n, a, w);
+        } else if (n == 4) {
+            cftfsub(n, a, w);
+        }
+    }
+}
+
+
+void ddct(int n, int isgn, double *a, int *ip, double *w)
+{
+    void makewt(int nw, int *ip, double *w);
+    void makect(int nc, int *ip, double *c);
+    void bitrv2(int n, int *ip, double *a);
+    void cftfsub(int n, double *a, double *w);
+    void cftbsub(int n, double *a, double *w);
+    void rftfsub(int n, double *a, int nc, double *c);
+    void rftbsub(int n, double *a, int nc, double *c);
+    void dctsub(int n, double *a, int nc, double *c);
+    int j, nw, nc;
+    double xr;
+    
+    nw = ip[0];
+    if (n > (nw << 2)) {
+        nw = n >> 2;
+        makewt(nw, ip, w);
+    }
+    nc = ip[1];
+    if (n > nc) {
+        nc = n;
+        makect(nc, ip, w + nw);
+    }
+    if (isgn < 0) {
+        xr = a[n - 1];
+        for (j = n - 2; j >= 2; j -= 2) {
+            a[j + 1] = a[j] - a[j - 1];
+            a[j] += a[j - 1];
+        }
+        a[1] = a[0] - xr;
+        a[0] += xr;
+        if (n > 4) {
+            rftbsub(n, a, nc, w + nw);
+            bitrv2(n, ip + 2, a);
+            cftbsub(n, a, w);
+        } else if (n == 4) {
+            cftfsub(n, a, w);
+        }
+    }
+    dctsub(n, a, nc, w + nw);
+    if (isgn >= 0) {
+        if (n > 4) {
+            bitrv2(n, ip + 2, a);
+            cftfsub(n, a, w);
+            rftfsub(n, a, nc, w + nw);
+        } else if (n == 4) {
+            cftfsub(n, a, w);
+        }
+        xr = a[0] - a[1];
+        a[0] += a[1];
+        for (j = 2; j < n; j += 2) {
+            a[j - 1] = a[j] - a[j + 1];
+            a[j] += a[j + 1];
+        }
+        a[n - 1] = xr;
+    }
+}
+
+
+void ddst(int n, int isgn, double *a, int *ip, double *w)
+{
+    void makewt(int nw, int *ip, double *w);
+    void makect(int nc, int *ip, double *c);
+    void bitrv2(int n, int *ip, double *a);
+    void cftfsub(int n, double *a, double *w);
+    void cftbsub(int n, double *a, double *w);
+    void rftfsub(int n, double *a, int nc, double *c);
+    void rftbsub(int n, double *a, int nc, double *c);
+    void dstsub(int n, double *a, int nc, double *c);
+    int j, nw, nc;
+    double xr;
+    
+    nw = ip[0];
+    if (n > (nw << 2)) {
+        nw = n >> 2;
+        makewt(nw, ip, w);
+    }
+    nc = ip[1];
+    if (n > nc) {
+        nc = n;
+        makect(nc, ip, w + nw);
+    }
+    if (isgn < 0) {
+        xr = a[n - 1];
+        for (j = n - 2; j >= 2; j -= 2) {
+            a[j + 1] = -a[j] - a[j - 1];
+            a[j] -= a[j - 1];
+        }
+        a[1] = a[0] + xr;
+        a[0] -= xr;
+        if (n > 4) {
+            rftbsub(n, a, nc, w + nw);
+            bitrv2(n, ip + 2, a);
+            cftbsub(n, a, w);
+        } else if (n == 4) {
+            cftfsub(n, a, w);
+        }
+    }
+    dstsub(n, a, nc, w + nw);
+    if (isgn >= 0) {
+        if (n > 4) {
+            bitrv2(n, ip + 2, a);
+            cftfsub(n, a, w);
+            rftfsub(n, a, nc, w + nw);
+        } else if (n == 4) {
+            cftfsub(n, a, w);
+        }
+        xr = a[0] - a[1];
+        a[0] += a[1];
+        for (j = 2; j < n; j += 2) {
+            a[j - 1] = -a[j] - a[j + 1];
+            a[j] -= a[j + 1];
+        }
+        a[n - 1] = -xr;
+    }
+}
+
+
+void dfct(int n, double *a, double *t, int *ip, double *w)
+{
+    void makewt(int nw, int *ip, double *w);
+    void makect(int nc, int *ip, double *c);
+    void bitrv2(int n, int *ip, double *a);
+    void cftfsub(int n, double *a, double *w);
+    void rftfsub(int n, double *a, int nc, double *c);
+    void dctsub(int n, double *a, int nc, double *c);
+    int j, k, l, m, mh, nw, nc;
+    double xr, xi, yr, yi;
+    
+    nw = ip[0];
+    if (n > (nw << 3)) {
+        nw = n >> 3;
+        makewt(nw, ip, w);
+    }
+    nc = ip[1];
+    if (n > (nc << 1)) {
+        nc = n >> 1;
+        makect(nc, ip, w + nw);
+    }
+    m = n >> 1;
+    yi = a[m];
+    xi = a[0] + a[n];
+    a[0] -= a[n];
+    t[0] = xi - yi;
+    t[m] = xi + yi;
+    if (n > 2) {
+        mh = m >> 1;
+        for (j = 1; j < mh; j++) {
+            k = m - j;
+            xr = a[j] - a[n - j];
+            xi = a[j] + a[n - j];
+            yr = a[k] - a[n - k];
+            yi = a[k] + a[n - k];
+            a[j] = xr;
+            a[k] = yr;
+            t[j] = xi - yi;
+            t[k] = xi + yi;
+        }
+        t[mh] = a[mh] + a[n - mh];
+        a[mh] -= a[n - mh];
+        dctsub(m, a, nc, w + nw);
+        if (m > 4) {
+            bitrv2(m, ip + 2, a);
+            cftfsub(m, a, w);
+            rftfsub(m, a, nc, w + nw);
+        } else if (m == 4) {
+            cftfsub(m, a, w);
+        }
+        a[n - 1] = a[0] - a[1];
+        a[1] = a[0] + a[1];
+        for (j = m - 2; j >= 2; j -= 2) {
+            a[2 * j + 1] = a[j] + a[j + 1];
+            a[2 * j - 1] = a[j] - a[j + 1];
+        }
+        l = 2;
+        m = mh;
+        while (m >= 2) {
+            dctsub(m, t, nc, w + nw);
+            if (m > 4) {
+                bitrv2(m, ip + 2, t);
+                cftfsub(m, t, w);
+                rftfsub(m, t, nc, w + nw);
+            } else if (m == 4) {
+                cftfsub(m, t, w);
+            }
+            a[n - l] = t[0] - t[1];
+            a[l] = t[0] + t[1];
+            k = 0;
+            for (j = 2; j < m; j += 2) {
+                k += l << 2;
+                a[k - l] = t[j] - t[j + 1];
+                a[k + l] = t[j] + t[j + 1];
+            }
+            l <<= 1;
+            mh = m >> 1;
+            for (j = 0; j < mh; j++) {
+                k = m - j;
+                t[j] = t[m + k] - t[m + j];
+                t[k] = t[m + k] + t[m + j];
+            }
+            t[mh] = t[m + mh];
+            m = mh;
+        }
+        a[l] = t[0];
+        a[n] = t[2] - t[1];
+        a[0] = t[2] + t[1];
+    } else {
+        a[1] = a[0];
+        a[2] = t[0];
+        a[0] = t[1];
+    }
+}
+
+
+void dfst(int n, double *a, double *t, int *ip, double *w)
+{
+    void makewt(int nw, int *ip, double *w);
+    void makect(int nc, int *ip, double *c);
+    void bitrv2(int n, int *ip, double *a);
+    void cftfsub(int n, double *a, double *w);
+    void rftfsub(int n, double *a, int nc, double *c);
+    void dstsub(int n, double *a, int nc, double *c);
+    int j, k, l, m, mh, nw, nc;
+    double xr, xi, yr, yi;
+    
+    nw = ip[0];
+    if (n > (nw << 3)) {
+        nw = n >> 3;
+        makewt(nw, ip, w);
+    }
+    nc = ip[1];
+    if (n > (nc << 1)) {
+        nc = n >> 1;
+        makect(nc, ip, w + nw);
+    }
+    if (n > 2) {
+        m = n >> 1;
+        mh = m >> 1;
+        for (j = 1; j < mh; j++) {
+            k = m - j;
+            xr = a[j] + a[n - j];
+            xi = a[j] - a[n - j];
+            yr = a[k] + a[n - k];
+            yi = a[k] - a[n - k];
+            a[j] = xr;
+            a[k] = yr;
+            t[j] = xi + yi;
+            t[k] = xi - yi;
+        }
+        t[0] = a[mh] - a[n - mh];
+        a[mh] += a[n - mh];
+        a[0] = a[m];
+        dstsub(m, a, nc, w + nw);
+        if (m > 4) {
+            bitrv2(m, ip + 2, a);
+            cftfsub(m, a, w);
+            rftfsub(m, a, nc, w + nw);
+        } else if (m == 4) {
+            cftfsub(m, a, w);
+        }
+        a[n - 1] = a[1] - a[0];
+        a[1] = a[0] + a[1];
+        for (j = m - 2; j >= 2; j -= 2) {
+            a[2 * j + 1] = a[j] - a[j + 1];
+            a[2 * j - 1] = -a[j] - a[j + 1];
+        }
+        l = 2;
+        m = mh;
+        while (m >= 2) {
+            dstsub(m, t, nc, w + nw);
+            if (m > 4) {
+                bitrv2(m, ip + 2, t);
+                cftfsub(m, t, w);
+                rftfsub(m, t, nc, w + nw);
+            } else if (m == 4) {
+                cftfsub(m, t, w);
+            }
+            a[n - l] = t[1] - t[0];
+            a[l] = t[0] + t[1];
+            k = 0;
+            for (j = 2; j < m; j += 2) {
+                k += l << 2;
+                a[k - l] = -t[j] - t[j + 1];
+                a[k + l] = t[j] - t[j + 1];
+            }
+            l <<= 1;
+            mh = m >> 1;
+            for (j = 1; j < mh; j++) {
+                k = m - j;
+                t[j] = t[m + k] + t[m + j];
+                t[k] = t[m + k] - t[m + j];
+            }
+            t[0] = t[m + mh];
+            m = mh;
+        }
+        a[l] = t[0];
+    }
+    a[0] = 0;
+}
+
+
+/* -------- initializing routines -------- */
+
+
+#include <math.h>
+
+void makewt(int nw, int *ip, double *w)
+{
+    void bitrv2(int n, int *ip, double *a);
+    int j, nwh;
+    double delta, x, y;
+    
+    ip[0] = nw;
+    ip[1] = 1;
+    if (nw > 2) {
+        nwh = nw >> 1;
+        delta = atan(1.0) / nwh;
+        w[0] = 1;
+        w[1] = 0;
+        w[nwh] = cos(delta * nwh);
+        w[nwh + 1] = w[nwh];
+        if (nwh > 2) {
+            for (j = 2; j < nwh; j += 2) {
+                x = cos(delta * j);
+                y = sin(delta * j);
+                w[j] = x;
+                w[j + 1] = y;
+                w[nw - j] = y;
+                w[nw - j + 1] = x;
+            }
+            for (j = nwh - 2; j >= 2; j -= 2) {
+                x = w[2 * j];
+                y = w[2 * j + 1];
+                w[nwh + j] = x;
+                w[nwh + j + 1] = y;
+            }
+            bitrv2(nw, ip + 2, w);
+        }
+    }
+}
+
+
+void makect(int nc, int *ip, double *c)
+{
+    int j, nch;
+    double delta;
+    
+    ip[1] = nc;
+    if (nc > 1) {
+        nch = nc >> 1;
+        delta = atan(1.0) / nch;
+        c[0] = cos(delta * nch);
+        c[nch] = 0.5 * c[0];
+        for (j = 1; j < nch; j++) {
+            c[j] = 0.5 * cos(delta * j);
+            c[nc - j] = 0.5 * sin(delta * j);
+        }
+    }
+}
+
+
+/* -------- child routines -------- */
+
+
+void bitrv2(int n, int *ip, double *a)
+{
+    int j, j1, k, k1, l, m, m2;
+    double xr, xi, yr, yi;
+    
+    ip[0] = 0;
+    l = n;
+    m = 1;
+    while ((m << 3) < l) {
+        l >>= 1;
+        for (j = 0; j < m; j++) {
+            ip[m + j] = ip[j] + l;
+        }
+        m <<= 1;
+    }
+    m2 = 2 * m;
+    if ((m << 3) == l) {
+        for (k = 0; k < m; k++) {
+            for (j = 0; j < k; j++) {
+                j1 = 2 * j + ip[k];
+                k1 = 2 * k + ip[j];
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m2;
+                k1 += 2 * m2;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m2;
+                k1 -= m2;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m2;
+                k1 += 2 * m2;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+            }
+            j1 = 2 * k + m2 + ip[k];
+            k1 = j1 + m2;
+            xr = a[j1];
+            xi = a[j1 + 1];
+            yr = a[k1];
+            yi = a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+        }
+    } else {
+        for (k = 1; k < m; k++) {
+            for (j = 0; j < k; j++) {
+                j1 = 2 * j + ip[k];
+                k1 = 2 * k + ip[j];
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m2;
+                k1 += m2;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+            }
+        }
+    }
+}
+
+
+void bitrv2conj(int n, int *ip, double *a)
+{
+    int j, j1, k, k1, l, m, m2;
+    double xr, xi, yr, yi;
+    
+    ip[0] = 0;
+    l = n;
+    m = 1;
+    while ((m << 3) < l) {
+        l >>= 1;
+        for (j = 0; j < m; j++) {
+            ip[m + j] = ip[j] + l;
+        }
+        m <<= 1;
+    }
+    m2 = 2 * m;
+    if ((m << 3) == l) {
+        for (k = 0; k < m; k++) {
+            for (j = 0; j < k; j++) {
+                j1 = 2 * j + ip[k];
+                k1 = 2 * k + ip[j];
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m2;
+                k1 += 2 * m2;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m2;
+                k1 -= m2;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m2;
+                k1 += 2 * m2;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+            }
+            k1 = 2 * k + ip[k];
+            a[k1 + 1] = -a[k1 + 1];
+            j1 = k1 + m2;
+            k1 = j1 + m2;
+            xr = a[j1];
+            xi = -a[j1 + 1];
+            yr = a[k1];
+            yi = -a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            k1 += m2;
+            a[k1 + 1] = -a[k1 + 1];
+        }
+    } else {
+        a[1] = -a[1];
+        a[m2 + 1] = -a[m2 + 1];
+        for (k = 1; k < m; k++) {
+            for (j = 0; j < k; j++) {
+                j1 = 2 * j + ip[k];
+                k1 = 2 * k + ip[j];
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m2;
+                k1 += m2;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+            }
+            k1 = 2 * k + ip[k];
+            a[k1 + 1] = -a[k1 + 1];
+            a[k1 + m2 + 1] = -a[k1 + m2 + 1];
+        }
+    }
+}
+
+
+void cftfsub(int n, double *a, double *w)
+{
+    void cft1st(int n, double *a, double *w);
+    void cftmdl(int n, int l, double *a, double *w);
+    int j, j1, j2, j3, l;
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i;
+    
+    l = 2;
+    if (n >= 16) {
+        cft1st(n, a, w);
+        l = 16;
+        while ((l << 3) <= n) {
+            cftmdl(n, l, a, w);
+            l <<= 3;
+        }
+    }
+    if ((l << 1) < n) {
+        for (j = 0; j < l; j += 2) {
+            j1 = j + l;
+            j2 = j1 + l;
+            j3 = j2 + l;
+            x0r = a[j] + a[j1];
+            x0i = a[j + 1] + a[j1 + 1];
+            x1r = a[j] - a[j1];
+            x1i = a[j + 1] - a[j1 + 1];
+            x2r = a[j2] + a[j3];
+            x2i = a[j2 + 1] + a[j3 + 1];
+            x3r = a[j2] - a[j3];
+            x3i = a[j2 + 1] - a[j3 + 1];
+            a[j] = x0r + x2r;
+            a[j + 1] = x0i + x2i;
+            a[j2] = x0r - x2r;
+            a[j2 + 1] = x0i - x2i;
+            a[j1] = x1r - x3i;
+            a[j1 + 1] = x1i + x3r;
+            a[j3] = x1r + x3i;
+            a[j3 + 1] = x1i - x3r;
+        }
+    } else if ((l << 1) == n) {
+        for (j = 0; j < l; j += 2) {
+            j1 = j + l;
+            x0r = a[j] - a[j1];
+            x0i = a[j + 1] - a[j1 + 1];
+            a[j] += a[j1];
+            a[j + 1] += a[j1 + 1];
+            a[j1] = x0r;
+            a[j1 + 1] = x0i;
+        }
+    }
+}
+
+
+void cftbsub(int n, double *a, double *w)
+{
+    void cft1st(int n, double *a, double *w);
+    void cftmdl(int n, int l, double *a, double *w);
+    int j, j1, j2, j3, j4, j5, j6, j7, l;
+    double wn4r, x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i, 
+        y0r, y0i, y1r, y1i, y2r, y2i, y3r, y3i, 
+        y4r, y4i, y5r, y5i, y6r, y6i, y7r, y7i;
+    
+    l = 2;
+    if (n > 16) {
+        cft1st(n, a, w);
+        l = 16;
+        while ((l << 3) < n) {
+            cftmdl(n, l, a, w);
+            l <<= 3;
+        }
+    }
+    if ((l << 2) < n) {
+        wn4r = w[2];
+        for (j = 0; j < l; j += 2) {
+            j1 = j + l;
+            j2 = j1 + l;
+            j3 = j2 + l;
+            j4 = j3 + l;
+            j5 = j4 + l;
+            j6 = j5 + l;
+            j7 = j6 + l;
+            x0r = a[j] + a[j1];
+            x0i = -a[j + 1] - a[j1 + 1];
+            x1r = a[j] - a[j1];
+            x1i = -a[j + 1] + a[j1 + 1];
+            x2r = a[j2] + a[j3];
+            x2i = a[j2 + 1] + a[j3 + 1];
+            x3r = a[j2] - a[j3];
+            x3i = a[j2 + 1] - a[j3 + 1];
+            y0r = x0r + x2r;
+            y0i = x0i - x2i;
+            y2r = x0r - x2r;
+            y2i = x0i + x2i;
+            y1r = x1r - x3i;
+            y1i = x1i - x3r;
+            y3r = x1r + x3i;
+            y3i = x1i + x3r;
+            x0r = a[j4] + a[j5];
+            x0i = a[j4 + 1] + a[j5 + 1];
+            x1r = a[j4] - a[j5];
+            x1i = a[j4 + 1] - a[j5 + 1];
+            x2r = a[j6] + a[j7];
+            x2i = a[j6 + 1] + a[j7 + 1];
+            x3r = a[j6] - a[j7];
+            x3i = a[j6 + 1] - a[j7 + 1];
+            y4r = x0r + x2r;
+            y4i = x0i + x2i;
+            y6r = x0r - x2r;
+            y6i = x0i - x2i;
+            x0r = x1r - x3i;
+            x0i = x1i + x3r;
+            x2r = x1r + x3i;
+            x2i = x1i - x3r;
+            y5r = wn4r * (x0r - x0i);
+            y5i = wn4r * (x0r + x0i);
+            y7r = wn4r * (x2r - x2i);
+            y7i = wn4r * (x2r + x2i);
+            a[j1] = y1r + y5r;
+            a[j1 + 1] = y1i - y5i;
+            a[j5] = y1r - y5r;
+            a[j5 + 1] = y1i + y5i;
+            a[j3] = y3r - y7i;
+            a[j3 + 1] = y3i - y7r;
+            a[j7] = y3r + y7i;
+            a[j7 + 1] = y3i + y7r;
+            a[j] = y0r + y4r;
+            a[j + 1] = y0i - y4i;
+            a[j4] = y0r - y4r;
+            a[j4 + 1] = y0i + y4i;
+            a[j2] = y2r - y6i;
+            a[j2 + 1] = y2i - y6r;
+            a[j6] = y2r + y6i;
+            a[j6 + 1] = y2i + y6r;
+        }
+    } else if ((l << 2) == n) {
+        for (j = 0; j < l; j += 2) {
+            j1 = j + l;
+            j2 = j1 + l;
+            j3 = j2 + l;
+            x0r = a[j] + a[j1];
+            x0i = -a[j + 1] - a[j1 + 1];
+            x1r = a[j] - a[j1];
+            x1i = -a[j + 1] + a[j1 + 1];
+            x2r = a[j2] + a[j3];
+            x2i = a[j2 + 1] + a[j3 + 1];
+            x3r = a[j2] - a[j3];
+            x3i = a[j2 + 1] - a[j3 + 1];
+            a[j] = x0r + x2r;
+            a[j + 1] = x0i - x2i;
+            a[j2] = x0r - x2r;
+            a[j2 + 1] = x0i + x2i;
+            a[j1] = x1r - x3i;
+            a[j1 + 1] = x1i - x3r;
+            a[j3] = x1r + x3i;
+            a[j3 + 1] = x1i + x3r;
+        }
+    } else {
+        for (j = 0; j < l; j += 2) {
+            j1 = j + l;
+            x0r = a[j] - a[j1];
+            x0i = -a[j + 1] + a[j1 + 1];
+            a[j] += a[j1];
+            a[j + 1] = -a[j + 1] - a[j1 + 1];
+            a[j1] = x0r;
+            a[j1 + 1] = x0i;
+        }
+    }
+}
+
+
+void cft1st(int n, double *a, double *w)
+{
+    int j, k1;
+    double wn4r, wtmp, wk1r, wk1i, wk2r, wk2i, wk3r, wk3i, 
+        wk4r, wk4i, wk5r, wk5i, wk6r, wk6i, wk7r, wk7i;
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i, 
+        y0r, y0i, y1r, y1i, y2r, y2i, y3r, y3i, 
+        y4r, y4i, y5r, y5i, y6r, y6i, y7r, y7i;
+    
+    wn4r = w[2];
+    x0r = a[0] + a[2];
+    x0i = a[1] + a[3];
+    x1r = a[0] - a[2];
+    x1i = a[1] - a[3];
+    x2r = a[4] + a[6];
+    x2i = a[5] + a[7];
+    x3r = a[4] - a[6];
+    x3i = a[5] - a[7];
+    y0r = x0r + x2r;
+    y0i = x0i + x2i;
+    y2r = x0r - x2r;
+    y2i = x0i - x2i;
+    y1r = x1r - x3i;
+    y1i = x1i + x3r;
+    y3r = x1r + x3i;
+    y3i = x1i - x3r;
+    x0r = a[8] + a[10];
+    x0i = a[9] + a[11];
+    x1r = a[8] - a[10];
+    x1i = a[9] - a[11];
+    x2r = a[12] + a[14];
+    x2i = a[13] + a[15];
+    x3r = a[12] - a[14];
+    x3i = a[13] - a[15];
+    y4r = x0r + x2r;
+    y4i = x0i + x2i;
+    y6r = x0r - x2r;
+    y6i = x0i - x2i;
+    x0r = x1r - x3i;
+    x0i = x1i + x3r;
+    x2r = x1r + x3i;
+    x2i = x1i - x3r;
+    y5r = wn4r * (x0r - x0i);
+    y5i = wn4r * (x0r + x0i);
+    y7r = wn4r * (x2r - x2i);
+    y7i = wn4r * (x2r + x2i);
+    a[2] = y1r + y5r;
+    a[3] = y1i + y5i;
+    a[10] = y1r - y5r;
+    a[11] = y1i - y5i;
+    a[6] = y3r - y7i;
+    a[7] = y3i + y7r;
+    a[14] = y3r + y7i;
+    a[15] = y3i - y7r;
+    a[0] = y0r + y4r;
+    a[1] = y0i + y4i;
+    a[8] = y0r - y4r;
+    a[9] = y0i - y4i;
+    a[4] = y2r - y6i;
+    a[5] = y2i + y6r;
+    a[12] = y2r + y6i;
+    a[13] = y2i - y6r;
+    if (n > 16) {
+        wk1r = w[4];
+        wk1i = w[5];
+        x0r = a[16] + a[18];
+        x0i = a[17] + a[19];
+        x1r = a[16] - a[18];
+        x1i = a[17] - a[19];
+        x2r = a[20] + a[22];
+        x2i = a[21] + a[23];
+        x3r = a[20] - a[22];
+        x3i = a[21] - a[23];
+        y0r = x0r + x2r;
+        y0i = x0i + x2i;
+        y2r = x0r - x2r;
+        y2i = x0i - x2i;
+        y1r = x1r - x3i;
+        y1i = x1i + x3r;
+        y3r = x1r + x3i;
+        y3i = x1i - x3r;
+        x0r = a[24] + a[26];
+        x0i = a[25] + a[27];
+        x1r = a[24] - a[26];
+        x1i = a[25] - a[27];
+        x2r = a[28] + a[30];
+        x2i = a[29] + a[31];
+        x3r = a[28] - a[30];
+        x3i = a[29] - a[31];
+        y4r = x0r + x2r;
+        y4i = x0i + x2i;
+        y6r = x0r - x2r;
+        y6i = x0i - x2i;
+        x0r = x1r - x3i;
+        x0i = x1i + x3r;
+        x2r = x1r + x3i;
+        x2i = x3r - x1i;
+        y5r = wk1i * x0r - wk1r * x0i;
+        y5i = wk1i * x0i + wk1r * x0r;
+        y7r = wk1r * x2r + wk1i * x2i;
+        y7i = wk1r * x2i - wk1i * x2r;
+        x0r = wk1r * y1r - wk1i * y1i;
+        x0i = wk1r * y1i + wk1i * y1r;
+        a[18] = x0r + y5r;
+        a[19] = x0i + y5i;
+        a[26] = y5i - x0i;
+        a[27] = x0r - y5r;
+        x0r = wk1i * y3r - wk1r * y3i;
+        x0i = wk1i * y3i + wk1r * y3r;
+        a[22] = x0r - y7r;
+        a[23] = x0i + y7i;
+        a[30] = y7i - x0i;
+        a[31] = x0r + y7r;
+        a[16] = y0r + y4r;
+        a[17] = y0i + y4i;
+        a[24] = y4i - y0i;
+        a[25] = y0r - y4r;
+        x0r = y2r - y6i;
+        x0i = y2i + y6r;
+        a[20] = wn4r * (x0r - x0i);
+        a[21] = wn4r * (x0i + x0r);
+        x0r = y6r - y2i;
+        x0i = y2r + y6i;
+        a[28] = wn4r * (x0r - x0i);
+        a[29] = wn4r * (x0i + x0r);
+        k1 = 4;
+        for (j = 32; j < n; j += 16) {
+            k1 += 4;
+            wk1r = w[k1];
+            wk1i = w[k1 + 1];
+            wk2r = w[k1 + 2];
+            wk2i = w[k1 + 3];
+            wtmp = 2 * wk2i;
+            wk3r = wk1r - wtmp * wk1i;
+            wk3i = wtmp * wk1r - wk1i;
+            wk4r = 1 - wtmp * wk2i;
+            wk4i = wtmp * wk2r;
+            wtmp = 2 * wk4i;
+            wk5r = wk3r - wtmp * wk1i;
+            wk5i = wtmp * wk1r - wk3i;
+            wk6r = wk2r - wtmp * wk2i;
+            wk6i = wtmp * wk2r - wk2i;
+            wk7r = wk1r - wtmp * wk3i;
+            wk7i = wtmp * wk3r - wk1i;
+            x0r = a[j] + a[j + 2];
+            x0i = a[j + 1] + a[j + 3];
+            x1r = a[j] - a[j + 2];
+            x1i = a[j + 1] - a[j + 3];
+            x2r = a[j + 4] + a[j + 6];
+            x2i = a[j + 5] + a[j + 7];
+            x3r = a[j + 4] - a[j + 6];
+            x3i = a[j + 5] - a[j + 7];
+            y0r = x0r + x2r;
+            y0i = x0i + x2i;
+            y2r = x0r - x2r;
+            y2i = x0i - x2i;
+            y1r = x1r - x3i;
+            y1i = x1i + x3r;
+            y3r = x1r + x3i;
+            y3i = x1i - x3r;
+            x0r = a[j + 8] + a[j + 10];
+            x0i = a[j + 9] + a[j + 11];
+            x1r = a[j + 8] - a[j + 10];
+            x1i = a[j + 9] - a[j + 11];
+            x2r = a[j + 12] + a[j + 14];
+            x2i = a[j + 13] + a[j + 15];
+            x3r = a[j + 12] - a[j + 14];
+            x3i = a[j + 13] - a[j + 15];
+            y4r = x0r + x2r;
+            y4i = x0i + x2i;
+            y6r = x0r - x2r;
+            y6i = x0i - x2i;
+            x0r = x1r - x3i;
+            x0i = x1i + x3r;
+            x2r = x1r + x3i;
+            x2i = x1i - x3r;
+            y5r = wn4r * (x0r - x0i);
+            y5i = wn4r * (x0r + x0i);
+            y7r = wn4r * (x2r - x2i);
+            y7i = wn4r * (x2r + x2i);
+            x0r = y1r + y5r;
+            x0i = y1i + y5i;
+            a[j + 2] = wk1r * x0r - wk1i * x0i;
+            a[j + 3] = wk1r * x0i + wk1i * x0r;
+            x0r = y1r - y5r;
+            x0i = y1i - y5i;
+            a[j + 10] = wk5r * x0r - wk5i * x0i;
+            a[j + 11] = wk5r * x0i + wk5i * x0r;
+            x0r = y3r - y7i;
+            x0i = y3i + y7r;
+            a[j + 6] = wk3r * x0r - wk3i * x0i;
+            a[j + 7] = wk3r * x0i + wk3i * x0r;
+            x0r = y3r + y7i;
+            x0i = y3i - y7r;
+            a[j + 14] = wk7r * x0r - wk7i * x0i;
+            a[j + 15] = wk7r * x0i + wk7i * x0r;
+            a[j] = y0r + y4r;
+            a[j + 1] = y0i + y4i;
+            x0r = y0r - y4r;
+            x0i = y0i - y4i;
+            a[j + 8] = wk4r * x0r - wk4i * x0i;
+            a[j + 9] = wk4r * x0i + wk4i * x0r;
+            x0r = y2r - y6i;
+            x0i = y2i + y6r;
+            a[j + 4] = wk2r * x0r - wk2i * x0i;
+            a[j + 5] = wk2r * x0i + wk2i * x0r;
+            x0r = y2r + y6i;
+            x0i = y2i - y6r;
+            a[j + 12] = wk6r * x0r - wk6i * x0i;
+            a[j + 13] = wk6r * x0i + wk6i * x0r;
+        }
+    }
+}
+
+
+void cftmdl(int n, int l, double *a, double *w)
+{
+    int j, j1, j2, j3, j4, j5, j6, j7, k, k1, m;
+    double wn4r, wtmp, wk1r, wk1i, wk2r, wk2i, wk3r, wk3i, 
+        wk4r, wk4i, wk5r, wk5i, wk6r, wk6i, wk7r, wk7i;
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i, 
+        y0r, y0i, y1r, y1i, y2r, y2i, y3r, y3i, 
+        y4r, y4i, y5r, y5i, y6r, y6i, y7r, y7i;
+    
+    m = l << 3;
+    wn4r = w[2];
+    for (j = 0; j < l; j += 2) {
+        j1 = j + l;
+        j2 = j1 + l;
+        j3 = j2 + l;
+        j4 = j3 + l;
+        j5 = j4 + l;
+        j6 = j5 + l;
+        j7 = j6 + l;
+        x0r = a[j] + a[j1];
+        x0i = a[j + 1] + a[j1 + 1];
+        x1r = a[j] - a[j1];
+        x1i = a[j + 1] - a[j1 + 1];
+        x2r = a[j2] + a[j3];
+        x2i = a[j2 + 1] + a[j3 + 1];
+        x3r = a[j2] - a[j3];
+        x3i = a[j2 + 1] - a[j3 + 1];
+        y0r = x0r + x2r;
+        y0i = x0i + x2i;
+        y2r = x0r - x2r;
+        y2i = x0i - x2i;
+        y1r = x1r - x3i;
+        y1i = x1i + x3r;
+        y3r = x1r + x3i;
+        y3i = x1i - x3r;
+        x0r = a[j4] + a[j5];
+        x0i = a[j4 + 1] + a[j5 + 1];
+        x1r = a[j4] - a[j5];
+        x1i = a[j4 + 1] - a[j5 + 1];
+        x2r = a[j6] + a[j7];
+        x2i = a[j6 + 1] + a[j7 + 1];
+        x3r = a[j6] - a[j7];
+        x3i = a[j6 + 1] - a[j7 + 1];
+        y4r = x0r + x2r;
+        y4i = x0i + x2i;
+        y6r = x0r - x2r;
+        y6i = x0i - x2i;
+        x0r = x1r - x3i;
+        x0i = x1i + x3r;
+        x2r = x1r + x3i;
+        x2i = x1i - x3r;
+        y5r = wn4r * (x0r - x0i);
+        y5i = wn4r * (x0r + x0i);
+        y7r = wn4r * (x2r - x2i);
+        y7i = wn4r * (x2r + x2i);
+        a[j1] = y1r + y5r;
+        a[j1 + 1] = y1i + y5i;
+        a[j5] = y1r - y5r;
+        a[j5 + 1] = y1i - y5i;
+        a[j3] = y3r - y7i;
+        a[j3 + 1] = y3i + y7r;
+        a[j7] = y3r + y7i;
+        a[j7 + 1] = y3i - y7r;
+        a[j] = y0r + y4r;
+        a[j + 1] = y0i + y4i;
+        a[j4] = y0r - y4r;
+        a[j4 + 1] = y0i - y4i;
+        a[j2] = y2r - y6i;
+        a[j2 + 1] = y2i + y6r;
+        a[j6] = y2r + y6i;
+        a[j6 + 1] = y2i - y6r;
+    }
+    if (m < n) {
+        wk1r = w[4];
+        wk1i = w[5];
+        for (j = m; j < l + m; j += 2) {
+            j1 = j + l;
+            j2 = j1 + l;
+            j3 = j2 + l;
+            j4 = j3 + l;
+            j5 = j4 + l;
+            j6 = j5 + l;
+            j7 = j6 + l;
+            x0r = a[j] + a[j1];
+            x0i = a[j + 1] + a[j1 + 1];
+            x1r = a[j] - a[j1];
+            x1i = a[j + 1] - a[j1 + 1];
+            x2r = a[j2] + a[j3];
+            x2i = a[j2 + 1] + a[j3 + 1];
+            x3r = a[j2] - a[j3];
+            x3i = a[j2 + 1] - a[j3 + 1];
+            y0r = x0r + x2r;
+            y0i = x0i + x2i;
+            y2r = x0r - x2r;
+            y2i = x0i - x2i;
+            y1r = x1r - x3i;
+            y1i = x1i + x3r;
+            y3r = x1r + x3i;
+            y3i = x1i - x3r;
+            x0r = a[j4] + a[j5];
+            x0i = a[j4 + 1] + a[j5 + 1];
+            x1r = a[j4] - a[j5];
+            x1i = a[j4 + 1] - a[j5 + 1];
+            x2r = a[j6] + a[j7];
+            x2i = a[j6 + 1] + a[j7 + 1];
+            x3r = a[j6] - a[j7];
+            x3i = a[j6 + 1] - a[j7 + 1];
+            y4r = x0r + x2r;
+            y4i = x0i + x2i;
+            y6r = x0r - x2r;
+            y6i = x0i - x2i;
+            x0r = x1r - x3i;
+            x0i = x1i + x3r;
+            x2r = x1r + x3i;
+            x2i = x3r - x1i;
+            y5r = wk1i * x0r - wk1r * x0i;
+            y5i = wk1i * x0i + wk1r * x0r;
+            y7r = wk1r * x2r + wk1i * x2i;
+            y7i = wk1r * x2i - wk1i * x2r;
+            x0r = wk1r * y1r - wk1i * y1i;
+            x0i = wk1r * y1i + wk1i * y1r;
+            a[j1] = x0r + y5r;
+            a[j1 + 1] = x0i + y5i;
+            a[j5] = y5i - x0i;
+            a[j5 + 1] = x0r - y5r;
+            x0r = wk1i * y3r - wk1r * y3i;
+            x0i = wk1i * y3i + wk1r * y3r;
+            a[j3] = x0r - y7r;
+            a[j3 + 1] = x0i + y7i;
+            a[j7] = y7i - x0i;
+            a[j7 + 1] = x0r + y7r;
+            a[j] = y0r + y4r;
+            a[j + 1] = y0i + y4i;
+            a[j4] = y4i - y0i;
+            a[j4 + 1] = y0r - y4r;
+            x0r = y2r - y6i;
+            x0i = y2i + y6r;
+            a[j2] = wn4r * (x0r - x0i);
+            a[j2 + 1] = wn4r * (x0i + x0r);
+            x0r = y6r - y2i;
+            x0i = y2r + y6i;
+            a[j6] = wn4r * (x0r - x0i);
+            a[j6 + 1] = wn4r * (x0i + x0r);
+        }
+        k1 = 4;
+        for (k = 2 * m; k < n; k += m) {
+            k1 += 4;
+            wk1r = w[k1];
+            wk1i = w[k1 + 1];
+            wk2r = w[k1 + 2];
+            wk2i = w[k1 + 3];
+            wtmp = 2 * wk2i;
+            wk3r = wk1r - wtmp * wk1i;
+            wk3i = wtmp * wk1r - wk1i;
+            wk4r = 1 - wtmp * wk2i;
+            wk4i = wtmp * wk2r;
+            wtmp = 2 * wk4i;
+            wk5r = wk3r - wtmp * wk1i;
+            wk5i = wtmp * wk1r - wk3i;
+            wk6r = wk2r - wtmp * wk2i;
+            wk6i = wtmp * wk2r - wk2i;
+            wk7r = wk1r - wtmp * wk3i;
+            wk7i = wtmp * wk3r - wk1i;
+            for (j = k; j < l + k; j += 2) {
+                j1 = j + l;
+                j2 = j1 + l;
+                j3 = j2 + l;
+                j4 = j3 + l;
+                j5 = j4 + l;
+                j6 = j5 + l;
+                j7 = j6 + l;
+                x0r = a[j] + a[j1];
+                x0i = a[j + 1] + a[j1 + 1];
+                x1r = a[j] - a[j1];
+                x1i = a[j + 1] - a[j1 + 1];
+                x2r = a[j2] + a[j3];
+                x2i = a[j2 + 1] + a[j3 + 1];
+                x3r = a[j2] - a[j3];
+                x3i = a[j2 + 1] - a[j3 + 1];
+                y0r = x0r + x2r;
+                y0i = x0i + x2i;
+                y2r = x0r - x2r;
+                y2i = x0i - x2i;
+                y1r = x1r - x3i;
+                y1i = x1i + x3r;
+                y3r = x1r + x3i;
+                y3i = x1i - x3r;
+                x0r = a[j4] + a[j5];
+                x0i = a[j4 + 1] + a[j5 + 1];
+                x1r = a[j4] - a[j5];
+                x1i = a[j4 + 1] - a[j5 + 1];
+                x2r = a[j6] + a[j7];
+                x2i = a[j6 + 1] + a[j7 + 1];
+                x3r = a[j6] - a[j7];
+                x3i = a[j6 + 1] - a[j7 + 1];
+                y4r = x0r + x2r;
+                y4i = x0i + x2i;
+                y6r = x0r - x2r;
+                y6i = x0i - x2i;
+                x0r = x1r - x3i;
+                x0i = x1i + x3r;
+                x2r = x1r + x3i;
+                x2i = x1i - x3r;
+                y5r = wn4r * (x0r - x0i);
+                y5i = wn4r * (x0r + x0i);
+                y7r = wn4r * (x2r - x2i);
+                y7i = wn4r * (x2r + x2i);
+                x0r = y1r + y5r;
+                x0i = y1i + y5i;
+                a[j1] = wk1r * x0r - wk1i * x0i;
+                a[j1 + 1] = wk1r * x0i + wk1i * x0r;
+                x0r = y1r - y5r;
+                x0i = y1i - y5i;
+                a[j5] = wk5r * x0r - wk5i * x0i;
+                a[j5 + 1] = wk5r * x0i + wk5i * x0r;
+                x0r = y3r - y7i;
+                x0i = y3i + y7r;
+                a[j3] = wk3r * x0r - wk3i * x0i;
+                a[j3 + 1] = wk3r * x0i + wk3i * x0r;
+                x0r = y3r + y7i;
+                x0i = y3i - y7r;
+                a[j7] = wk7r * x0r - wk7i * x0i;
+                a[j7 + 1] = wk7r * x0i + wk7i * x0r;
+                a[j] = y0r + y4r;
+                a[j + 1] = y0i + y4i;
+                x0r = y0r - y4r;
+                x0i = y0i - y4i;
+                a[j4] = wk4r * x0r - wk4i * x0i;
+                a[j4 + 1] = wk4r * x0i + wk4i * x0r;
+                x0r = y2r - y6i;
+                x0i = y2i + y6r;
+                a[j2] = wk2r * x0r - wk2i * x0i;
+                a[j2 + 1] = wk2r * x0i + wk2i * x0r;
+                x0r = y2r + y6i;
+                x0i = y2i - y6r;
+                a[j6] = wk6r * x0r - wk6i * x0i;
+                a[j6 + 1] = wk6r * x0i + wk6i * x0r;
+            }
+        }
+    }
+}
+
+
+void rftfsub(int n, double *a, int nc, double *c)
+{
+    int j, k, kk, ks, m;
+    double wkr, wki, xr, xi, yr, yi;
+    
+    m = n >> 1;
+    ks = 2 * nc / m;
+    kk = 0;
+    for (j = 2; j < m; j += 2) {
+        k = n - j;
+        kk += ks;
+        wkr = 0.5 - c[nc - kk];
+        wki = c[kk];
+        xr = a[j] - a[k];
+        xi = a[j + 1] + a[k + 1];
+        yr = wkr * xr - wki * xi;
+        yi = wkr * xi + wki * xr;
+        a[j] -= yr;
+        a[j + 1] -= yi;
+        a[k] += yr;
+        a[k + 1] -= yi;
+    }
+}
+
+
+void rftbsub(int n, double *a, int nc, double *c)
+{
+    int j, k, kk, ks, m;
+    double wkr, wki, xr, xi, yr, yi;
+    
+    a[1] = -a[1];
+    m = n >> 1;
+    ks = 2 * nc / m;
+    kk = 0;
+    for (j = 2; j < m; j += 2) {
+        k = n - j;
+        kk += ks;
+        wkr = 0.5 - c[nc - kk];
+        wki = c[kk];
+        xr = a[j] - a[k];
+        xi = a[j + 1] + a[k + 1];
+        yr = wkr * xr + wki * xi;
+        yi = wkr * xi - wki * xr;
+        a[j] -= yr;
+        a[j + 1] = yi - a[j + 1];
+        a[k] += yr;
+        a[k + 1] = yi - a[k + 1];
+    }
+    a[m + 1] = -a[m + 1];
+}
+
+
+void dctsub(int n, double *a, int nc, double *c)
+{
+    int j, k, kk, ks, m;
+    double wkr, wki, xr;
+    
+    m = n >> 1;
+    ks = nc / n;
+    kk = 0;
+    for (j = 1; j < m; j++) {
+        k = n - j;
+        kk += ks;
+        wkr = c[kk] - c[nc - kk];
+        wki = c[kk] + c[nc - kk];
+        xr = wki * a[j] - wkr * a[k];
+        a[j] = wkr * a[j] + wki * a[k];
+        a[k] = xr;
+    }
+    a[m] *= c[0];
+}
+
+
+void dstsub(int n, double *a, int nc, double *c)
+{
+    int j, k, kk, ks, m;
+    double wkr, wki, xr;
+    
+    m = n >> 1;
+    ks = nc / n;
+    kk = 0;
+    for (j = 1; j < m; j++) {
+        k = n - j;
+        kk += ks;
+        wkr = c[kk] - c[nc - kk];
+        wki = c[kk] + c[nc - kk];
+        xr = wki * a[k] - wkr * a[j];
+        a[k] = wkr * a[k] + wki * a[j];
+        a[j] = xr;
+    }
+    a[m] *= c[0];
+}
+
diff --git a/third_party/tflite-micro/third_party/fft2d/fft8g_h.c b/third_party/tflite-micro/third_party/fft2d/fft8g_h.c
new file mode 100644
index 00000000..4711a6ee
--- /dev/null
+++ b/third_party/tflite-micro/third_party/fft2d/fft8g_h.c
@@ -0,0 +1,1659 @@
+/*
+Fast Fourier/Cosine/Sine Transform
+    dimension   :one
+    data length :power of 2
+    decimation  :frequency
+    radix       :8, 4, 2
+    data        :inplace
+    table       :not use
+functions
+    cdft: Complex Discrete Fourier Transform
+    rdft: Real Discrete Fourier Transform
+    ddct: Discrete Cosine Transform
+    ddst: Discrete Sine Transform
+    dfct: Cosine Transform of RDFT (Real Symmetric DFT)
+    dfst: Sine Transform of RDFT (Real Anti-symmetric DFT)
+function prototypes
+    void cdft(int, int, double *);
+    void rdft(int, int, double *);
+    void ddct(int, int, double *);
+    void ddst(int, int, double *);
+    void dfct(int, double *);
+    void dfst(int, double *);
+
+
+-------- Complex DFT (Discrete Fourier Transform) --------
+    [definition]
+        <case1>
+            X[k] = sum_j=0^n-1 x[j]*exp(2*pi*i*j*k/n), 0<=k<n
+        <case2>
+            X[k] = sum_j=0^n-1 x[j]*exp(-2*pi*i*j*k/n), 0<=k<n
+        (notes: sum_j=0^n-1 is a summation from j=0 to n-1)
+    [usage]
+        <case1>
+            cdft(2*n, 1, a);
+        <case2>
+            cdft(2*n, -1, a);
+    [parameters]
+        2*n            :data length (int)
+                        n >= 1, n = power of 2
+        a[0...2*n-1]   :input/output data (double *)
+                        input data
+                            a[2*j] = Re(x[j]), 
+                            a[2*j+1] = Im(x[j]), 0<=j<n
+                        output data
+                            a[2*k] = Re(X[k]), 
+                            a[2*k+1] = Im(X[k]), 0<=k<n
+    [remark]
+        Inverse of 
+            cdft(2*n, -1, a);
+        is 
+            cdft(2*n, 1, a);
+            for (j = 0; j <= 2 * n - 1; j++) {
+                a[j] *= 1.0 / n;
+            }
+        .
+
+
+-------- Real DFT / Inverse of Real DFT --------
+    [definition]
+        <case1> RDFT
+            R[k] = sum_j=0^n-1 a[j]*cos(2*pi*j*k/n), 0<=k<=n/2
+            I[k] = sum_j=0^n-1 a[j]*sin(2*pi*j*k/n), 0<k<n/2
+        <case2> IRDFT (excluding scale)
+            a[k] = (R[0] + R[n/2]*cos(pi*k))/2 + 
+                   sum_j=1^n/2-1 R[j]*cos(2*pi*j*k/n) + 
+                   sum_j=1^n/2-1 I[j]*sin(2*pi*j*k/n), 0<=k<n
+    [usage]
+        <case1>
+            rdft(n, 1, a);
+        <case2>
+            rdft(n, -1, a);
+    [parameters]
+        n              :data length (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        <case1>
+                            output data
+                                a[2*k] = R[k], 0<=k<n/2
+                                a[2*k+1] = I[k], 0<k<n/2
+                                a[1] = R[n/2]
+                        <case2>
+                            input data
+                                a[2*j] = R[j], 0<=j<n/2
+                                a[2*j+1] = I[j], 0<j<n/2
+                                a[1] = R[n/2]
+    [remark]
+        Inverse of 
+            rdft(n, 1, a);
+        is 
+            rdft(n, -1, a);
+            for (j = 0; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- DCT (Discrete Cosine Transform) / Inverse of DCT --------
+    [definition]
+        <case1> IDCT (excluding scale)
+            C[k] = sum_j=0^n-1 a[j]*cos(pi*j*(k+1/2)/n), 0<=k<n
+        <case2> DCT
+            C[k] = sum_j=0^n-1 a[j]*cos(pi*(j+1/2)*k/n), 0<=k<n
+    [usage]
+        <case1>
+            ddct(n, 1, a);
+        <case2>
+            ddct(n, -1, a);
+    [parameters]
+        n              :data length (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        output data
+                            a[k] = C[k], 0<=k<n
+    [remark]
+        Inverse of 
+            ddct(n, -1, a);
+        is 
+            a[0] *= 0.5;
+            ddct(n, 1, a);
+            for (j = 0; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- DST (Discrete Sine Transform) / Inverse of DST --------
+    [definition]
+        <case1> IDST (excluding scale)
+            S[k] = sum_j=1^n A[j]*sin(pi*j*(k+1/2)/n), 0<=k<n
+        <case2> DST
+            S[k] = sum_j=0^n-1 a[j]*sin(pi*(j+1/2)*k/n), 0<k<=n
+    [usage]
+        <case1>
+            ddst(n, 1, a);
+        <case2>
+            ddst(n, -1, a);
+    [parameters]
+        n              :data length (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        <case1>
+                            input data
+                                a[j] = A[j], 0<j<n
+                                a[0] = A[n]
+                            output data
+                                a[k] = S[k], 0<=k<n
+                        <case2>
+                            output data
+                                a[k] = S[k], 0<k<n
+                                a[0] = S[n]
+    [remark]
+        Inverse of 
+            ddst(n, -1, a);
+        is 
+            a[0] *= 0.5;
+            ddst(n, 1, a);
+            for (j = 0; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- Cosine Transform of RDFT (Real Symmetric DFT) --------
+    [definition]
+        C[k] = sum_j=0^n a[j]*cos(pi*j*k/n), 0<=k<=n
+    [usage]
+        dfct(n, a);
+    [parameters]
+        n              :data length - 1 (int)
+                        n >= 2, n = power of 2
+        a[0...n]       :input/output data (double *)
+                        output data
+                            a[k] = C[k], 0<=k<=n
+    [remark]
+        Inverse of 
+            a[0] *= 0.5;
+            a[n] *= 0.5;
+            dfct(n, a);
+        is 
+            a[0] *= 0.5;
+            a[n] *= 0.5;
+            dfct(n, a);
+            for (j = 0; j <= n; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- Sine Transform of RDFT (Real Anti-symmetric DFT) --------
+    [definition]
+        S[k] = sum_j=1^n-1 a[j]*sin(pi*j*k/n), 0<k<n
+    [usage]
+        dfst(n, a);
+    [parameters]
+        n              :data length + 1 (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        output data
+                            a[k] = S[k], 0<k<n
+                        (a[0] is used for work area)
+    [remark]
+        Inverse of 
+            dfst(n, a);
+        is 
+            dfst(n, a);
+            for (j = 1; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+*/
+
+
+void cdft(int n, int isgn, double *a)
+{
+    void bitrv2(int n, double *a);
+    void bitrv2conj(int n, double *a);
+    void cftfsub(int n, double *a);
+    void cftbsub(int n, double *a);
+    
+    if (n > 4) {
+        if (isgn >= 0) {
+            bitrv2(n, a);
+            cftfsub(n, a);
+        } else {
+            bitrv2conj(n, a);
+            cftbsub(n, a);
+        }
+    } else if (n == 4) {
+        cftfsub(n, a);
+    }
+}
+
+
+void rdft(int n, int isgn, double *a)
+{
+    void bitrv2(int n, double *a);
+    void cftfsub(int n, double *a);
+    void cftbsub(int n, double *a);
+    void rftfsub(int n, double *a);
+    void rftbsub(int n, double *a);
+    double xi;
+    
+    if (isgn >= 0) {
+        if (n > 4) {
+            bitrv2(n, a);
+            cftfsub(n, a);
+            rftfsub(n, a);
+        } else if (n == 4) {
+            cftfsub(n, a);
+        }
+        xi = a[0] - a[1];
+        a[0] += a[1];
+        a[1] = xi;
+    } else {
+        a[1] = 0.5 * (a[0] - a[1]);
+        a[0] -= a[1];
+        if (n > 4) {
+            rftbsub(n, a);
+            bitrv2(n, a);
+            cftbsub(n, a);
+        } else if (n == 4) {
+            cftfsub(n, a);
+        }
+    }
+}
+
+
+void ddct(int n, int isgn, double *a)
+{
+    void bitrv2(int n, double *a);
+    void cftfsub(int n, double *a);
+    void cftbsub(int n, double *a);
+    void rftfsub(int n, double *a);
+    void rftbsub(int n, double *a);
+    void dctsub(int n, double *a);
+    void dctsub4(int n, double *a);
+    int j;
+    double xr;
+    
+    if (isgn < 0) {
+        xr = a[n - 1];
+        for (j = n - 2; j >= 2; j -= 2) {
+            a[j + 1] = a[j] - a[j - 1];
+            a[j] += a[j - 1];
+        }
+        a[1] = a[0] - xr;
+        a[0] += xr;
+        if (n > 4) {
+            rftbsub(n, a);
+            bitrv2(n, a);
+            cftbsub(n, a);
+        } else if (n == 4) {
+            cftfsub(n, a);
+        }
+    }
+    if (n > 4) {
+        dctsub(n, a);
+    } else {
+        dctsub4(n, a);
+    }
+    if (isgn >= 0) {
+        if (n > 4) {
+            bitrv2(n, a);
+            cftfsub(n, a);
+            rftfsub(n, a);
+        } else if (n == 4) {
+            cftfsub(n, a);
+        }
+        xr = a[0] - a[1];
+        a[0] += a[1];
+        for (j = 2; j < n; j += 2) {
+            a[j - 1] = a[j] - a[j + 1];
+            a[j] += a[j + 1];
+        }
+        a[n - 1] = xr;
+    }
+}
+
+
+void ddst(int n, int isgn, double *a)
+{
+    void bitrv2(int n, double *a);
+    void cftfsub(int n, double *a);
+    void cftbsub(int n, double *a);
+    void rftfsub(int n, double *a);
+    void rftbsub(int n, double *a);
+    void dstsub(int n, double *a);
+    void dstsub4(int n, double *a);
+    int j;
+    double xr;
+    
+    if (isgn < 0) {
+        xr = a[n - 1];
+        for (j = n - 2; j >= 2; j -= 2) {
+            a[j + 1] = -a[j] - a[j - 1];
+            a[j] -= a[j - 1];
+        }
+        a[1] = a[0] + xr;
+        a[0] -= xr;
+        if (n > 4) {
+            rftbsub(n, a);
+            bitrv2(n, a);
+            cftbsub(n, a);
+        } else if (n == 4) {
+            cftfsub(n, a);
+        }
+    }
+    if (n > 4) {
+        dstsub(n, a);
+    } else {
+        dstsub4(n, a);
+    }
+    if (isgn >= 0) {
+        if (n > 4) {
+            bitrv2(n, a);
+            cftfsub(n, a);
+            rftfsub(n, a);
+        } else if (n == 4) {
+            cftfsub(n, a);
+        }
+        xr = a[0] - a[1];
+        a[0] += a[1];
+        for (j = 2; j < n; j += 2) {
+            a[j - 1] = -a[j] - a[j + 1];
+            a[j] -= a[j + 1];
+        }
+        a[n - 1] = -xr;
+    }
+}
+
+
+void dfct(int n, double *a)
+{
+    void ddct(int n, int isgn, double *a);
+    void bitrv1(int n, double *a);
+    int j, k, m, mh;
+    double xr, xi, yr, yi, an;
+    
+    m = n >> 1;
+    for (j = 0; j < m; j++) {
+        k = n - j;
+        xr = a[j] + a[k];
+        a[j] -= a[k];
+        a[k] = xr;
+    }
+    an = a[n];
+    while (m >= 2) {
+        ddct(m, 1, a);
+        bitrv1(m, a);
+        mh = m >> 1;
+        xi = a[m];
+        a[m] = a[0];
+        a[0] = an - xi;
+        an += xi;
+        for (j = 1; j < mh; j++) {
+            k = m - j;
+            xr = a[m + k];
+            xi = a[m + j];
+            yr = a[j];
+            yi = a[k];
+            a[m + j] = yr;
+            a[m + k] = yi;
+            a[j] = xr - xi;
+            a[k] = xr + xi;
+        }
+        xr = a[mh];
+        a[mh] = a[m + mh];
+        a[m + mh] = xr;
+        m = mh;
+    }
+    xi = a[1];
+    a[1] = a[0];
+    a[0] = an + xi;
+    a[n] = an - xi;
+    bitrv1(n, a);
+}
+
+
+void dfst(int n, double *a)
+{
+    void ddst(int n, int isgn, double *a);
+    void bitrv1(int n, double *a);
+    int j, k, m, mh;
+    double xr, xi, yr, yi;
+    
+    m = n >> 1;
+    for (j = 1; j < m; j++) {
+        k = n - j;
+        xr = a[j] - a[k];
+        a[j] += a[k];
+        a[k] = xr;
+    }
+    a[0] = a[m];
+    while (m >= 2) {
+        ddst(m, 1, a);
+        bitrv1(m, a);
+        mh = m >> 1;
+        for (j = 1; j < mh; j++) {
+            k = m - j;
+            xr = a[m + k];
+            xi = a[m + j];
+            yr = a[j];
+            yi = a[k];
+            a[m + j] = yr;
+            a[m + k] = yi;
+            a[j] = xr + xi;
+            a[k] = xr - xi;
+        }
+        a[m] = a[0];
+        a[0] = a[m + mh];
+        a[m + mh] = a[mh];
+        m = mh;
+    }
+    a[1] = a[0];
+    a[0] = 0;
+    bitrv1(n, a);
+}
+
+
+/* -------- child routines -------- */
+
+
+#include <math.h>
+#ifndef M_PI_2
+#define M_PI_2      1.570796326794896619231321691639751442098584699687
+#endif
+#ifndef WR5000  /* cos(M_PI_2*0.5000) */
+#define WR5000      0.707106781186547524400844362104849039284835937688
+#endif
+#ifndef WR2500  /* cos(M_PI_2*0.2500) */
+#define WR2500      0.923879532511286756128183189396788286822416625863
+#endif
+#ifndef WI2500  /* sin(M_PI_2*0.2500) */
+#define WI2500      0.382683432365089771728459984030398866761344562485
+#endif
+
+
+#ifndef RDFT_LOOP_DIV  /* control of the RDFT's speed & tolerance */
+#define RDFT_LOOP_DIV 64
+#endif
+
+#ifndef DCST_LOOP_DIV  /* control of the DCT,DST's speed & tolerance */
+#define DCST_LOOP_DIV 64
+#endif
+
+
+void bitrv2(int n, double *a)
+{
+    int j0, k0, j1, k1, l, m, i, j, k;
+    double xr, xi, yr, yi;
+    
+    l = n >> 2;
+    m = 2;
+    while (m < l) {
+        l >>= 1;
+        m <<= 1;
+    }
+    if (m == l) {
+        j0 = 0;
+        for (k0 = 0; k0 < m; k0 += 2) {
+            k = k0;
+            for (j = j0; j < j0 + k0; j += 2) {
+                xr = a[j];
+                xi = a[j + 1];
+                yr = a[k];
+                yi = a[k + 1];
+                a[j] = yr;
+                a[j + 1] = yi;
+                a[k] = xr;
+                a[k + 1] = xi;
+                j1 = j + m;
+                k1 = k + 2 * m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m;
+                k1 -= m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m;
+                k1 += 2 * m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                for (i = n >> 1; i > (k ^= i); i >>= 1);
+            }
+            j1 = j0 + k0 + m;
+            k1 = j1 + m;
+            xr = a[j1];
+            xi = a[j1 + 1];
+            yr = a[k1];
+            yi = a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            for (i = n >> 1; i > (j0 ^= i); i >>= 1);
+        }
+    } else {
+        j0 = 0;
+        for (k0 = 2; k0 < m; k0 += 2) {
+            for (i = n >> 1; i > (j0 ^= i); i >>= 1);
+            k = k0;
+            for (j = j0; j < j0 + k0; j += 2) {
+                xr = a[j];
+                xi = a[j + 1];
+                yr = a[k];
+                yi = a[k + 1];
+                a[j] = yr;
+                a[j + 1] = yi;
+                a[k] = xr;
+                a[k + 1] = xi;
+                j1 = j + m;
+                k1 = k + m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                for (i = n >> 1; i > (k ^= i); i >>= 1);
+            }
+        }
+    }
+}
+
+
+void bitrv2conj(int n, double *a)
+{
+    int j0, k0, j1, k1, l, m, i, j, k;
+    double xr, xi, yr, yi;
+    
+    l = n >> 2;
+    m = 2;
+    while (m < l) {
+        l >>= 1;
+        m <<= 1;
+    }
+    if (m == l) {
+        j0 = 0;
+        for (k0 = 0; k0 < m; k0 += 2) {
+            k = k0;
+            for (j = j0; j < j0 + k0; j += 2) {
+                xr = a[j];
+                xi = -a[j + 1];
+                yr = a[k];
+                yi = -a[k + 1];
+                a[j] = yr;
+                a[j + 1] = yi;
+                a[k] = xr;
+                a[k + 1] = xi;
+                j1 = j + m;
+                k1 = k + 2 * m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m;
+                k1 -= m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m;
+                k1 += 2 * m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                for (i = n >> 1; i > (k ^= i); i >>= 1);
+            }
+            k1 = j0 + k0;
+            a[k1 + 1] = -a[k1 + 1];
+            j1 = k1 + m;
+            k1 = j1 + m;
+            xr = a[j1];
+            xi = -a[j1 + 1];
+            yr = a[k1];
+            yi = -a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            k1 += m;
+            a[k1 + 1] = -a[k1 + 1];
+            for (i = n >> 1; i > (j0 ^= i); i >>= 1);
+        }
+    } else {
+        a[1] = -a[1];
+        a[m + 1] = -a[m + 1];
+        j0 = 0;
+        for (k0 = 2; k0 < m; k0 += 2) {
+            for (i = n >> 1; i > (j0 ^= i); i >>= 1);
+            k = k0;
+            for (j = j0; j < j0 + k0; j += 2) {
+                xr = a[j];
+                xi = -a[j + 1];
+                yr = a[k];
+                yi = -a[k + 1];
+                a[j] = yr;
+                a[j + 1] = yi;
+                a[k] = xr;
+                a[k + 1] = xi;
+                j1 = j + m;
+                k1 = k + m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                for (i = n >> 1; i > (k ^= i); i >>= 1);
+            }
+            k1 = j0 + k0;
+            a[k1 + 1] = -a[k1 + 1];
+            a[k1 + m + 1] = -a[k1 + m + 1];
+        }
+    }
+}
+
+
+void bitrv1(int n, double *a)
+{
+    int j0, k0, j1, k1, l, m, i, j, k;
+    double x;
+    
+    l = n >> 2;
+    m = 1;
+    while (m < l) {
+        l >>= 1;
+        m <<= 1;
+    }
+    if (m == l) {
+        j0 = 0;
+        for (k0 = 0; k0 < m; k0++) {
+            k = k0;
+            for (j = j0; j < j0 + k0; j++) {
+                x = a[j];
+                a[j] = a[k];
+                a[k] = x;
+                j1 = j + m;
+                k1 = k + 2 * m;
+                x = a[j1];
+                a[j1] = a[k1];
+                a[k1] = x;
+                j1 += m;
+                k1 -= m;
+                x = a[j1];
+                a[j1] = a[k1];
+                a[k1] = x;
+                j1 += m;
+                k1 += 2 * m;
+                x = a[j1];
+                a[j1] = a[k1];
+                a[k1] = x;
+                for (i = n >> 1; i > (k ^= i); i >>= 1);
+            }
+            j1 = j0 + k0 + m;
+            k1 = j1 + m;
+            x = a[j1];
+            a[j1] = a[k1];
+            a[k1] = x;
+            for (i = n >> 1; i > (j0 ^= i); i >>= 1);
+        }
+    } else {
+        j0 = 0;
+        for (k0 = 1; k0 < m; k0++) {
+            for (i = n >> 1; i > (j0 ^= i); i >>= 1);
+            k = k0;
+            for (j = j0; j < j0 + k0; j++) {
+                x = a[j];
+                a[j] = a[k];
+                a[k] = x;
+                j1 = j + m;
+                k1 = k + m;
+                x = a[j1];
+                a[j1] = a[k1];
+                a[k1] = x;
+                for (i = n >> 1; i > (k ^= i); i >>= 1);
+            }
+        }
+    }
+}
+
+
+void cftfsub(int n, double *a)
+{
+    void cft1st(int n, double *a);
+    void cftmdl(int n, int l, double *a);
+    int j, j1, j2, j3, l;
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i;
+    
+    l = 2;
+    if (n >= 16) {
+        cft1st(n, a);
+        l = 16;
+        while ((l << 3) <= n) {
+            cftmdl(n, l, a);
+            l <<= 3;
+        }
+    }
+    if ((l << 1) < n) {
+        for (j = 0; j < l; j += 2) {
+            j1 = j + l;
+            j2 = j1 + l;
+            j3 = j2 + l;
+            x0r = a[j] + a[j1];
+            x0i = a[j + 1] + a[j1 + 1];
+            x1r = a[j] - a[j1];
+            x1i = a[j + 1] - a[j1 + 1];
+            x2r = a[j2] + a[j3];
+            x2i = a[j2 + 1] + a[j3 + 1];
+            x3r = a[j2] - a[j3];
+            x3i = a[j2 + 1] - a[j3 + 1];
+            a[j] = x0r + x2r;
+            a[j + 1] = x0i + x2i;
+            a[j2] = x0r - x2r;
+            a[j2 + 1] = x0i - x2i;
+            a[j1] = x1r - x3i;
+            a[j1 + 1] = x1i + x3r;
+            a[j3] = x1r + x3i;
+            a[j3 + 1] = x1i - x3r;
+        }
+    } else if ((l << 1) == n) {
+        for (j = 0; j < l; j += 2) {
+            j1 = j + l;
+            x0r = a[j] - a[j1];
+            x0i = a[j + 1] - a[j1 + 1];
+            a[j] += a[j1];
+            a[j + 1] += a[j1 + 1];
+            a[j1] = x0r;
+            a[j1 + 1] = x0i;
+        }
+    }
+}
+
+
+void cftbsub(int n, double *a)
+{
+    void cft1st(int n, double *a);
+    void cftmdl(int n, int l, double *a);
+    int j, j1, j2, j3, j4, j5, j6, j7, l;
+    double wn4r, x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i, 
+        y0r, y0i, y1r, y1i, y2r, y2i, y3r, y3i, 
+        y4r, y4i, y5r, y5i, y6r, y6i, y7r, y7i;
+    
+    l = 2;
+    if (n > 16) {
+        cft1st(n, a);
+        l = 16;
+        while ((l << 3) < n) {
+            cftmdl(n, l, a);
+            l <<= 3;
+        }
+    }
+    if ((l << 2) < n) {
+        wn4r = WR5000;
+        for (j = 0; j < l; j += 2) {
+            j1 = j + l;
+            j2 = j1 + l;
+            j3 = j2 + l;
+            j4 = j3 + l;
+            j5 = j4 + l;
+            j6 = j5 + l;
+            j7 = j6 + l;
+            x0r = a[j] + a[j1];
+            x0i = -a[j + 1] - a[j1 + 1];
+            x1r = a[j] - a[j1];
+            x1i = -a[j + 1] + a[j1 + 1];
+            x2r = a[j2] + a[j3];
+            x2i = a[j2 + 1] + a[j3 + 1];
+            x3r = a[j2] - a[j3];
+            x3i = a[j2 + 1] - a[j3 + 1];
+            y0r = x0r + x2r;
+            y0i = x0i - x2i;
+            y2r = x0r - x2r;
+            y2i = x0i + x2i;
+            y1r = x1r - x3i;
+            y1i = x1i - x3r;
+            y3r = x1r + x3i;
+            y3i = x1i + x3r;
+            x0r = a[j4] + a[j5];
+            x0i = a[j4 + 1] + a[j5 + 1];
+            x1r = a[j4] - a[j5];
+            x1i = a[j4 + 1] - a[j5 + 1];
+            x2r = a[j6] + a[j7];
+            x2i = a[j6 + 1] + a[j7 + 1];
+            x3r = a[j6] - a[j7];
+            x3i = a[j6 + 1] - a[j7 + 1];
+            y4r = x0r + x2r;
+            y4i = x0i + x2i;
+            y6r = x0r - x2r;
+            y6i = x0i - x2i;
+            x0r = x1r - x3i;
+            x0i = x1i + x3r;
+            x2r = x1r + x3i;
+            x2i = x1i - x3r;
+            y5r = wn4r * (x0r - x0i);
+            y5i = wn4r * (x0r + x0i);
+            y7r = wn4r * (x2r - x2i);
+            y7i = wn4r * (x2r + x2i);
+            a[j1] = y1r + y5r;
+            a[j1 + 1] = y1i - y5i;
+            a[j5] = y1r - y5r;
+            a[j5 + 1] = y1i + y5i;
+            a[j3] = y3r - y7i;
+            a[j3 + 1] = y3i - y7r;
+            a[j7] = y3r + y7i;
+            a[j7 + 1] = y3i + y7r;
+            a[j] = y0r + y4r;
+            a[j + 1] = y0i - y4i;
+            a[j4] = y0r - y4r;
+            a[j4 + 1] = y0i + y4i;
+            a[j2] = y2r - y6i;
+            a[j2 + 1] = y2i - y6r;
+            a[j6] = y2r + y6i;
+            a[j6 + 1] = y2i + y6r;
+        }
+    } else if ((l << 2) == n) {
+        for (j = 0; j < l; j += 2) {
+            j1 = j + l;
+            j2 = j1 + l;
+            j3 = j2 + l;
+            x0r = a[j] + a[j1];
+            x0i = -a[j + 1] - a[j1 + 1];
+            x1r = a[j] - a[j1];
+            x1i = -a[j + 1] + a[j1 + 1];
+            x2r = a[j2] + a[j3];
+            x2i = a[j2 + 1] + a[j3 + 1];
+            x3r = a[j2] - a[j3];
+            x3i = a[j2 + 1] - a[j3 + 1];
+            a[j] = x0r + x2r;
+            a[j + 1] = x0i - x2i;
+            a[j2] = x0r - x2r;
+            a[j2 + 1] = x0i + x2i;
+            a[j1] = x1r - x3i;
+            a[j1 + 1] = x1i - x3r;
+            a[j3] = x1r + x3i;
+            a[j3 + 1] = x1i + x3r;
+        }
+    } else {
+        for (j = 0; j < l; j += 2) {
+            j1 = j + l;
+            x0r = a[j] - a[j1];
+            x0i = -a[j + 1] + a[j1 + 1];
+            a[j] += a[j1];
+            a[j + 1] = -a[j + 1] - a[j1 + 1];
+            a[j1] = x0r;
+            a[j1 + 1] = x0i;
+        }
+    }
+}
+
+
+void cft1st(int n, double *a)
+{
+    int j, kj, kr;
+    double ew, wn4r, wtmp, wk1r, wk1i, wk2r, wk2i, wk3r, wk3i, 
+        wk4r, wk4i, wk5r, wk5i, wk6r, wk6i, wk7r, wk7i;
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i, 
+        y0r, y0i, y1r, y1i, y2r, y2i, y3r, y3i, 
+        y4r, y4i, y5r, y5i, y6r, y6i, y7r, y7i;
+    
+    wn4r = WR5000;
+    x0r = a[0] + a[2];
+    x0i = a[1] + a[3];
+    x1r = a[0] - a[2];
+    x1i = a[1] - a[3];
+    x2r = a[4] + a[6];
+    x2i = a[5] + a[7];
+    x3r = a[4] - a[6];
+    x3i = a[5] - a[7];
+    y0r = x0r + x2r;
+    y0i = x0i + x2i;
+    y2r = x0r - x2r;
+    y2i = x0i - x2i;
+    y1r = x1r - x3i;
+    y1i = x1i + x3r;
+    y3r = x1r + x3i;
+    y3i = x1i - x3r;
+    x0r = a[8] + a[10];
+    x0i = a[9] + a[11];
+    x1r = a[8] - a[10];
+    x1i = a[9] - a[11];
+    x2r = a[12] + a[14];
+    x2i = a[13] + a[15];
+    x3r = a[12] - a[14];
+    x3i = a[13] - a[15];
+    y4r = x0r + x2r;
+    y4i = x0i + x2i;
+    y6r = x0r - x2r;
+    y6i = x0i - x2i;
+    x0r = x1r - x3i;
+    x0i = x1i + x3r;
+    x2r = x1r + x3i;
+    x2i = x1i - x3r;
+    y5r = wn4r * (x0r - x0i);
+    y5i = wn4r * (x0r + x0i);
+    y7r = wn4r * (x2r - x2i);
+    y7i = wn4r * (x2r + x2i);
+    a[2] = y1r + y5r;
+    a[3] = y1i + y5i;
+    a[10] = y1r - y5r;
+    a[11] = y1i - y5i;
+    a[6] = y3r - y7i;
+    a[7] = y3i + y7r;
+    a[14] = y3r + y7i;
+    a[15] = y3i - y7r;
+    a[0] = y0r + y4r;
+    a[1] = y0i + y4i;
+    a[8] = y0r - y4r;
+    a[9] = y0i - y4i;
+    a[4] = y2r - y6i;
+    a[5] = y2i + y6r;
+    a[12] = y2r + y6i;
+    a[13] = y2i - y6r;
+    if (n > 16) {
+        wk1r = WR2500;
+        wk1i = WI2500;
+        x0r = a[16] + a[18];
+        x0i = a[17] + a[19];
+        x1r = a[16] - a[18];
+        x1i = a[17] - a[19];
+        x2r = a[20] + a[22];
+        x2i = a[21] + a[23];
+        x3r = a[20] - a[22];
+        x3i = a[21] - a[23];
+        y0r = x0r + x2r;
+        y0i = x0i + x2i;
+        y2r = x0r - x2r;
+        y2i = x0i - x2i;
+        y1r = x1r - x3i;
+        y1i = x1i + x3r;
+        y3r = x1r + x3i;
+        y3i = x1i - x3r;
+        x0r = a[24] + a[26];
+        x0i = a[25] + a[27];
+        x1r = a[24] - a[26];
+        x1i = a[25] - a[27];
+        x2r = a[28] + a[30];
+        x2i = a[29] + a[31];
+        x3r = a[28] - a[30];
+        x3i = a[29] - a[31];
+        y4r = x0r + x2r;
+        y4i = x0i + x2i;
+        y6r = x0r - x2r;
+        y6i = x0i - x2i;
+        x0r = x1r - x3i;
+        x0i = x1i + x3r;
+        x2r = x1r + x3i;
+        x2i = x3r - x1i;
+        y5r = wk1i * x0r - wk1r * x0i;
+        y5i = wk1i * x0i + wk1r * x0r;
+        y7r = wk1r * x2r + wk1i * x2i;
+        y7i = wk1r * x2i - wk1i * x2r;
+        x0r = wk1r * y1r - wk1i * y1i;
+        x0i = wk1r * y1i + wk1i * y1r;
+        a[18] = x0r + y5r;
+        a[19] = x0i + y5i;
+        a[26] = y5i - x0i;
+        a[27] = x0r - y5r;
+        x0r = wk1i * y3r - wk1r * y3i;
+        x0i = wk1i * y3i + wk1r * y3r;
+        a[22] = x0r - y7r;
+        a[23] = x0i + y7i;
+        a[30] = y7i - x0i;
+        a[31] = x0r + y7r;
+        a[16] = y0r + y4r;
+        a[17] = y0i + y4i;
+        a[24] = y4i - y0i;
+        a[25] = y0r - y4r;
+        x0r = y2r - y6i;
+        x0i = y2i + y6r;
+        a[20] = wn4r * (x0r - x0i);
+        a[21] = wn4r * (x0i + x0r);
+        x0r = y6r - y2i;
+        x0i = y2r + y6i;
+        a[28] = wn4r * (x0r - x0i);
+        a[29] = wn4r * (x0i + x0r);
+        ew = M_PI_2 / n;
+        kr = n >> 2;
+        for (j = 32; j < n; j += 16) {
+            for (kj = n >> 2; kj > (kr ^= kj); kj >>= 1);
+            wk1r = cos(ew * kr);
+            wk1i = sin(ew * kr);
+            wk2r = 1 - 2 * wk1i * wk1i;
+            wk2i = 2 * wk1i * wk1r;
+            wtmp = 2 * wk2i;
+            wk3r = wk1r - wtmp * wk1i;
+            wk3i = wtmp * wk1r - wk1i;
+            wk4r = 1 - wtmp * wk2i;
+            wk4i = wtmp * wk2r;
+            wtmp = 2 * wk4i;
+            wk5r = wk3r - wtmp * wk1i;
+            wk5i = wtmp * wk1r - wk3i;
+            wk6r = wk2r - wtmp * wk2i;
+            wk6i = wtmp * wk2r - wk2i;
+            wk7r = wk1r - wtmp * wk3i;
+            wk7i = wtmp * wk3r - wk1i;
+            x0r = a[j] + a[j + 2];
+            x0i = a[j + 1] + a[j + 3];
+            x1r = a[j] - a[j + 2];
+            x1i = a[j + 1] - a[j + 3];
+            x2r = a[j + 4] + a[j + 6];
+            x2i = a[j + 5] + a[j + 7];
+            x3r = a[j + 4] - a[j + 6];
+            x3i = a[j + 5] - a[j + 7];
+            y0r = x0r + x2r;
+            y0i = x0i + x2i;
+            y2r = x0r - x2r;
+            y2i = x0i - x2i;
+            y1r = x1r - x3i;
+            y1i = x1i + x3r;
+            y3r = x1r + x3i;
+            y3i = x1i - x3r;
+            x0r = a[j + 8] + a[j + 10];
+            x0i = a[j + 9] + a[j + 11];
+            x1r = a[j + 8] - a[j + 10];
+            x1i = a[j + 9] - a[j + 11];
+            x2r = a[j + 12] + a[j + 14];
+            x2i = a[j + 13] + a[j + 15];
+            x3r = a[j + 12] - a[j + 14];
+            x3i = a[j + 13] - a[j + 15];
+            y4r = x0r + x2r;
+            y4i = x0i + x2i;
+            y6r = x0r - x2r;
+            y6i = x0i - x2i;
+            x0r = x1r - x3i;
+            x0i = x1i + x3r;
+            x2r = x1r + x3i;
+            x2i = x1i - x3r;
+            y5r = wn4r * (x0r - x0i);
+            y5i = wn4r * (x0r + x0i);
+            y7r = wn4r * (x2r - x2i);
+            y7i = wn4r * (x2r + x2i);
+            x0r = y1r + y5r;
+            x0i = y1i + y5i;
+            a[j + 2] = wk1r * x0r - wk1i * x0i;
+            a[j + 3] = wk1r * x0i + wk1i * x0r;
+            x0r = y1r - y5r;
+            x0i = y1i - y5i;
+            a[j + 10] = wk5r * x0r - wk5i * x0i;
+            a[j + 11] = wk5r * x0i + wk5i * x0r;
+            x0r = y3r - y7i;
+            x0i = y3i + y7r;
+            a[j + 6] = wk3r * x0r - wk3i * x0i;
+            a[j + 7] = wk3r * x0i + wk3i * x0r;
+            x0r = y3r + y7i;
+            x0i = y3i - y7r;
+            a[j + 14] = wk7r * x0r - wk7i * x0i;
+            a[j + 15] = wk7r * x0i + wk7i * x0r;
+            a[j] = y0r + y4r;
+            a[j + 1] = y0i + y4i;
+            x0r = y0r - y4r;
+            x0i = y0i - y4i;
+            a[j + 8] = wk4r * x0r - wk4i * x0i;
+            a[j + 9] = wk4r * x0i + wk4i * x0r;
+            x0r = y2r - y6i;
+            x0i = y2i + y6r;
+            a[j + 4] = wk2r * x0r - wk2i * x0i;
+            a[j + 5] = wk2r * x0i + wk2i * x0r;
+            x0r = y2r + y6i;
+            x0i = y2i - y6r;
+            a[j + 12] = wk6r * x0r - wk6i * x0i;
+            a[j + 13] = wk6r * x0i + wk6i * x0r;
+        }
+    }
+}
+
+
+void cftmdl(int n, int l, double *a)
+{
+    int j, j1, j2, j3, j4, j5, j6, j7, k, kj, kr, m;
+    double ew, wn4r, wtmp, wk1r, wk1i, wk2r, wk2i, wk3r, wk3i, 
+        wk4r, wk4i, wk5r, wk5i, wk6r, wk6i, wk7r, wk7i;
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i, 
+        y0r, y0i, y1r, y1i, y2r, y2i, y3r, y3i, 
+        y4r, y4i, y5r, y5i, y6r, y6i, y7r, y7i;
+    
+    m = l << 3;
+    wn4r = WR5000;
+    for (j = 0; j < l; j += 2) {
+        j1 = j + l;
+        j2 = j1 + l;
+        j3 = j2 + l;
+        j4 = j3 + l;
+        j5 = j4 + l;
+        j6 = j5 + l;
+        j7 = j6 + l;
+        x0r = a[j] + a[j1];
+        x0i = a[j + 1] + a[j1 + 1];
+        x1r = a[j] - a[j1];
+        x1i = a[j + 1] - a[j1 + 1];
+        x2r = a[j2] + a[j3];
+        x2i = a[j2 + 1] + a[j3 + 1];
+        x3r = a[j2] - a[j3];
+        x3i = a[j2 + 1] - a[j3 + 1];
+        y0r = x0r + x2r;
+        y0i = x0i + x2i;
+        y2r = x0r - x2r;
+        y2i = x0i - x2i;
+        y1r = x1r - x3i;
+        y1i = x1i + x3r;
+        y3r = x1r + x3i;
+        y3i = x1i - x3r;
+        x0r = a[j4] + a[j5];
+        x0i = a[j4 + 1] + a[j5 + 1];
+        x1r = a[j4] - a[j5];
+        x1i = a[j4 + 1] - a[j5 + 1];
+        x2r = a[j6] + a[j7];
+        x2i = a[j6 + 1] + a[j7 + 1];
+        x3r = a[j6] - a[j7];
+        x3i = a[j6 + 1] - a[j7 + 1];
+        y4r = x0r + x2r;
+        y4i = x0i + x2i;
+        y6r = x0r - x2r;
+        y6i = x0i - x2i;
+        x0r = x1r - x3i;
+        x0i = x1i + x3r;
+        x2r = x1r + x3i;
+        x2i = x1i - x3r;
+        y5r = wn4r * (x0r - x0i);
+        y5i = wn4r * (x0r + x0i);
+        y7r = wn4r * (x2r - x2i);
+        y7i = wn4r * (x2r + x2i);
+        a[j1] = y1r + y5r;
+        a[j1 + 1] = y1i + y5i;
+        a[j5] = y1r - y5r;
+        a[j5 + 1] = y1i - y5i;
+        a[j3] = y3r - y7i;
+        a[j3 + 1] = y3i + y7r;
+        a[j7] = y3r + y7i;
+        a[j7 + 1] = y3i - y7r;
+        a[j] = y0r + y4r;
+        a[j + 1] = y0i + y4i;
+        a[j4] = y0r - y4r;
+        a[j4 + 1] = y0i - y4i;
+        a[j2] = y2r - y6i;
+        a[j2 + 1] = y2i + y6r;
+        a[j6] = y2r + y6i;
+        a[j6 + 1] = y2i - y6r;
+    }
+    if (m < n) {
+        wk1r = WR2500;
+        wk1i = WI2500;
+        for (j = m; j < l + m; j += 2) {
+            j1 = j + l;
+            j2 = j1 + l;
+            j3 = j2 + l;
+            j4 = j3 + l;
+            j5 = j4 + l;
+            j6 = j5 + l;
+            j7 = j6 + l;
+            x0r = a[j] + a[j1];
+            x0i = a[j + 1] + a[j1 + 1];
+            x1r = a[j] - a[j1];
+            x1i = a[j + 1] - a[j1 + 1];
+            x2r = a[j2] + a[j3];
+            x2i = a[j2 + 1] + a[j3 + 1];
+            x3r = a[j2] - a[j3];
+            x3i = a[j2 + 1] - a[j3 + 1];
+            y0r = x0r + x2r;
+            y0i = x0i + x2i;
+            y2r = x0r - x2r;
+            y2i = x0i - x2i;
+            y1r = x1r - x3i;
+            y1i = x1i + x3r;
+            y3r = x1r + x3i;
+            y3i = x1i - x3r;
+            x0r = a[j4] + a[j5];
+            x0i = a[j4 + 1] + a[j5 + 1];
+            x1r = a[j4] - a[j5];
+            x1i = a[j4 + 1] - a[j5 + 1];
+            x2r = a[j6] + a[j7];
+            x2i = a[j6 + 1] + a[j7 + 1];
+            x3r = a[j6] - a[j7];
+            x3i = a[j6 + 1] - a[j7 + 1];
+            y4r = x0r + x2r;
+            y4i = x0i + x2i;
+            y6r = x0r - x2r;
+            y6i = x0i - x2i;
+            x0r = x1r - x3i;
+            x0i = x1i + x3r;
+            x2r = x1r + x3i;
+            x2i = x3r - x1i;
+            y5r = wk1i * x0r - wk1r * x0i;
+            y5i = wk1i * x0i + wk1r * x0r;
+            y7r = wk1r * x2r + wk1i * x2i;
+            y7i = wk1r * x2i - wk1i * x2r;
+            x0r = wk1r * y1r - wk1i * y1i;
+            x0i = wk1r * y1i + wk1i * y1r;
+            a[j1] = x0r + y5r;
+            a[j1 + 1] = x0i + y5i;
+            a[j5] = y5i - x0i;
+            a[j5 + 1] = x0r - y5r;
+            x0r = wk1i * y3r - wk1r * y3i;
+            x0i = wk1i * y3i + wk1r * y3r;
+            a[j3] = x0r - y7r;
+            a[j3 + 1] = x0i + y7i;
+            a[j7] = y7i - x0i;
+            a[j7 + 1] = x0r + y7r;
+            a[j] = y0r + y4r;
+            a[j + 1] = y0i + y4i;
+            a[j4] = y4i - y0i;
+            a[j4 + 1] = y0r - y4r;
+            x0r = y2r - y6i;
+            x0i = y2i + y6r;
+            a[j2] = wn4r * (x0r - x0i);
+            a[j2 + 1] = wn4r * (x0i + x0r);
+            x0r = y6r - y2i;
+            x0i = y2r + y6i;
+            a[j6] = wn4r * (x0r - x0i);
+            a[j6 + 1] = wn4r * (x0i + x0r);
+        }
+        ew = M_PI_2 / n;
+        kr = n >> 2;
+        for (k = 2 * m; k < n; k += m) {
+            for (kj = n >> 2; kj > (kr ^= kj); kj >>= 1);
+            wk1r = cos(ew * kr);
+            wk1i = sin(ew * kr);
+            wk2r = 1 - 2 * wk1i * wk1i;
+            wk2i = 2 * wk1i * wk1r;
+            wtmp = 2 * wk2i;
+            wk3r = wk1r - wtmp * wk1i;
+            wk3i = wtmp * wk1r - wk1i;
+            wk4r = 1 - wtmp * wk2i;
+            wk4i = wtmp * wk2r;
+            wtmp = 2 * wk4i;
+            wk5r = wk3r - wtmp * wk1i;
+            wk5i = wtmp * wk1r - wk3i;
+            wk6r = wk2r - wtmp * wk2i;
+            wk6i = wtmp * wk2r - wk2i;
+            wk7r = wk1r - wtmp * wk3i;
+            wk7i = wtmp * wk3r - wk1i;
+            for (j = k; j < l + k; j += 2) {
+                j1 = j + l;
+                j2 = j1 + l;
+                j3 = j2 + l;
+                j4 = j3 + l;
+                j5 = j4 + l;
+                j6 = j5 + l;
+                j7 = j6 + l;
+                x0r = a[j] + a[j1];
+                x0i = a[j + 1] + a[j1 + 1];
+                x1r = a[j] - a[j1];
+                x1i = a[j + 1] - a[j1 + 1];
+                x2r = a[j2] + a[j3];
+                x2i = a[j2 + 1] + a[j3 + 1];
+                x3r = a[j2] - a[j3];
+                x3i = a[j2 + 1] - a[j3 + 1];
+                y0r = x0r + x2r;
+                y0i = x0i + x2i;
+                y2r = x0r - x2r;
+                y2i = x0i - x2i;
+                y1r = x1r - x3i;
+                y1i = x1i + x3r;
+                y3r = x1r + x3i;
+                y3i = x1i - x3r;
+                x0r = a[j4] + a[j5];
+                x0i = a[j4 + 1] + a[j5 + 1];
+                x1r = a[j4] - a[j5];
+                x1i = a[j4 + 1] - a[j5 + 1];
+                x2r = a[j6] + a[j7];
+                x2i = a[j6 + 1] + a[j7 + 1];
+                x3r = a[j6] - a[j7];
+                x3i = a[j6 + 1] - a[j7 + 1];
+                y4r = x0r + x2r;
+                y4i = x0i + x2i;
+                y6r = x0r - x2r;
+                y6i = x0i - x2i;
+                x0r = x1r - x3i;
+                x0i = x1i + x3r;
+                x2r = x1r + x3i;
+                x2i = x1i - x3r;
+                y5r = wn4r * (x0r - x0i);
+                y5i = wn4r * (x0r + x0i);
+                y7r = wn4r * (x2r - x2i);
+                y7i = wn4r * (x2r + x2i);
+                x0r = y1r + y5r;
+                x0i = y1i + y5i;
+                a[j1] = wk1r * x0r - wk1i * x0i;
+                a[j1 + 1] = wk1r * x0i + wk1i * x0r;
+                x0r = y1r - y5r;
+                x0i = y1i - y5i;
+                a[j5] = wk5r * x0r - wk5i * x0i;
+                a[j5 + 1] = wk5r * x0i + wk5i * x0r;
+                x0r = y3r - y7i;
+                x0i = y3i + y7r;
+                a[j3] = wk3r * x0r - wk3i * x0i;
+                a[j3 + 1] = wk3r * x0i + wk3i * x0r;
+                x0r = y3r + y7i;
+                x0i = y3i - y7r;
+                a[j7] = wk7r * x0r - wk7i * x0i;
+                a[j7 + 1] = wk7r * x0i + wk7i * x0r;
+                a[j] = y0r + y4r;
+                a[j + 1] = y0i + y4i;
+                x0r = y0r - y4r;
+                x0i = y0i - y4i;
+                a[j4] = wk4r * x0r - wk4i * x0i;
+                a[j4 + 1] = wk4r * x0i + wk4i * x0r;
+                x0r = y2r - y6i;
+                x0i = y2i + y6r;
+                a[j2] = wk2r * x0r - wk2i * x0i;
+                a[j2 + 1] = wk2r * x0i + wk2i * x0r;
+                x0r = y2r + y6i;
+                x0i = y2i - y6r;
+                a[j6] = wk6r * x0r - wk6i * x0i;
+                a[j6 + 1] = wk6r * x0i + wk6i * x0r;
+            }
+        }
+    }
+}
+
+
+void rftfsub(int n, double *a)
+{
+    int i, i0, j, k;
+    double ec, w1r, w1i, wkr, wki, wdr, wdi, ss, xr, xi, yr, yi;
+    
+    ec = 2 * M_PI_2 / n;
+    wkr = 0;
+    wki = 0;
+    wdi = cos(ec);
+    wdr = sin(ec);
+    wdi *= wdr;
+    wdr *= wdr;
+    w1r = 1 - 2 * wdr;
+    w1i = 2 * wdi;
+    ss = 2 * w1i;
+    i = n >> 1;
+    for (;;) {
+        i0 = i - 4 * RDFT_LOOP_DIV;
+        if (i0 < 4) {
+            i0 = 4;
+        }
+        for (j = i - 4; j >= i0; j -= 4) {
+            k = n - j;
+            xr = a[j + 2] - a[k - 2];
+            xi = a[j + 3] + a[k - 1];
+            yr = wdr * xr - wdi * xi;
+            yi = wdr * xi + wdi * xr;
+            a[j + 2] -= yr;
+            a[j + 3] -= yi;
+            a[k - 2] += yr;
+            a[k - 1] -= yi;
+            wkr += ss * wdi;
+            wki += ss * (0.5 - wdr);
+            xr = a[j] - a[k];
+            xi = a[j + 1] + a[k + 1];
+            yr = wkr * xr - wki * xi;
+            yi = wkr * xi + wki * xr;
+            a[j] -= yr;
+            a[j + 1] -= yi;
+            a[k] += yr;
+            a[k + 1] -= yi;
+            wdr += ss * wki;
+            wdi += ss * (0.5 - wkr);
+        }
+        if (i0 == 4) {
+            break;
+        }
+        wkr = 0.5 * sin(ec * i0);
+        wki = 0.5 * cos(ec * i0);
+        wdr = 0.5 - (wkr * w1r - wki * w1i);
+        wdi = wkr * w1i + wki * w1r;
+        wkr = 0.5 - wkr;
+        i = i0;
+    }
+    xr = a[2] - a[n - 2];
+    xi = a[3] + a[n - 1];
+    yr = wdr * xr - wdi * xi;
+    yi = wdr * xi + wdi * xr;
+    a[2] -= yr;
+    a[3] -= yi;
+    a[n - 2] += yr;
+    a[n - 1] -= yi;
+}
+
+
+void rftbsub(int n, double *a)
+{
+    int i, i0, j, k;
+    double ec, w1r, w1i, wkr, wki, wdr, wdi, ss, xr, xi, yr, yi;
+    
+    ec = 2 * M_PI_2 / n;
+    wkr = 0;
+    wki = 0;
+    wdi = cos(ec);
+    wdr = sin(ec);
+    wdi *= wdr;
+    wdr *= wdr;
+    w1r = 1 - 2 * wdr;
+    w1i = 2 * wdi;
+    ss = 2 * w1i;
+    i = n >> 1;
+    a[i + 1] = -a[i + 1];
+    for (;;) {
+        i0 = i - 4 * RDFT_LOOP_DIV;
+        if (i0 < 4) {
+            i0 = 4;
+        }
+        for (j = i - 4; j >= i0; j -= 4) {
+            k = n - j;
+            xr = a[j + 2] - a[k - 2];
+            xi = a[j + 3] + a[k - 1];
+            yr = wdr * xr + wdi * xi;
+            yi = wdr * xi - wdi * xr;
+            a[j + 2] -= yr;
+            a[j + 3] = yi - a[j + 3];
+            a[k - 2] += yr;
+            a[k - 1] = yi - a[k - 1];
+            wkr += ss * wdi;
+            wki += ss * (0.5 - wdr);
+            xr = a[j] - a[k];
+            xi = a[j + 1] + a[k + 1];
+            yr = wkr * xr + wki * xi;
+            yi = wkr * xi - wki * xr;
+            a[j] -= yr;
+            a[j + 1] = yi - a[j + 1];
+            a[k] += yr;
+            a[k + 1] = yi - a[k + 1];
+            wdr += ss * wki;
+            wdi += ss * (0.5 - wkr);
+        }
+        if (i0 == 4) {
+            break;
+        }
+        wkr = 0.5 * sin(ec * i0);
+        wki = 0.5 * cos(ec * i0);
+        wdr = 0.5 - (wkr * w1r - wki * w1i);
+        wdi = wkr * w1i + wki * w1r;
+        wkr = 0.5 - wkr;
+        i = i0;
+    }
+    xr = a[2] - a[n - 2];
+    xi = a[3] + a[n - 1];
+    yr = wdr * xr + wdi * xi;
+    yi = wdr * xi - wdi * xr;
+    a[2] -= yr;
+    a[3] = yi - a[3];
+    a[n - 2] += yr;
+    a[n - 1] = yi - a[n - 1];
+    a[1] = -a[1];
+}
+
+
+void dctsub(int n, double *a)
+{
+    int i, i0, j, k, m;
+    double ec, w1r, w1i, wkr, wki, wdr, wdi, ss, xr, xi, yr, yi;
+    
+    ec = M_PI_2 / n;
+    wkr = 0.5;
+    wki = 0.5;
+    w1r = cos(ec);
+    w1i = sin(ec);
+    wdr = 0.5 * (w1r - w1i);
+    wdi = 0.5 * (w1r + w1i);
+    ss = 2 * w1i;
+    m = n >> 1;
+    i = 0;
+    for (;;) {
+        i0 = i + 2 * DCST_LOOP_DIV;
+        if (i0 > m - 2) {
+            i0 = m - 2;
+        }
+        for (j = i + 2; j <= i0; j += 2) {
+            k = n - j;
+            xr = wdi * a[j - 1] - wdr * a[k + 1];
+            xi = wdr * a[j - 1] + wdi * a[k + 1];
+            wkr -= ss * wdi;
+            wki += ss * wdr;
+            yr = wki * a[j] - wkr * a[k];
+            yi = wkr * a[j] + wki * a[k];
+            wdr -= ss * wki;
+            wdi += ss * wkr;
+            a[k + 1] = xr;
+            a[k] = yr;
+            a[j - 1] = xi;
+            a[j] = yi;
+        }
+        if (i0 == m - 2) {
+            break;
+        }
+        wdr = cos(ec * i0);
+        wdi = sin(ec * i0);
+        wkr = 0.5 * (wdr - wdi);
+        wki = 0.5 * (wdr + wdi);
+        wdr = wkr * w1r - wki * w1i;
+        wdi = wkr * w1i + wki * w1r;
+        i = i0;
+    }
+    xr = wdi * a[m - 1] - wdr * a[m + 1];
+    a[m - 1] = wdr * a[m - 1] + wdi * a[m + 1];
+    a[m + 1] = xr;
+    a[m] *= wki + ss * wdr;
+}
+
+
+void dstsub(int n, double *a)
+{
+    int i, i0, j, k, m;
+    double ec, w1r, w1i, wkr, wki, wdr, wdi, ss, xr, xi, yr, yi;
+    
+    ec = M_PI_2 / n;
+    wkr = 0.5;
+    wki = 0.5;
+    w1r = cos(ec);
+    w1i = sin(ec);
+    wdr = 0.5 * (w1r - w1i);
+    wdi = 0.5 * (w1r + w1i);
+    ss = 2 * w1i;
+    m = n >> 1;
+    i = 0;
+    for (;;) {
+        i0 = i + 2 * DCST_LOOP_DIV;
+        if (i0 > m - 2) {
+            i0 = m - 2;
+        }
+        for (j = i + 2; j <= i0; j += 2) {
+            k = n - j;
+            xr = wdi * a[k + 1] - wdr * a[j - 1];
+            xi = wdr * a[k + 1] + wdi * a[j - 1];
+            wkr -= ss * wdi;
+            wki += ss * wdr;
+            yr = wki * a[k] - wkr * a[j];
+            yi = wkr * a[k] + wki * a[j];
+            wdr -= ss * wki;
+            wdi += ss * wkr;
+            a[j - 1] = xr;
+            a[j] = yr;
+            a[k + 1] = xi;
+            a[k] = yi;
+        }
+        if (i0 == m - 2) {
+            break;
+        }
+        wdr = cos(ec * i0);
+        wdi = sin(ec * i0);
+        wkr = 0.5 * (wdr - wdi);
+        wki = 0.5 * (wdr + wdi);
+        wdr = wkr * w1r - wki * w1i;
+        wdi = wkr * w1i + wki * w1r;
+        i = i0;
+    }
+    xr = wdi * a[m + 1] - wdr * a[m - 1];
+    a[m + 1] = wdr * a[m + 1] + wdi * a[m - 1];
+    a[m - 1] = xr;
+    a[m] *= wki + ss * wdr;
+}
+
+
+void dctsub4(int n, double *a)
+{
+    int m;
+    double wki, wdr, wdi, xr;
+    
+    wki = WR5000;
+    m = n >> 1;
+    if (m == 2) {
+        wdr = wki * WI2500;
+        wdi = wki * WR2500;
+        xr = wdi * a[1] - wdr * a[3];
+        a[1] = wdr * a[1] + wdi * a[3];
+        a[3] = xr;
+    }
+    a[m] *= wki;
+}
+
+
+void dstsub4(int n, double *a)
+{
+    int m;
+    double wki, wdr, wdi, xr;
+    
+    wki = WR5000;
+    m = n >> 1;
+    if (m == 2) {
+        wdr = wki * WI2500;
+        wdi = wki * WR2500;
+        xr = wdi * a[3] - wdr * a[1];
+        a[3] = wdr * a[3] + wdi * a[1];
+        a[1] = xr;
+    }
+    a[m] *= wki;
+}
+
diff --git a/third_party/tflite-micro/third_party/fft2d/fftsg.c b/third_party/tflite-micro/third_party/fft2d/fftsg.c
new file mode 100644
index 00000000..43d75344
--- /dev/null
+++ b/third_party/tflite-micro/third_party/fft2d/fftsg.c
@@ -0,0 +1,3314 @@
+/*
+Fast Fourier/Cosine/Sine Transform
+    dimension   :one
+    data length :power of 2
+    decimation  :frequency
+    radix       :split-radix
+    data        :inplace
+    table       :use
+functions
+    cdft: Complex Discrete Fourier Transform
+    rdft: Real Discrete Fourier Transform
+    ddct: Discrete Cosine Transform
+    ddst: Discrete Sine Transform
+    dfct: Cosine Transform of RDFT (Real Symmetric DFT)
+    dfst: Sine Transform of RDFT (Real Anti-symmetric DFT)
+function prototypes
+    void cdft(int, int, double *, int *, double *);
+    void rdft(int, int, double *, int *, double *);
+    void ddct(int, int, double *, int *, double *);
+    void ddst(int, int, double *, int *, double *);
+    void dfct(int, double *, double *, int *, double *);
+    void dfst(int, double *, double *, int *, double *);
+macro definitions
+    USE_CDFT_PTHREADS : default=not defined
+        CDFT_THREADS_BEGIN_N  : must be >= 512, default=8192
+        CDFT_4THREADS_BEGIN_N : must be >= 512, default=65536
+    USE_CDFT_WINTHREADS : default=not defined
+        CDFT_THREADS_BEGIN_N  : must be >= 512, default=32768
+        CDFT_4THREADS_BEGIN_N : must be >= 512, default=524288
+
+
+-------- Complex DFT (Discrete Fourier Transform) --------
+    [definition]
+        <case1>
+            X[k] = sum_j=0^n-1 x[j]*exp(2*pi*i*j*k/n), 0<=k<n
+        <case2>
+            X[k] = sum_j=0^n-1 x[j]*exp(-2*pi*i*j*k/n), 0<=k<n
+        (notes: sum_j=0^n-1 is a summation from j=0 to n-1)
+    [usage]
+        <case1>
+            ip[0] = 0; // first time only
+            cdft(2*n, 1, a, ip, w);
+        <case2>
+            ip[0] = 0; // first time only
+            cdft(2*n, -1, a, ip, w);
+    [parameters]
+        2*n            :data length (int)
+                        n >= 1, n = power of 2
+        a[0...2*n-1]   :input/output data (double *)
+                        input data
+                            a[2*j] = Re(x[j]), 
+                            a[2*j+1] = Im(x[j]), 0<=j<n
+                        output data
+                            a[2*k] = Re(X[k]), 
+                            a[2*k+1] = Im(X[k]), 0<=k<n
+        ip[0...*]      :work area for bit reversal (int *)
+                        length of ip >= 2+sqrt(n)
+                        strictly, 
+                        length of ip >= 
+                            2+(1<<(int)(log(n+0.5)/log(2))/2).
+                        ip[0],ip[1] are pointers of the cos/sin table.
+        w[0...n/2-1]   :cos/sin table (double *)
+                        w[],ip[] are initialized if ip[0] == 0.
+    [remark]
+        Inverse of 
+            cdft(2*n, -1, a, ip, w);
+        is 
+            cdft(2*n, 1, a, ip, w);
+            for (j = 0; j <= 2 * n - 1; j++) {
+                a[j] *= 1.0 / n;
+            }
+        .
+
+
+-------- Real DFT / Inverse of Real DFT --------
+    [definition]
+        <case1> RDFT
+            R[k] = sum_j=0^n-1 a[j]*cos(2*pi*j*k/n), 0<=k<=n/2
+            I[k] = sum_j=0^n-1 a[j]*sin(2*pi*j*k/n), 0<k<n/2
+        <case2> IRDFT (excluding scale)
+            a[k] = (R[0] + R[n/2]*cos(pi*k))/2 + 
+                   sum_j=1^n/2-1 R[j]*cos(2*pi*j*k/n) + 
+                   sum_j=1^n/2-1 I[j]*sin(2*pi*j*k/n), 0<=k<n
+    [usage]
+        <case1>
+            ip[0] = 0; // first time only
+            rdft(n, 1, a, ip, w);
+        <case2>
+            ip[0] = 0; // first time only
+            rdft(n, -1, a, ip, w);
+    [parameters]
+        n              :data length (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        <case1>
+                            output data
+                                a[2*k] = R[k], 0<=k<n/2
+                                a[2*k+1] = I[k], 0<k<n/2
+                                a[1] = R[n/2]
+                        <case2>
+                            input data
+                                a[2*j] = R[j], 0<=j<n/2
+                                a[2*j+1] = I[j], 0<j<n/2
+                                a[1] = R[n/2]
+        ip[0...*]      :work area for bit reversal (int *)
+                        length of ip >= 2+sqrt(n/2)
+                        strictly, 
+                        length of ip >= 
+                            2+(1<<(int)(log(n/2+0.5)/log(2))/2).
+                        ip[0],ip[1] are pointers of the cos/sin table.
+        w[0...n/2-1]   :cos/sin table (double *)
+                        w[],ip[] are initialized if ip[0] == 0.
+    [remark]
+        Inverse of 
+            rdft(n, 1, a, ip, w);
+        is 
+            rdft(n, -1, a, ip, w);
+            for (j = 0; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- DCT (Discrete Cosine Transform) / Inverse of DCT --------
+    [definition]
+        <case1> IDCT (excluding scale)
+            C[k] = sum_j=0^n-1 a[j]*cos(pi*j*(k+1/2)/n), 0<=k<n
+        <case2> DCT
+            C[k] = sum_j=0^n-1 a[j]*cos(pi*(j+1/2)*k/n), 0<=k<n
+    [usage]
+        <case1>
+            ip[0] = 0; // first time only
+            ddct(n, 1, a, ip, w);
+        <case2>
+            ip[0] = 0; // first time only
+            ddct(n, -1, a, ip, w);
+    [parameters]
+        n              :data length (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        output data
+                            a[k] = C[k], 0<=k<n
+        ip[0...*]      :work area for bit reversal (int *)
+                        length of ip >= 2+sqrt(n/2)
+                        strictly, 
+                        length of ip >= 
+                            2+(1<<(int)(log(n/2+0.5)/log(2))/2).
+                        ip[0],ip[1] are pointers of the cos/sin table.
+        w[0...n*5/4-1] :cos/sin table (double *)
+                        w[],ip[] are initialized if ip[0] == 0.
+    [remark]
+        Inverse of 
+            ddct(n, -1, a, ip, w);
+        is 
+            a[0] *= 0.5;
+            ddct(n, 1, a, ip, w);
+            for (j = 0; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- DST (Discrete Sine Transform) / Inverse of DST --------
+    [definition]
+        <case1> IDST (excluding scale)
+            S[k] = sum_j=1^n A[j]*sin(pi*j*(k+1/2)/n), 0<=k<n
+        <case2> DST
+            S[k] = sum_j=0^n-1 a[j]*sin(pi*(j+1/2)*k/n), 0<k<=n
+    [usage]
+        <case1>
+            ip[0] = 0; // first time only
+            ddst(n, 1, a, ip, w);
+        <case2>
+            ip[0] = 0; // first time only
+            ddst(n, -1, a, ip, w);
+    [parameters]
+        n              :data length (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        <case1>
+                            input data
+                                a[j] = A[j], 0<j<n
+                                a[0] = A[n]
+                            output data
+                                a[k] = S[k], 0<=k<n
+                        <case2>
+                            output data
+                                a[k] = S[k], 0<k<n
+                                a[0] = S[n]
+        ip[0...*]      :work area for bit reversal (int *)
+                        length of ip >= 2+sqrt(n/2)
+                        strictly, 
+                        length of ip >= 
+                            2+(1<<(int)(log(n/2+0.5)/log(2))/2).
+                        ip[0],ip[1] are pointers of the cos/sin table.
+        w[0...n*5/4-1] :cos/sin table (double *)
+                        w[],ip[] are initialized if ip[0] == 0.
+    [remark]
+        Inverse of 
+            ddst(n, -1, a, ip, w);
+        is 
+            a[0] *= 0.5;
+            ddst(n, 1, a, ip, w);
+            for (j = 0; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- Cosine Transform of RDFT (Real Symmetric DFT) --------
+    [definition]
+        C[k] = sum_j=0^n a[j]*cos(pi*j*k/n), 0<=k<=n
+    [usage]
+        ip[0] = 0; // first time only
+        dfct(n, a, t, ip, w);
+    [parameters]
+        n              :data length - 1 (int)
+                        n >= 2, n = power of 2
+        a[0...n]       :input/output data (double *)
+                        output data
+                            a[k] = C[k], 0<=k<=n
+        t[0...n/2]     :work area (double *)
+        ip[0...*]      :work area for bit reversal (int *)
+                        length of ip >= 2+sqrt(n/4)
+                        strictly, 
+                        length of ip >= 
+                            2+(1<<(int)(log(n/4+0.5)/log(2))/2).
+                        ip[0],ip[1] are pointers of the cos/sin table.
+        w[0...n*5/8-1] :cos/sin table (double *)
+                        w[],ip[] are initialized if ip[0] == 0.
+    [remark]
+        Inverse of 
+            a[0] *= 0.5;
+            a[n] *= 0.5;
+            dfct(n, a, t, ip, w);
+        is 
+            a[0] *= 0.5;
+            a[n] *= 0.5;
+            dfct(n, a, t, ip, w);
+            for (j = 0; j <= n; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- Sine Transform of RDFT (Real Anti-symmetric DFT) --------
+    [definition]
+        S[k] = sum_j=1^n-1 a[j]*sin(pi*j*k/n), 0<k<n
+    [usage]
+        ip[0] = 0; // first time only
+        dfst(n, a, t, ip, w);
+    [parameters]
+        n              :data length + 1 (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        output data
+                            a[k] = S[k], 0<k<n
+                        (a[0] is used for work area)
+        t[0...n/2-1]   :work area (double *)
+        ip[0...*]      :work area for bit reversal (int *)
+                        length of ip >= 2+sqrt(n/4)
+                        strictly, 
+                        length of ip >= 
+                            2+(1<<(int)(log(n/4+0.5)/log(2))/2).
+                        ip[0],ip[1] are pointers of the cos/sin table.
+        w[0...n*5/8-1] :cos/sin table (double *)
+                        w[],ip[] are initialized if ip[0] == 0.
+    [remark]
+        Inverse of 
+            dfst(n, a, t, ip, w);
+        is 
+            dfst(n, a, t, ip, w);
+            for (j = 1; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+Appendix :
+    The cos/sin table is recalculated when the larger table required.
+    w[] and ip[] are compatible with all routines.
+*/
+
+
+void cdft(int n, int isgn, double *a, int *ip, double *w)
+{
+    void makewt(int nw, int *ip, double *w);
+    void cftfsub(int n, double *a, int *ip, int nw, double *w);
+    void cftbsub(int n, double *a, int *ip, int nw, double *w);
+    int nw;
+    
+    nw = ip[0];
+    if (n > (nw << 2)) {
+        nw = n >> 2;
+        makewt(nw, ip, w);
+    }
+    if (isgn >= 0) {
+        cftfsub(n, a, ip, nw, w);
+    } else {
+        cftbsub(n, a, ip, nw, w);
+    }
+}
+
+
+void rdft(int n, int isgn, double *a, int *ip, double *w)
+{
+    void makewt(int nw, int *ip, double *w);
+    void makect(int nc, int *ip, double *c);
+    void cftfsub(int n, double *a, int *ip, int nw, double *w);
+    void cftbsub(int n, double *a, int *ip, int nw, double *w);
+    void rftfsub(int n, double *a, int nc, double *c);
+    void rftbsub(int n, double *a, int nc, double *c);
+    int nw, nc;
+    double xi;
+    
+    nw = ip[0];
+    if (n > (nw << 2)) {
+        nw = n >> 2;
+        makewt(nw, ip, w);
+    }
+    nc = ip[1];
+    if (n > (nc << 2)) {
+        nc = n >> 2;
+        makect(nc, ip, w + nw);
+    }
+    if (isgn >= 0) {
+        if (n > 4) {
+            cftfsub(n, a, ip, nw, w);
+            rftfsub(n, a, nc, w + nw);
+        } else if (n == 4) {
+            cftfsub(n, a, ip, nw, w);
+        }
+        xi = a[0] - a[1];
+        a[0] += a[1];
+        a[1] = xi;
+    } else {
+        a[1] = 0.5 * (a[0] - a[1]);
+        a[0] -= a[1];
+        if (n > 4) {
+            rftbsub(n, a, nc, w + nw);
+            cftbsub(n, a, ip, nw, w);
+        } else if (n == 4) {
+            cftbsub(n, a, ip, nw, w);
+        }
+    }
+}
+
+
+void ddct(int n, int isgn, double *a, int *ip, double *w)
+{
+    void makewt(int nw, int *ip, double *w);
+    void makect(int nc, int *ip, double *c);
+    void cftfsub(int n, double *a, int *ip, int nw, double *w);
+    void cftbsub(int n, double *a, int *ip, int nw, double *w);
+    void rftfsub(int n, double *a, int nc, double *c);
+    void rftbsub(int n, double *a, int nc, double *c);
+    void dctsub(int n, double *a, int nc, double *c);
+    int j, nw, nc;
+    double xr;
+    
+    nw = ip[0];
+    if (n > (nw << 2)) {
+        nw = n >> 2;
+        makewt(nw, ip, w);
+    }
+    nc = ip[1];
+    if (n > nc) {
+        nc = n;
+        makect(nc, ip, w + nw);
+    }
+    if (isgn < 0) {
+        xr = a[n - 1];
+        for (j = n - 2; j >= 2; j -= 2) {
+            a[j + 1] = a[j] - a[j - 1];
+            a[j] += a[j - 1];
+        }
+        a[1] = a[0] - xr;
+        a[0] += xr;
+        if (n > 4) {
+            rftbsub(n, a, nc, w + nw);
+            cftbsub(n, a, ip, nw, w);
+        } else if (n == 4) {
+            cftbsub(n, a, ip, nw, w);
+        }
+    }
+    dctsub(n, a, nc, w + nw);
+    if (isgn >= 0) {
+        if (n > 4) {
+            cftfsub(n, a, ip, nw, w);
+            rftfsub(n, a, nc, w + nw);
+        } else if (n == 4) {
+            cftfsub(n, a, ip, nw, w);
+        }
+        xr = a[0] - a[1];
+        a[0] += a[1];
+        for (j = 2; j < n; j += 2) {
+            a[j - 1] = a[j] - a[j + 1];
+            a[j] += a[j + 1];
+        }
+        a[n - 1] = xr;
+    }
+}
+
+
+void ddst(int n, int isgn, double *a, int *ip, double *w)
+{
+    void makewt(int nw, int *ip, double *w);
+    void makect(int nc, int *ip, double *c);
+    void cftfsub(int n, double *a, int *ip, int nw, double *w);
+    void cftbsub(int n, double *a, int *ip, int nw, double *w);
+    void rftfsub(int n, double *a, int nc, double *c);
+    void rftbsub(int n, double *a, int nc, double *c);
+    void dstsub(int n, double *a, int nc, double *c);
+    int j, nw, nc;
+    double xr;
+    
+    nw = ip[0];
+    if (n > (nw << 2)) {
+        nw = n >> 2;
+        makewt(nw, ip, w);
+    }
+    nc = ip[1];
+    if (n > nc) {
+        nc = n;
+        makect(nc, ip, w + nw);
+    }
+    if (isgn < 0) {
+        xr = a[n - 1];
+        for (j = n - 2; j >= 2; j -= 2) {
+            a[j + 1] = -a[j] - a[j - 1];
+            a[j] -= a[j - 1];
+        }
+        a[1] = a[0] + xr;
+        a[0] -= xr;
+        if (n > 4) {
+            rftbsub(n, a, nc, w + nw);
+            cftbsub(n, a, ip, nw, w);
+        } else if (n == 4) {
+            cftbsub(n, a, ip, nw, w);
+        }
+    }
+    dstsub(n, a, nc, w + nw);
+    if (isgn >= 0) {
+        if (n > 4) {
+            cftfsub(n, a, ip, nw, w);
+            rftfsub(n, a, nc, w + nw);
+        } else if (n == 4) {
+            cftfsub(n, a, ip, nw, w);
+        }
+        xr = a[0] - a[1];
+        a[0] += a[1];
+        for (j = 2; j < n; j += 2) {
+            a[j - 1] = -a[j] - a[j + 1];
+            a[j] -= a[j + 1];
+        }
+        a[n - 1] = -xr;
+    }
+}
+
+
+void dfct(int n, double *a, double *t, int *ip, double *w)
+{
+    void makewt(int nw, int *ip, double *w);
+    void makect(int nc, int *ip, double *c);
+    void cftfsub(int n, double *a, int *ip, int nw, double *w);
+    void rftfsub(int n, double *a, int nc, double *c);
+    void dctsub(int n, double *a, int nc, double *c);
+    int j, k, l, m, mh, nw, nc;
+    double xr, xi, yr, yi;
+    
+    nw = ip[0];
+    if (n > (nw << 3)) {
+        nw = n >> 3;
+        makewt(nw, ip, w);
+    }
+    nc = ip[1];
+    if (n > (nc << 1)) {
+        nc = n >> 1;
+        makect(nc, ip, w + nw);
+    }
+    m = n >> 1;
+    yi = a[m];
+    xi = a[0] + a[n];
+    a[0] -= a[n];
+    t[0] = xi - yi;
+    t[m] = xi + yi;
+    if (n > 2) {
+        mh = m >> 1;
+        for (j = 1; j < mh; j++) {
+            k = m - j;
+            xr = a[j] - a[n - j];
+            xi = a[j] + a[n - j];
+            yr = a[k] - a[n - k];
+            yi = a[k] + a[n - k];
+            a[j] = xr;
+            a[k] = yr;
+            t[j] = xi - yi;
+            t[k] = xi + yi;
+        }
+        t[mh] = a[mh] + a[n - mh];
+        a[mh] -= a[n - mh];
+        dctsub(m, a, nc, w + nw);
+        if (m > 4) {
+            cftfsub(m, a, ip, nw, w);
+            rftfsub(m, a, nc, w + nw);
+        } else if (m == 4) {
+            cftfsub(m, a, ip, nw, w);
+        }
+        a[n - 1] = a[0] - a[1];
+        a[1] = a[0] + a[1];
+        for (j = m - 2; j >= 2; j -= 2) {
+            a[2 * j + 1] = a[j] + a[j + 1];
+            a[2 * j - 1] = a[j] - a[j + 1];
+        }
+        l = 2;
+        m = mh;
+        while (m >= 2) {
+            dctsub(m, t, nc, w + nw);
+            if (m > 4) {
+                cftfsub(m, t, ip, nw, w);
+                rftfsub(m, t, nc, w + nw);
+            } else if (m == 4) {
+                cftfsub(m, t, ip, nw, w);
+            }
+            a[n - l] = t[0] - t[1];
+            a[l] = t[0] + t[1];
+            k = 0;
+            for (j = 2; j < m; j += 2) {
+                k += l << 2;
+                a[k - l] = t[j] - t[j + 1];
+                a[k + l] = t[j] + t[j + 1];
+            }
+            l <<= 1;
+            mh = m >> 1;
+            for (j = 0; j < mh; j++) {
+                k = m - j;
+                t[j] = t[m + k] - t[m + j];
+                t[k] = t[m + k] + t[m + j];
+            }
+            t[mh] = t[m + mh];
+            m = mh;
+        }
+        a[l] = t[0];
+        a[n] = t[2] - t[1];
+        a[0] = t[2] + t[1];
+    } else {
+        a[1] = a[0];
+        a[2] = t[0];
+        a[0] = t[1];
+    }
+}
+
+
+void dfst(int n, double *a, double *t, int *ip, double *w)
+{
+    void makewt(int nw, int *ip, double *w);
+    void makect(int nc, int *ip, double *c);
+    void cftfsub(int n, double *a, int *ip, int nw, double *w);
+    void rftfsub(int n, double *a, int nc, double *c);
+    void dstsub(int n, double *a, int nc, double *c);
+    int j, k, l, m, mh, nw, nc;
+    double xr, xi, yr, yi;
+    
+    nw = ip[0];
+    if (n > (nw << 3)) {
+        nw = n >> 3;
+        makewt(nw, ip, w);
+    }
+    nc = ip[1];
+    if (n > (nc << 1)) {
+        nc = n >> 1;
+        makect(nc, ip, w + nw);
+    }
+    if (n > 2) {
+        m = n >> 1;
+        mh = m >> 1;
+        for (j = 1; j < mh; j++) {
+            k = m - j;
+            xr = a[j] + a[n - j];
+            xi = a[j] - a[n - j];
+            yr = a[k] + a[n - k];
+            yi = a[k] - a[n - k];
+            a[j] = xr;
+            a[k] = yr;
+            t[j] = xi + yi;
+            t[k] = xi - yi;
+        }
+        t[0] = a[mh] - a[n - mh];
+        a[mh] += a[n - mh];
+        a[0] = a[m];
+        dstsub(m, a, nc, w + nw);
+        if (m > 4) {
+            cftfsub(m, a, ip, nw, w);
+            rftfsub(m, a, nc, w + nw);
+        } else if (m == 4) {
+            cftfsub(m, a, ip, nw, w);
+        }
+        a[n - 1] = a[1] - a[0];
+        a[1] = a[0] + a[1];
+        for (j = m - 2; j >= 2; j -= 2) {
+            a[2 * j + 1] = a[j] - a[j + 1];
+            a[2 * j - 1] = -a[j] - a[j + 1];
+        }
+        l = 2;
+        m = mh;
+        while (m >= 2) {
+            dstsub(m, t, nc, w + nw);
+            if (m > 4) {
+                cftfsub(m, t, ip, nw, w);
+                rftfsub(m, t, nc, w + nw);
+            } else if (m == 4) {
+                cftfsub(m, t, ip, nw, w);
+            }
+            a[n - l] = t[1] - t[0];
+            a[l] = t[0] + t[1];
+            k = 0;
+            for (j = 2; j < m; j += 2) {
+                k += l << 2;
+                a[k - l] = -t[j] - t[j + 1];
+                a[k + l] = t[j] - t[j + 1];
+            }
+            l <<= 1;
+            mh = m >> 1;
+            for (j = 1; j < mh; j++) {
+                k = m - j;
+                t[j] = t[m + k] + t[m + j];
+                t[k] = t[m + k] - t[m + j];
+            }
+            t[0] = t[m + mh];
+            m = mh;
+        }
+        a[l] = t[0];
+    }
+    a[0] = 0;
+}
+
+
+/* -------- initializing routines -------- */
+
+
+#include <math.h>
+
+void makewt(int nw, int *ip, double *w)
+{
+    void makeipt(int nw, int *ip);
+    int j, nwh, nw0, nw1;
+    double delta, wn4r, wk1r, wk1i, wk3r, wk3i;
+    
+    ip[0] = nw;
+    ip[1] = 1;
+    if (nw > 2) {
+        nwh = nw >> 1;
+        delta = atan(1.0) / nwh;
+        wn4r = cos(delta * nwh);
+        w[0] = 1;
+        w[1] = wn4r;
+        if (nwh == 4) {
+            w[2] = cos(delta * 2);
+            w[3] = sin(delta * 2);
+        } else if (nwh > 4) {
+            makeipt(nw, ip);
+            w[2] = 0.5 / cos(delta * 2);
+            w[3] = 0.5 / cos(delta * 6);
+            for (j = 4; j < nwh; j += 4) {
+                w[j] = cos(delta * j);
+                w[j + 1] = sin(delta * j);
+                w[j + 2] = cos(3 * delta * j);
+                w[j + 3] = -sin(3 * delta * j);
+            }
+        }
+        nw0 = 0;
+        while (nwh > 2) {
+            nw1 = nw0 + nwh;
+            nwh >>= 1;
+            w[nw1] = 1;
+            w[nw1 + 1] = wn4r;
+            if (nwh == 4) {
+                wk1r = w[nw0 + 4];
+                wk1i = w[nw0 + 5];
+                w[nw1 + 2] = wk1r;
+                w[nw1 + 3] = wk1i;
+            } else if (nwh > 4) {
+                wk1r = w[nw0 + 4];
+                wk3r = w[nw0 + 6];
+                w[nw1 + 2] = 0.5 / wk1r;
+                w[nw1 + 3] = 0.5 / wk3r;
+                for (j = 4; j < nwh; j += 4) {
+                    wk1r = w[nw0 + 2 * j];
+                    wk1i = w[nw0 + 2 * j + 1];
+                    wk3r = w[nw0 + 2 * j + 2];
+                    wk3i = w[nw0 + 2 * j + 3];
+                    w[nw1 + j] = wk1r;
+                    w[nw1 + j + 1] = wk1i;
+                    w[nw1 + j + 2] = wk3r;
+                    w[nw1 + j + 3] = wk3i;
+                }
+            }
+            nw0 = nw1;
+        }
+    }
+}
+
+
+void makeipt(int nw, int *ip)
+{
+    int j, l, m, m2, p, q;
+    
+    ip[2] = 0;
+    ip[3] = 16;
+    m = 2;
+    for (l = nw; l > 32; l >>= 2) {
+        m2 = m << 1;
+        q = m2 << 3;
+        for (j = m; j < m2; j++) {
+            p = ip[j] << 2;
+            ip[m + j] = p;
+            ip[m2 + j] = p + q;
+        }
+        m = m2;
+    }
+}
+
+
+void makect(int nc, int *ip, double *c)
+{
+    int j, nch;
+    double delta;
+    
+    ip[1] = nc;
+    if (nc > 1) {
+        nch = nc >> 1;
+        delta = atan(1.0) / nch;
+        c[0] = cos(delta * nch);
+        c[nch] = 0.5 * c[0];
+        for (j = 1; j < nch; j++) {
+            c[j] = 0.5 * cos(delta * j);
+            c[nc - j] = 0.5 * sin(delta * j);
+        }
+    }
+}
+
+
+/* -------- child routines -------- */
+
+
+#ifdef USE_CDFT_PTHREADS
+#define USE_CDFT_THREADS
+#ifndef CDFT_THREADS_BEGIN_N
+#define CDFT_THREADS_BEGIN_N 8192
+#endif
+#ifndef CDFT_4THREADS_BEGIN_N
+#define CDFT_4THREADS_BEGIN_N 65536
+#endif
+#include <pthread.h>
+#include <stdio.h>
+#include <stdlib.h>
+#define cdft_thread_t pthread_t
+#define cdft_thread_create(thp,func,argp) { \
+    if (pthread_create(thp, NULL, func, (void *) argp) != 0) { \
+        fprintf(stderr, "cdft thread error\n"); \
+        exit(1); \
+    } \
+}
+#define cdft_thread_wait(th) { \
+    if (pthread_join(th, NULL) != 0) { \
+        fprintf(stderr, "cdft thread error\n"); \
+        exit(1); \
+    } \
+}
+#endif /* USE_CDFT_PTHREADS */
+
+
+#ifdef USE_CDFT_WINTHREADS
+#define USE_CDFT_THREADS
+#ifndef CDFT_THREADS_BEGIN_N
+#define CDFT_THREADS_BEGIN_N 32768
+#endif
+#ifndef CDFT_4THREADS_BEGIN_N
+#define CDFT_4THREADS_BEGIN_N 524288
+#endif
+#include <windows.h>
+#include <stdio.h>
+#include <stdlib.h>
+#define cdft_thread_t HANDLE
+#define cdft_thread_create(thp,func,argp) { \
+    DWORD thid; \
+    *(thp) = CreateThread(NULL, 0, (LPTHREAD_START_ROUTINE) func, (LPVOID) argp, 0, &thid); \
+    if (*(thp) == 0) { \
+        fprintf(stderr, "cdft thread error\n"); \
+        exit(1); \
+    } \
+}
+#define cdft_thread_wait(th) { \
+    WaitForSingleObject(th, INFINITE); \
+    CloseHandle(th); \
+}
+#endif /* USE_CDFT_WINTHREADS */
+
+
+void cftfsub(int n, double *a, int *ip, int nw, double *w)
+{
+    void bitrv2(int n, int *ip, double *a);
+    void bitrv216(double *a);
+    void bitrv208(double *a);
+    void cftf1st(int n, double *a, double *w);
+    void cftrec4(int n, double *a, int nw, double *w);
+    void cftleaf(int n, int isplt, double *a, int nw, double *w);
+    void cftfx41(int n, double *a, int nw, double *w);
+    void cftf161(double *a, double *w);
+    void cftf081(double *a, double *w);
+    void cftf040(double *a);
+    void cftx020(double *a);
+#ifdef USE_CDFT_THREADS
+    void cftrec4_th(int n, double *a, int nw, double *w);
+#endif /* USE_CDFT_THREADS */
+    
+    if (n > 8) {
+        if (n > 32) {
+            cftf1st(n, a, &w[nw - (n >> 2)]);
+#ifdef USE_CDFT_THREADS
+            if (n > CDFT_THREADS_BEGIN_N) {
+                cftrec4_th(n, a, nw, w);
+            } else 
+#endif /* USE_CDFT_THREADS */
+            if (n > 512) {
+                cftrec4(n, a, nw, w);
+            } else if (n > 128) {
+                cftleaf(n, 1, a, nw, w);
+            } else {
+                cftfx41(n, a, nw, w);
+            }
+            bitrv2(n, ip, a);
+        } else if (n == 32) {
+            cftf161(a, &w[nw - 8]);
+            bitrv216(a);
+        } else {
+            cftf081(a, w);
+            bitrv208(a);
+        }
+    } else if (n == 8) {
+        cftf040(a);
+    } else if (n == 4) {
+        cftx020(a);
+    }
+}
+
+
+void cftbsub(int n, double *a, int *ip, int nw, double *w)
+{
+    void bitrv2conj(int n, int *ip, double *a);
+    void bitrv216neg(double *a);
+    void bitrv208neg(double *a);
+    void cftb1st(int n, double *a, double *w);
+    void cftrec4(int n, double *a, int nw, double *w);
+    void cftleaf(int n, int isplt, double *a, int nw, double *w);
+    void cftfx41(int n, double *a, int nw, double *w);
+    void cftf161(double *a, double *w);
+    void cftf081(double *a, double *w);
+    void cftb040(double *a);
+    void cftx020(double *a);
+#ifdef USE_CDFT_THREADS
+    void cftrec4_th(int n, double *a, int nw, double *w);
+#endif /* USE_CDFT_THREADS */
+    
+    if (n > 8) {
+        if (n > 32) {
+            cftb1st(n, a, &w[nw - (n >> 2)]);
+#ifdef USE_CDFT_THREADS
+            if (n > CDFT_THREADS_BEGIN_N) {
+                cftrec4_th(n, a, nw, w);
+            } else 
+#endif /* USE_CDFT_THREADS */
+            if (n > 512) {
+                cftrec4(n, a, nw, w);
+            } else if (n > 128) {
+                cftleaf(n, 1, a, nw, w);
+            } else {
+                cftfx41(n, a, nw, w);
+            }
+            bitrv2conj(n, ip, a);
+        } else if (n == 32) {
+            cftf161(a, &w[nw - 8]);
+            bitrv216neg(a);
+        } else {
+            cftf081(a, w);
+            bitrv208neg(a);
+        }
+    } else if (n == 8) {
+        cftb040(a);
+    } else if (n == 4) {
+        cftx020(a);
+    }
+}
+
+
+void bitrv2(int n, int *ip, double *a)
+{
+    int j, j1, k, k1, l, m, nh, nm;
+    double xr, xi, yr, yi;
+    
+    m = 1;
+    for (l = n >> 2; l > 8; l >>= 2) {
+        m <<= 1;
+    }
+    nh = n >> 1;
+    nm = 4 * m;
+    if (l == 8) {
+        for (k = 0; k < m; k++) {
+            for (j = 0; j < k; j++) {
+                j1 = 4 * j + 2 * ip[m + k];
+                k1 = 4 * k + 2 * ip[m + j];
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nm;
+                k1 += 2 * nm;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nm;
+                k1 -= nm;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nm;
+                k1 += 2 * nm;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nh;
+                k1 += 2;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nm;
+                k1 -= 2 * nm;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nm;
+                k1 += nm;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nm;
+                k1 -= 2 * nm;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += 2;
+                k1 += nh;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nm;
+                k1 += 2 * nm;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nm;
+                k1 -= nm;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nm;
+                k1 += 2 * nm;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nh;
+                k1 -= 2;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nm;
+                k1 -= 2 * nm;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nm;
+                k1 += nm;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nm;
+                k1 -= 2 * nm;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+            }
+            k1 = 4 * k + 2 * ip[m + k];
+            j1 = k1 + 2;
+            k1 += nh;
+            xr = a[j1];
+            xi = a[j1 + 1];
+            yr = a[k1];
+            yi = a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            j1 += nm;
+            k1 += 2 * nm;
+            xr = a[j1];
+            xi = a[j1 + 1];
+            yr = a[k1];
+            yi = a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            j1 += nm;
+            k1 -= nm;
+            xr = a[j1];
+            xi = a[j1 + 1];
+            yr = a[k1];
+            yi = a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            j1 -= 2;
+            k1 -= nh;
+            xr = a[j1];
+            xi = a[j1 + 1];
+            yr = a[k1];
+            yi = a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            j1 += nh + 2;
+            k1 += nh + 2;
+            xr = a[j1];
+            xi = a[j1 + 1];
+            yr = a[k1];
+            yi = a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            j1 -= nh - nm;
+            k1 += 2 * nm - 2;
+            xr = a[j1];
+            xi = a[j1 + 1];
+            yr = a[k1];
+            yi = a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+        }
+    } else {
+        for (k = 0; k < m; k++) {
+            for (j = 0; j < k; j++) {
+                j1 = 4 * j + ip[m + k];
+                k1 = 4 * k + ip[m + j];
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nm;
+                k1 += nm;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nh;
+                k1 += 2;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nm;
+                k1 -= nm;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += 2;
+                k1 += nh;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nm;
+                k1 += nm;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nh;
+                k1 -= 2;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nm;
+                k1 -= nm;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+            }
+            k1 = 4 * k + ip[m + k];
+            j1 = k1 + 2;
+            k1 += nh;
+            xr = a[j1];
+            xi = a[j1 + 1];
+            yr = a[k1];
+            yi = a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            j1 += nm;
+            k1 += nm;
+            xr = a[j1];
+            xi = a[j1 + 1];
+            yr = a[k1];
+            yi = a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+        }
+    }
+}
+
+
+void bitrv2conj(int n, int *ip, double *a)
+{
+    int j, j1, k, k1, l, m, nh, nm;
+    double xr, xi, yr, yi;
+    
+    m = 1;
+    for (l = n >> 2; l > 8; l >>= 2) {
+        m <<= 1;
+    }
+    nh = n >> 1;
+    nm = 4 * m;
+    if (l == 8) {
+        for (k = 0; k < m; k++) {
+            for (j = 0; j < k; j++) {
+                j1 = 4 * j + 2 * ip[m + k];
+                k1 = 4 * k + 2 * ip[m + j];
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nm;
+                k1 += 2 * nm;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nm;
+                k1 -= nm;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nm;
+                k1 += 2 * nm;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nh;
+                k1 += 2;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nm;
+                k1 -= 2 * nm;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nm;
+                k1 += nm;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nm;
+                k1 -= 2 * nm;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += 2;
+                k1 += nh;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nm;
+                k1 += 2 * nm;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nm;
+                k1 -= nm;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nm;
+                k1 += 2 * nm;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nh;
+                k1 -= 2;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nm;
+                k1 -= 2 * nm;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nm;
+                k1 += nm;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nm;
+                k1 -= 2 * nm;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+            }
+            k1 = 4 * k + 2 * ip[m + k];
+            j1 = k1 + 2;
+            k1 += nh;
+            a[j1 - 1] = -a[j1 - 1];
+            xr = a[j1];
+            xi = -a[j1 + 1];
+            yr = a[k1];
+            yi = -a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            a[k1 + 3] = -a[k1 + 3];
+            j1 += nm;
+            k1 += 2 * nm;
+            xr = a[j1];
+            xi = -a[j1 + 1];
+            yr = a[k1];
+            yi = -a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            j1 += nm;
+            k1 -= nm;
+            xr = a[j1];
+            xi = -a[j1 + 1];
+            yr = a[k1];
+            yi = -a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            j1 -= 2;
+            k1 -= nh;
+            xr = a[j1];
+            xi = -a[j1 + 1];
+            yr = a[k1];
+            yi = -a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            j1 += nh + 2;
+            k1 += nh + 2;
+            xr = a[j1];
+            xi = -a[j1 + 1];
+            yr = a[k1];
+            yi = -a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            j1 -= nh - nm;
+            k1 += 2 * nm - 2;
+            a[j1 - 1] = -a[j1 - 1];
+            xr = a[j1];
+            xi = -a[j1 + 1];
+            yr = a[k1];
+            yi = -a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            a[k1 + 3] = -a[k1 + 3];
+        }
+    } else {
+        for (k = 0; k < m; k++) {
+            for (j = 0; j < k; j++) {
+                j1 = 4 * j + ip[m + k];
+                k1 = 4 * k + ip[m + j];
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nm;
+                k1 += nm;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nh;
+                k1 += 2;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nm;
+                k1 -= nm;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += 2;
+                k1 += nh;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nm;
+                k1 += nm;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nh;
+                k1 -= 2;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nm;
+                k1 -= nm;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+            }
+            k1 = 4 * k + ip[m + k];
+            j1 = k1 + 2;
+            k1 += nh;
+            a[j1 - 1] = -a[j1 - 1];
+            xr = a[j1];
+            xi = -a[j1 + 1];
+            yr = a[k1];
+            yi = -a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            a[k1 + 3] = -a[k1 + 3];
+            j1 += nm;
+            k1 += nm;
+            a[j1 - 1] = -a[j1 - 1];
+            xr = a[j1];
+            xi = -a[j1 + 1];
+            yr = a[k1];
+            yi = -a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            a[k1 + 3] = -a[k1 + 3];
+        }
+    }
+}
+
+
+void bitrv216(double *a)
+{
+    double x1r, x1i, x2r, x2i, x3r, x3i, x4r, x4i, 
+        x5r, x5i, x7r, x7i, x8r, x8i, x10r, x10i, 
+        x11r, x11i, x12r, x12i, x13r, x13i, x14r, x14i;
+    
+    x1r = a[2];
+    x1i = a[3];
+    x2r = a[4];
+    x2i = a[5];
+    x3r = a[6];
+    x3i = a[7];
+    x4r = a[8];
+    x4i = a[9];
+    x5r = a[10];
+    x5i = a[11];
+    x7r = a[14];
+    x7i = a[15];
+    x8r = a[16];
+    x8i = a[17];
+    x10r = a[20];
+    x10i = a[21];
+    x11r = a[22];
+    x11i = a[23];
+    x12r = a[24];
+    x12i = a[25];
+    x13r = a[26];
+    x13i = a[27];
+    x14r = a[28];
+    x14i = a[29];
+    a[2] = x8r;
+    a[3] = x8i;
+    a[4] = x4r;
+    a[5] = x4i;
+    a[6] = x12r;
+    a[7] = x12i;
+    a[8] = x2r;
+    a[9] = x2i;
+    a[10] = x10r;
+    a[11] = x10i;
+    a[14] = x14r;
+    a[15] = x14i;
+    a[16] = x1r;
+    a[17] = x1i;
+    a[20] = x5r;
+    a[21] = x5i;
+    a[22] = x13r;
+    a[23] = x13i;
+    a[24] = x3r;
+    a[25] = x3i;
+    a[26] = x11r;
+    a[27] = x11i;
+    a[28] = x7r;
+    a[29] = x7i;
+}
+
+
+void bitrv216neg(double *a)
+{
+    double x1r, x1i, x2r, x2i, x3r, x3i, x4r, x4i, 
+        x5r, x5i, x6r, x6i, x7r, x7i, x8r, x8i, 
+        x9r, x9i, x10r, x10i, x11r, x11i, x12r, x12i, 
+        x13r, x13i, x14r, x14i, x15r, x15i;
+    
+    x1r = a[2];
+    x1i = a[3];
+    x2r = a[4];
+    x2i = a[5];
+    x3r = a[6];
+    x3i = a[7];
+    x4r = a[8];
+    x4i = a[9];
+    x5r = a[10];
+    x5i = a[11];
+    x6r = a[12];
+    x6i = a[13];
+    x7r = a[14];
+    x7i = a[15];
+    x8r = a[16];
+    x8i = a[17];
+    x9r = a[18];
+    x9i = a[19];
+    x10r = a[20];
+    x10i = a[21];
+    x11r = a[22];
+    x11i = a[23];
+    x12r = a[24];
+    x12i = a[25];
+    x13r = a[26];
+    x13i = a[27];
+    x14r = a[28];
+    x14i = a[29];
+    x15r = a[30];
+    x15i = a[31];
+    a[2] = x15r;
+    a[3] = x15i;
+    a[4] = x7r;
+    a[5] = x7i;
+    a[6] = x11r;
+    a[7] = x11i;
+    a[8] = x3r;
+    a[9] = x3i;
+    a[10] = x13r;
+    a[11] = x13i;
+    a[12] = x5r;
+    a[13] = x5i;
+    a[14] = x9r;
+    a[15] = x9i;
+    a[16] = x1r;
+    a[17] = x1i;
+    a[18] = x14r;
+    a[19] = x14i;
+    a[20] = x6r;
+    a[21] = x6i;
+    a[22] = x10r;
+    a[23] = x10i;
+    a[24] = x2r;
+    a[25] = x2i;
+    a[26] = x12r;
+    a[27] = x12i;
+    a[28] = x4r;
+    a[29] = x4i;
+    a[30] = x8r;
+    a[31] = x8i;
+}
+
+
+void bitrv208(double *a)
+{
+    double x1r, x1i, x3r, x3i, x4r, x4i, x6r, x6i;
+    
+    x1r = a[2];
+    x1i = a[3];
+    x3r = a[6];
+    x3i = a[7];
+    x4r = a[8];
+    x4i = a[9];
+    x6r = a[12];
+    x6i = a[13];
+    a[2] = x4r;
+    a[3] = x4i;
+    a[6] = x6r;
+    a[7] = x6i;
+    a[8] = x1r;
+    a[9] = x1i;
+    a[12] = x3r;
+    a[13] = x3i;
+}
+
+
+void bitrv208neg(double *a)
+{
+    double x1r, x1i, x2r, x2i, x3r, x3i, x4r, x4i, 
+        x5r, x5i, x6r, x6i, x7r, x7i;
+    
+    x1r = a[2];
+    x1i = a[3];
+    x2r = a[4];
+    x2i = a[5];
+    x3r = a[6];
+    x3i = a[7];
+    x4r = a[8];
+    x4i = a[9];
+    x5r = a[10];
+    x5i = a[11];
+    x6r = a[12];
+    x6i = a[13];
+    x7r = a[14];
+    x7i = a[15];
+    a[2] = x7r;
+    a[3] = x7i;
+    a[4] = x3r;
+    a[5] = x3i;
+    a[6] = x5r;
+    a[7] = x5i;
+    a[8] = x1r;
+    a[9] = x1i;
+    a[10] = x6r;
+    a[11] = x6i;
+    a[12] = x2r;
+    a[13] = x2i;
+    a[14] = x4r;
+    a[15] = x4i;
+}
+
+
+void cftf1st(int n, double *a, double *w)
+{
+    int j, j0, j1, j2, j3, k, m, mh;
+    double wn4r, csc1, csc3, wk1r, wk1i, wk3r, wk3i, 
+        wd1r, wd1i, wd3r, wd3i;
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i, 
+        y0r, y0i, y1r, y1i, y2r, y2i, y3r, y3i;
+    
+    mh = n >> 3;
+    m = 2 * mh;
+    j1 = m;
+    j2 = j1 + m;
+    j3 = j2 + m;
+    x0r = a[0] + a[j2];
+    x0i = a[1] + a[j2 + 1];
+    x1r = a[0] - a[j2];
+    x1i = a[1] - a[j2 + 1];
+    x2r = a[j1] + a[j3];
+    x2i = a[j1 + 1] + a[j3 + 1];
+    x3r = a[j1] - a[j3];
+    x3i = a[j1 + 1] - a[j3 + 1];
+    a[0] = x0r + x2r;
+    a[1] = x0i + x2i;
+    a[j1] = x0r - x2r;
+    a[j1 + 1] = x0i - x2i;
+    a[j2] = x1r - x3i;
+    a[j2 + 1] = x1i + x3r;
+    a[j3] = x1r + x3i;
+    a[j3 + 1] = x1i - x3r;
+    wn4r = w[1];
+    csc1 = w[2];
+    csc3 = w[3];
+    wd1r = 1;
+    wd1i = 0;
+    wd3r = 1;
+    wd3i = 0;
+    k = 0;
+    for (j = 2; j < mh - 2; j += 4) {
+        k += 4;
+        wk1r = csc1 * (wd1r + w[k]);
+        wk1i = csc1 * (wd1i + w[k + 1]);
+        wk3r = csc3 * (wd3r + w[k + 2]);
+        wk3i = csc3 * (wd3i + w[k + 3]);
+        wd1r = w[k];
+        wd1i = w[k + 1];
+        wd3r = w[k + 2];
+        wd3i = w[k + 3];
+        j1 = j + m;
+        j2 = j1 + m;
+        j3 = j2 + m;
+        x0r = a[j] + a[j2];
+        x0i = a[j + 1] + a[j2 + 1];
+        x1r = a[j] - a[j2];
+        x1i = a[j + 1] - a[j2 + 1];
+        y0r = a[j + 2] + a[j2 + 2];
+        y0i = a[j + 3] + a[j2 + 3];
+        y1r = a[j + 2] - a[j2 + 2];
+        y1i = a[j + 3] - a[j2 + 3];
+        x2r = a[j1] + a[j3];
+        x2i = a[j1 + 1] + a[j3 + 1];
+        x3r = a[j1] - a[j3];
+        x3i = a[j1 + 1] - a[j3 + 1];
+        y2r = a[j1 + 2] + a[j3 + 2];
+        y2i = a[j1 + 3] + a[j3 + 3];
+        y3r = a[j1 + 2] - a[j3 + 2];
+        y3i = a[j1 + 3] - a[j3 + 3];
+        a[j] = x0r + x2r;
+        a[j + 1] = x0i + x2i;
+        a[j + 2] = y0r + y2r;
+        a[j + 3] = y0i + y2i;
+        a[j1] = x0r - x2r;
+        a[j1 + 1] = x0i - x2i;
+        a[j1 + 2] = y0r - y2r;
+        a[j1 + 3] = y0i - y2i;
+        x0r = x1r - x3i;
+        x0i = x1i + x3r;
+        a[j2] = wk1r * x0r - wk1i * x0i;
+        a[j2 + 1] = wk1r * x0i + wk1i * x0r;
+        x0r = y1r - y3i;
+        x0i = y1i + y3r;
+        a[j2 + 2] = wd1r * x0r - wd1i * x0i;
+        a[j2 + 3] = wd1r * x0i + wd1i * x0r;
+        x0r = x1r + x3i;
+        x0i = x1i - x3r;
+        a[j3] = wk3r * x0r + wk3i * x0i;
+        a[j3 + 1] = wk3r * x0i - wk3i * x0r;
+        x0r = y1r + y3i;
+        x0i = y1i - y3r;
+        a[j3 + 2] = wd3r * x0r + wd3i * x0i;
+        a[j3 + 3] = wd3r * x0i - wd3i * x0r;
+        j0 = m - j;
+        j1 = j0 + m;
+        j2 = j1 + m;
+        j3 = j2 + m;
+        x0r = a[j0] + a[j2];
+        x0i = a[j0 + 1] + a[j2 + 1];
+        x1r = a[j0] - a[j2];
+        x1i = a[j0 + 1] - a[j2 + 1];
+        y0r = a[j0 - 2] + a[j2 - 2];
+        y0i = a[j0 - 1] + a[j2 - 1];
+        y1r = a[j0 - 2] - a[j2 - 2];
+        y1i = a[j0 - 1] - a[j2 - 1];
+        x2r = a[j1] + a[j3];
+        x2i = a[j1 + 1] + a[j3 + 1];
+        x3r = a[j1] - a[j3];
+        x3i = a[j1 + 1] - a[j3 + 1];
+        y2r = a[j1 - 2] + a[j3 - 2];
+        y2i = a[j1 - 1] + a[j3 - 1];
+        y3r = a[j1 - 2] - a[j3 - 2];
+        y3i = a[j1 - 1] - a[j3 - 1];
+        a[j0] = x0r + x2r;
+        a[j0 + 1] = x0i + x2i;
+        a[j0 - 2] = y0r + y2r;
+        a[j0 - 1] = y0i + y2i;
+        a[j1] = x0r - x2r;
+        a[j1 + 1] = x0i - x2i;
+        a[j1 - 2] = y0r - y2r;
+        a[j1 - 1] = y0i - y2i;
+        x0r = x1r - x3i;
+        x0i = x1i + x3r;
+        a[j2] = wk1i * x0r - wk1r * x0i;
+        a[j2 + 1] = wk1i * x0i + wk1r * x0r;
+        x0r = y1r - y3i;
+        x0i = y1i + y3r;
+        a[j2 - 2] = wd1i * x0r - wd1r * x0i;
+        a[j2 - 1] = wd1i * x0i + wd1r * x0r;
+        x0r = x1r + x3i;
+        x0i = x1i - x3r;
+        a[j3] = wk3i * x0r + wk3r * x0i;
+        a[j3 + 1] = wk3i * x0i - wk3r * x0r;
+        x0r = y1r + y3i;
+        x0i = y1i - y3r;
+        a[j3 - 2] = wd3i * x0r + wd3r * x0i;
+        a[j3 - 1] = wd3i * x0i - wd3r * x0r;
+    }
+    wk1r = csc1 * (wd1r + wn4r);
+    wk1i = csc1 * (wd1i + wn4r);
+    wk3r = csc3 * (wd3r - wn4r);
+    wk3i = csc3 * (wd3i - wn4r);
+    j0 = mh;
+    j1 = j0 + m;
+    j2 = j1 + m;
+    j3 = j2 + m;
+    x0r = a[j0 - 2] + a[j2 - 2];
+    x0i = a[j0 - 1] + a[j2 - 1];
+    x1r = a[j0 - 2] - a[j2 - 2];
+    x1i = a[j0 - 1] - a[j2 - 1];
+    x2r = a[j1 - 2] + a[j3 - 2];
+    x2i = a[j1 - 1] + a[j3 - 1];
+    x3r = a[j1 - 2] - a[j3 - 2];
+    x3i = a[j1 - 1] - a[j3 - 1];
+    a[j0 - 2] = x0r + x2r;
+    a[j0 - 1] = x0i + x2i;
+    a[j1 - 2] = x0r - x2r;
+    a[j1 - 1] = x0i - x2i;
+    x0r = x1r - x3i;
+    x0i = x1i + x3r;
+    a[j2 - 2] = wk1r * x0r - wk1i * x0i;
+    a[j2 - 1] = wk1r * x0i + wk1i * x0r;
+    x0r = x1r + x3i;
+    x0i = x1i - x3r;
+    a[j3 - 2] = wk3r * x0r + wk3i * x0i;
+    a[j3 - 1] = wk3r * x0i - wk3i * x0r;
+    x0r = a[j0] + a[j2];
+    x0i = a[j0 + 1] + a[j2 + 1];
+    x1r = a[j0] - a[j2];
+    x1i = a[j0 + 1] - a[j2 + 1];
+    x2r = a[j1] + a[j3];
+    x2i = a[j1 + 1] + a[j3 + 1];
+    x3r = a[j1] - a[j3];
+    x3i = a[j1 + 1] - a[j3 + 1];
+    a[j0] = x0r + x2r;
+    a[j0 + 1] = x0i + x2i;
+    a[j1] = x0r - x2r;
+    a[j1 + 1] = x0i - x2i;
+    x0r = x1r - x3i;
+    x0i = x1i + x3r;
+    a[j2] = wn4r * (x0r - x0i);
+    a[j2 + 1] = wn4r * (x0i + x0r);
+    x0r = x1r + x3i;
+    x0i = x1i - x3r;
+    a[j3] = -wn4r * (x0r + x0i);
+    a[j3 + 1] = -wn4r * (x0i - x0r);
+    x0r = a[j0 + 2] + a[j2 + 2];
+    x0i = a[j0 + 3] + a[j2 + 3];
+    x1r = a[j0 + 2] - a[j2 + 2];
+    x1i = a[j0 + 3] - a[j2 + 3];
+    x2r = a[j1 + 2] + a[j3 + 2];
+    x2i = a[j1 + 3] + a[j3 + 3];
+    x3r = a[j1 + 2] - a[j3 + 2];
+    x3i = a[j1 + 3] - a[j3 + 3];
+    a[j0 + 2] = x0r + x2r;
+    a[j0 + 3] = x0i + x2i;
+    a[j1 + 2] = x0r - x2r;
+    a[j1 + 3] = x0i - x2i;
+    x0r = x1r - x3i;
+    x0i = x1i + x3r;
+    a[j2 + 2] = wk1i * x0r - wk1r * x0i;
+    a[j2 + 3] = wk1i * x0i + wk1r * x0r;
+    x0r = x1r + x3i;
+    x0i = x1i - x3r;
+    a[j3 + 2] = wk3i * x0r + wk3r * x0i;
+    a[j3 + 3] = wk3i * x0i - wk3r * x0r;
+}
+
+
+void cftb1st(int n, double *a, double *w)
+{
+    int j, j0, j1, j2, j3, k, m, mh;
+    double wn4r, csc1, csc3, wk1r, wk1i, wk3r, wk3i, 
+        wd1r, wd1i, wd3r, wd3i;
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i, 
+        y0r, y0i, y1r, y1i, y2r, y2i, y3r, y3i;
+    
+    mh = n >> 3;
+    m = 2 * mh;
+    j1 = m;
+    j2 = j1 + m;
+    j3 = j2 + m;
+    x0r = a[0] + a[j2];
+    x0i = -a[1] - a[j2 + 1];
+    x1r = a[0] - a[j2];
+    x1i = -a[1] + a[j2 + 1];
+    x2r = a[j1] + a[j3];
+    x2i = a[j1 + 1] + a[j3 + 1];
+    x3r = a[j1] - a[j3];
+    x3i = a[j1 + 1] - a[j3 + 1];
+    a[0] = x0r + x2r;
+    a[1] = x0i - x2i;
+    a[j1] = x0r - x2r;
+    a[j1 + 1] = x0i + x2i;
+    a[j2] = x1r + x3i;
+    a[j2 + 1] = x1i + x3r;
+    a[j3] = x1r - x3i;
+    a[j3 + 1] = x1i - x3r;
+    wn4r = w[1];
+    csc1 = w[2];
+    csc3 = w[3];
+    wd1r = 1;
+    wd1i = 0;
+    wd3r = 1;
+    wd3i = 0;
+    k = 0;
+    for (j = 2; j < mh - 2; j += 4) {
+        k += 4;
+        wk1r = csc1 * (wd1r + w[k]);
+        wk1i = csc1 * (wd1i + w[k + 1]);
+        wk3r = csc3 * (wd3r + w[k + 2]);
+        wk3i = csc3 * (wd3i + w[k + 3]);
+        wd1r = w[k];
+        wd1i = w[k + 1];
+        wd3r = w[k + 2];
+        wd3i = w[k + 3];
+        j1 = j + m;
+        j2 = j1 + m;
+        j3 = j2 + m;
+        x0r = a[j] + a[j2];
+        x0i = -a[j + 1] - a[j2 + 1];
+        x1r = a[j] - a[j2];
+        x1i = -a[j + 1] + a[j2 + 1];
+        y0r = a[j + 2] + a[j2 + 2];
+        y0i = -a[j + 3] - a[j2 + 3];
+        y1r = a[j + 2] - a[j2 + 2];
+        y1i = -a[j + 3] + a[j2 + 3];
+        x2r = a[j1] + a[j3];
+        x2i = a[j1 + 1] + a[j3 + 1];
+        x3r = a[j1] - a[j3];
+        x3i = a[j1 + 1] - a[j3 + 1];
+        y2r = a[j1 + 2] + a[j3 + 2];
+        y2i = a[j1 + 3] + a[j3 + 3];
+        y3r = a[j1 + 2] - a[j3 + 2];
+        y3i = a[j1 + 3] - a[j3 + 3];
+        a[j] = x0r + x2r;
+        a[j + 1] = x0i - x2i;
+        a[j + 2] = y0r + y2r;
+        a[j + 3] = y0i - y2i;
+        a[j1] = x0r - x2r;
+        a[j1 + 1] = x0i + x2i;
+        a[j1 + 2] = y0r - y2r;
+        a[j1 + 3] = y0i + y2i;
+        x0r = x1r + x3i;
+        x0i = x1i + x3r;
+        a[j2] = wk1r * x0r - wk1i * x0i;
+        a[j2 + 1] = wk1r * x0i + wk1i * x0r;
+        x0r = y1r + y3i;
+        x0i = y1i + y3r;
+        a[j2 + 2] = wd1r * x0r - wd1i * x0i;
+        a[j2 + 3] = wd1r * x0i + wd1i * x0r;
+        x0r = x1r - x3i;
+        x0i = x1i - x3r;
+        a[j3] = wk3r * x0r + wk3i * x0i;
+        a[j3 + 1] = wk3r * x0i - wk3i * x0r;
+        x0r = y1r - y3i;
+        x0i = y1i - y3r;
+        a[j3 + 2] = wd3r * x0r + wd3i * x0i;
+        a[j3 + 3] = wd3r * x0i - wd3i * x0r;
+        j0 = m - j;
+        j1 = j0 + m;
+        j2 = j1 + m;
+        j3 = j2 + m;
+        x0r = a[j0] + a[j2];
+        x0i = -a[j0 + 1] - a[j2 + 1];
+        x1r = a[j0] - a[j2];
+        x1i = -a[j0 + 1] + a[j2 + 1];
+        y0r = a[j0 - 2] + a[j2 - 2];
+        y0i = -a[j0 - 1] - a[j2 - 1];
+        y1r = a[j0 - 2] - a[j2 - 2];
+        y1i = -a[j0 - 1] + a[j2 - 1];
+        x2r = a[j1] + a[j3];
+        x2i = a[j1 + 1] + a[j3 + 1];
+        x3r = a[j1] - a[j3];
+        x3i = a[j1 + 1] - a[j3 + 1];
+        y2r = a[j1 - 2] + a[j3 - 2];
+        y2i = a[j1 - 1] + a[j3 - 1];
+        y3r = a[j1 - 2] - a[j3 - 2];
+        y3i = a[j1 - 1] - a[j3 - 1];
+        a[j0] = x0r + x2r;
+        a[j0 + 1] = x0i - x2i;
+        a[j0 - 2] = y0r + y2r;
+        a[j0 - 1] = y0i - y2i;
+        a[j1] = x0r - x2r;
+        a[j1 + 1] = x0i + x2i;
+        a[j1 - 2] = y0r - y2r;
+        a[j1 - 1] = y0i + y2i;
+        x0r = x1r + x3i;
+        x0i = x1i + x3r;
+        a[j2] = wk1i * x0r - wk1r * x0i;
+        a[j2 + 1] = wk1i * x0i + wk1r * x0r;
+        x0r = y1r + y3i;
+        x0i = y1i + y3r;
+        a[j2 - 2] = wd1i * x0r - wd1r * x0i;
+        a[j2 - 1] = wd1i * x0i + wd1r * x0r;
+        x0r = x1r - x3i;
+        x0i = x1i - x3r;
+        a[j3] = wk3i * x0r + wk3r * x0i;
+        a[j3 + 1] = wk3i * x0i - wk3r * x0r;
+        x0r = y1r - y3i;
+        x0i = y1i - y3r;
+        a[j3 - 2] = wd3i * x0r + wd3r * x0i;
+        a[j3 - 1] = wd3i * x0i - wd3r * x0r;
+    }
+    wk1r = csc1 * (wd1r + wn4r);
+    wk1i = csc1 * (wd1i + wn4r);
+    wk3r = csc3 * (wd3r - wn4r);
+    wk3i = csc3 * (wd3i - wn4r);
+    j0 = mh;
+    j1 = j0 + m;
+    j2 = j1 + m;
+    j3 = j2 + m;
+    x0r = a[j0 - 2] + a[j2 - 2];
+    x0i = -a[j0 - 1] - a[j2 - 1];
+    x1r = a[j0 - 2] - a[j2 - 2];
+    x1i = -a[j0 - 1] + a[j2 - 1];
+    x2r = a[j1 - 2] + a[j3 - 2];
+    x2i = a[j1 - 1] + a[j3 - 1];
+    x3r = a[j1 - 2] - a[j3 - 2];
+    x3i = a[j1 - 1] - a[j3 - 1];
+    a[j0 - 2] = x0r + x2r;
+    a[j0 - 1] = x0i - x2i;
+    a[j1 - 2] = x0r - x2r;
+    a[j1 - 1] = x0i + x2i;
+    x0r = x1r + x3i;
+    x0i = x1i + x3r;
+    a[j2 - 2] = wk1r * x0r - wk1i * x0i;
+    a[j2 - 1] = wk1r * x0i + wk1i * x0r;
+    x0r = x1r - x3i;
+    x0i = x1i - x3r;
+    a[j3 - 2] = wk3r * x0r + wk3i * x0i;
+    a[j3 - 1] = wk3r * x0i - wk3i * x0r;
+    x0r = a[j0] + a[j2];
+    x0i = -a[j0 + 1] - a[j2 + 1];
+    x1r = a[j0] - a[j2];
+    x1i = -a[j0 + 1] + a[j2 + 1];
+    x2r = a[j1] + a[j3];
+    x2i = a[j1 + 1] + a[j3 + 1];
+    x3r = a[j1] - a[j3];
+    x3i = a[j1 + 1] - a[j3 + 1];
+    a[j0] = x0r + x2r;
+    a[j0 + 1] = x0i - x2i;
+    a[j1] = x0r - x2r;
+    a[j1 + 1] = x0i + x2i;
+    x0r = x1r + x3i;
+    x0i = x1i + x3r;
+    a[j2] = wn4r * (x0r - x0i);
+    a[j2 + 1] = wn4r * (x0i + x0r);
+    x0r = x1r - x3i;
+    x0i = x1i - x3r;
+    a[j3] = -wn4r * (x0r + x0i);
+    a[j3 + 1] = -wn4r * (x0i - x0r);
+    x0r = a[j0 + 2] + a[j2 + 2];
+    x0i = -a[j0 + 3] - a[j2 + 3];
+    x1r = a[j0 + 2] - a[j2 + 2];
+    x1i = -a[j0 + 3] + a[j2 + 3];
+    x2r = a[j1 + 2] + a[j3 + 2];
+    x2i = a[j1 + 3] + a[j3 + 3];
+    x3r = a[j1 + 2] - a[j3 + 2];
+    x3i = a[j1 + 3] - a[j3 + 3];
+    a[j0 + 2] = x0r + x2r;
+    a[j0 + 3] = x0i - x2i;
+    a[j1 + 2] = x0r - x2r;
+    a[j1 + 3] = x0i + x2i;
+    x0r = x1r + x3i;
+    x0i = x1i + x3r;
+    a[j2 + 2] = wk1i * x0r - wk1r * x0i;
+    a[j2 + 3] = wk1i * x0i + wk1r * x0r;
+    x0r = x1r - x3i;
+    x0i = x1i - x3r;
+    a[j3 + 2] = wk3i * x0r + wk3r * x0i;
+    a[j3 + 3] = wk3i * x0i - wk3r * x0r;
+}
+
+
+#ifdef USE_CDFT_THREADS
+struct cdft_arg_st {
+    int n0;
+    int n;
+    double *a;
+    int nw;
+    double *w;
+};
+typedef struct cdft_arg_st cdft_arg_t;
+
+
+void cftrec4_th(int n, double *a, int nw, double *w)
+{
+    void *cftrec1_th(void *p);
+    void *cftrec2_th(void *p);
+    int i, idiv4, m, nthread;
+    cdft_thread_t th[4];
+    cdft_arg_t ag[4];
+    
+    nthread = 2;
+    idiv4 = 0;
+    m = n >> 1;
+    if (n > CDFT_4THREADS_BEGIN_N) {
+        nthread = 4;
+        idiv4 = 1;
+        m >>= 1;
+    }
+    for (i = 0; i < nthread; i++) {
+        ag[i].n0 = n;
+        ag[i].n = m;
+        ag[i].a = &a[i * m];
+        ag[i].nw = nw;
+        ag[i].w = w;
+        if (i != idiv4) {
+            cdft_thread_create(&th[i], cftrec1_th, &ag[i]);
+        } else {
+            cdft_thread_create(&th[i], cftrec2_th, &ag[i]);
+        }
+    }
+    for (i = 0; i < nthread; i++) {
+        cdft_thread_wait(th[i]);
+    }
+}
+
+
+void *cftrec1_th(void *p)
+{
+    int cfttree(int n, int j, int k, double *a, int nw, double *w);
+    void cftleaf(int n, int isplt, double *a, int nw, double *w);
+    void cftmdl1(int n, double *a, double *w);
+    int isplt, j, k, m, n, n0, nw;
+    double *a, *w;
+    
+    n0 = ((cdft_arg_t *) p)->n0;
+    n = ((cdft_arg_t *) p)->n;
+    a = ((cdft_arg_t *) p)->a;
+    nw = ((cdft_arg_t *) p)->nw;
+    w = ((cdft_arg_t *) p)->w;
+    m = n0;
+    while (m > 512) {
+        m >>= 2;
+        cftmdl1(m, &a[n - m], &w[nw - (m >> 1)]);
+    }
+    cftleaf(m, 1, &a[n - m], nw, w);
+    k = 0;
+    for (j = n - m; j > 0; j -= m) {
+        k++;
+        isplt = cfttree(m, j, k, a, nw, w);
+        cftleaf(m, isplt, &a[j - m], nw, w);
+    }
+    return (void *) 0;
+}
+
+
+void *cftrec2_th(void *p)
+{
+    int cfttree(int n, int j, int k, double *a, int nw, double *w);
+    void cftleaf(int n, int isplt, double *a, int nw, double *w);
+    void cftmdl2(int n, double *a, double *w);
+    int isplt, j, k, m, n, n0, nw;
+    double *a, *w;
+    
+    n0 = ((cdft_arg_t *) p)->n0;
+    n = ((cdft_arg_t *) p)->n;
+    a = ((cdft_arg_t *) p)->a;
+    nw = ((cdft_arg_t *) p)->nw;
+    w = ((cdft_arg_t *) p)->w;
+    k = 1;
+    m = n0;
+    while (m > 512) {
+        m >>= 2;
+        k <<= 2;
+        cftmdl2(m, &a[n - m], &w[nw - m]);
+    }
+    cftleaf(m, 0, &a[n - m], nw, w);
+    k >>= 1;
+    for (j = n - m; j > 0; j -= m) {
+        k++;
+        isplt = cfttree(m, j, k, a, nw, w);
+        cftleaf(m, isplt, &a[j - m], nw, w);
+    }
+    return (void *) 0;
+}
+#endif /* USE_CDFT_THREADS */
+
+
+void cftrec4(int n, double *a, int nw, double *w)
+{
+    int cfttree(int n, int j, int k, double *a, int nw, double *w);
+    void cftleaf(int n, int isplt, double *a, int nw, double *w);
+    void cftmdl1(int n, double *a, double *w);
+    int isplt, j, k, m;
+    
+    m = n;
+    while (m > 512) {
+        m >>= 2;
+        cftmdl1(m, &a[n - m], &w[nw - (m >> 1)]);
+    }
+    cftleaf(m, 1, &a[n - m], nw, w);
+    k = 0;
+    for (j = n - m; j > 0; j -= m) {
+        k++;
+        isplt = cfttree(m, j, k, a, nw, w);
+        cftleaf(m, isplt, &a[j - m], nw, w);
+    }
+}
+
+
+int cfttree(int n, int j, int k, double *a, int nw, double *w)
+{
+    void cftmdl1(int n, double *a, double *w);
+    void cftmdl2(int n, double *a, double *w);
+    int i, isplt, m;
+    
+    if ((k & 3) != 0) {
+        isplt = k & 1;
+        if (isplt != 0) {
+            cftmdl1(n, &a[j - n], &w[nw - (n >> 1)]);
+        } else {
+            cftmdl2(n, &a[j - n], &w[nw - n]);
+        }
+    } else {
+        m = n;
+        for (i = k; (i & 3) == 0; i >>= 2) {
+            m <<= 2;
+        }
+        isplt = i & 1;
+        if (isplt != 0) {
+            while (m > 128) {
+                cftmdl1(m, &a[j - m], &w[nw - (m >> 1)]);
+                m >>= 2;
+            }
+        } else {
+            while (m > 128) {
+                cftmdl2(m, &a[j - m], &w[nw - m]);
+                m >>= 2;
+            }
+        }
+    }
+    return isplt;
+}
+
+
+void cftleaf(int n, int isplt, double *a, int nw, double *w)
+{
+    void cftmdl1(int n, double *a, double *w);
+    void cftmdl2(int n, double *a, double *w);
+    void cftf161(double *a, double *w);
+    void cftf162(double *a, double *w);
+    void cftf081(double *a, double *w);
+    void cftf082(double *a, double *w);
+    
+    if (n == 512) {
+        cftmdl1(128, a, &w[nw - 64]);
+        cftf161(a, &w[nw - 8]);
+        cftf162(&a[32], &w[nw - 32]);
+        cftf161(&a[64], &w[nw - 8]);
+        cftf161(&a[96], &w[nw - 8]);
+        cftmdl2(128, &a[128], &w[nw - 128]);
+        cftf161(&a[128], &w[nw - 8]);
+        cftf162(&a[160], &w[nw - 32]);
+        cftf161(&a[192], &w[nw - 8]);
+        cftf162(&a[224], &w[nw - 32]);
+        cftmdl1(128, &a[256], &w[nw - 64]);
+        cftf161(&a[256], &w[nw - 8]);
+        cftf162(&a[288], &w[nw - 32]);
+        cftf161(&a[320], &w[nw - 8]);
+        cftf161(&a[352], &w[nw - 8]);
+        if (isplt != 0) {
+            cftmdl1(128, &a[384], &w[nw - 64]);
+            cftf161(&a[480], &w[nw - 8]);
+        } else {
+            cftmdl2(128, &a[384], &w[nw - 128]);
+            cftf162(&a[480], &w[nw - 32]);
+        }
+        cftf161(&a[384], &w[nw - 8]);
+        cftf162(&a[416], &w[nw - 32]);
+        cftf161(&a[448], &w[nw - 8]);
+    } else {
+        cftmdl1(64, a, &w[nw - 32]);
+        cftf081(a, &w[nw - 8]);
+        cftf082(&a[16], &w[nw - 8]);
+        cftf081(&a[32], &w[nw - 8]);
+        cftf081(&a[48], &w[nw - 8]);
+        cftmdl2(64, &a[64], &w[nw - 64]);
+        cftf081(&a[64], &w[nw - 8]);
+        cftf082(&a[80], &w[nw - 8]);
+        cftf081(&a[96], &w[nw - 8]);
+        cftf082(&a[112], &w[nw - 8]);
+        cftmdl1(64, &a[128], &w[nw - 32]);
+        cftf081(&a[128], &w[nw - 8]);
+        cftf082(&a[144], &w[nw - 8]);
+        cftf081(&a[160], &w[nw - 8]);
+        cftf081(&a[176], &w[nw - 8]);
+        if (isplt != 0) {
+            cftmdl1(64, &a[192], &w[nw - 32]);
+            cftf081(&a[240], &w[nw - 8]);
+        } else {
+            cftmdl2(64, &a[192], &w[nw - 64]);
+            cftf082(&a[240], &w[nw - 8]);
+        }
+        cftf081(&a[192], &w[nw - 8]);
+        cftf082(&a[208], &w[nw - 8]);
+        cftf081(&a[224], &w[nw - 8]);
+    }
+}
+
+
+void cftmdl1(int n, double *a, double *w)
+{
+    int j, j0, j1, j2, j3, k, m, mh;
+    double wn4r, wk1r, wk1i, wk3r, wk3i;
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i;
+    
+    mh = n >> 3;
+    m = 2 * mh;
+    j1 = m;
+    j2 = j1 + m;
+    j3 = j2 + m;
+    x0r = a[0] + a[j2];
+    x0i = a[1] + a[j2 + 1];
+    x1r = a[0] - a[j2];
+    x1i = a[1] - a[j2 + 1];
+    x2r = a[j1] + a[j3];
+    x2i = a[j1 + 1] + a[j3 + 1];
+    x3r = a[j1] - a[j3];
+    x3i = a[j1 + 1] - a[j3 + 1];
+    a[0] = x0r + x2r;
+    a[1] = x0i + x2i;
+    a[j1] = x0r - x2r;
+    a[j1 + 1] = x0i - x2i;
+    a[j2] = x1r - x3i;
+    a[j2 + 1] = x1i + x3r;
+    a[j3] = x1r + x3i;
+    a[j3 + 1] = x1i - x3r;
+    wn4r = w[1];
+    k = 0;
+    for (j = 2; j < mh; j += 2) {
+        k += 4;
+        wk1r = w[k];
+        wk1i = w[k + 1];
+        wk3r = w[k + 2];
+        wk3i = w[k + 3];
+        j1 = j + m;
+        j2 = j1 + m;
+        j3 = j2 + m;
+        x0r = a[j] + a[j2];
+        x0i = a[j + 1] + a[j2 + 1];
+        x1r = a[j] - a[j2];
+        x1i = a[j + 1] - a[j2 + 1];
+        x2r = a[j1] + a[j3];
+        x2i = a[j1 + 1] + a[j3 + 1];
+        x3r = a[j1] - a[j3];
+        x3i = a[j1 + 1] - a[j3 + 1];
+        a[j] = x0r + x2r;
+        a[j + 1] = x0i + x2i;
+        a[j1] = x0r - x2r;
+        a[j1 + 1] = x0i - x2i;
+        x0r = x1r - x3i;
+        x0i = x1i + x3r;
+        a[j2] = wk1r * x0r - wk1i * x0i;
+        a[j2 + 1] = wk1r * x0i + wk1i * x0r;
+        x0r = x1r + x3i;
+        x0i = x1i - x3r;
+        a[j3] = wk3r * x0r + wk3i * x0i;
+        a[j3 + 1] = wk3r * x0i - wk3i * x0r;
+        j0 = m - j;
+        j1 = j0 + m;
+        j2 = j1 + m;
+        j3 = j2 + m;
+        x0r = a[j0] + a[j2];
+        x0i = a[j0 + 1] + a[j2 + 1];
+        x1r = a[j0] - a[j2];
+        x1i = a[j0 + 1] - a[j2 + 1];
+        x2r = a[j1] + a[j3];
+        x2i = a[j1 + 1] + a[j3 + 1];
+        x3r = a[j1] - a[j3];
+        x3i = a[j1 + 1] - a[j3 + 1];
+        a[j0] = x0r + x2r;
+        a[j0 + 1] = x0i + x2i;
+        a[j1] = x0r - x2r;
+        a[j1 + 1] = x0i - x2i;
+        x0r = x1r - x3i;
+        x0i = x1i + x3r;
+        a[j2] = wk1i * x0r - wk1r * x0i;
+        a[j2 + 1] = wk1i * x0i + wk1r * x0r;
+        x0r = x1r + x3i;
+        x0i = x1i - x3r;
+        a[j3] = wk3i * x0r + wk3r * x0i;
+        a[j3 + 1] = wk3i * x0i - wk3r * x0r;
+    }
+    j0 = mh;
+    j1 = j0 + m;
+    j2 = j1 + m;
+    j3 = j2 + m;
+    x0r = a[j0] + a[j2];
+    x0i = a[j0 + 1] + a[j2 + 1];
+    x1r = a[j0] - a[j2];
+    x1i = a[j0 + 1] - a[j2 + 1];
+    x2r = a[j1] + a[j3];
+    x2i = a[j1 + 1] + a[j3 + 1];
+    x3r = a[j1] - a[j3];
+    x3i = a[j1 + 1] - a[j3 + 1];
+    a[j0] = x0r + x2r;
+    a[j0 + 1] = x0i + x2i;
+    a[j1] = x0r - x2r;
+    a[j1 + 1] = x0i - x2i;
+    x0r = x1r - x3i;
+    x0i = x1i + x3r;
+    a[j2] = wn4r * (x0r - x0i);
+    a[j2 + 1] = wn4r * (x0i + x0r);
+    x0r = x1r + x3i;
+    x0i = x1i - x3r;
+    a[j3] = -wn4r * (x0r + x0i);
+    a[j3 + 1] = -wn4r * (x0i - x0r);
+}
+
+
+void cftmdl2(int n, double *a, double *w)
+{
+    int j, j0, j1, j2, j3, k, kr, m, mh;
+    double wn4r, wk1r, wk1i, wk3r, wk3i, wd1r, wd1i, wd3r, wd3i;
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i, y0r, y0i, y2r, y2i;
+    
+    mh = n >> 3;
+    m = 2 * mh;
+    wn4r = w[1];
+    j1 = m;
+    j2 = j1 + m;
+    j3 = j2 + m;
+    x0r = a[0] - a[j2 + 1];
+    x0i = a[1] + a[j2];
+    x1r = a[0] + a[j2 + 1];
+    x1i = a[1] - a[j2];
+    x2r = a[j1] - a[j3 + 1];
+    x2i = a[j1 + 1] + a[j3];
+    x3r = a[j1] + a[j3 + 1];
+    x3i = a[j1 + 1] - a[j3];
+    y0r = wn4r * (x2r - x2i);
+    y0i = wn4r * (x2i + x2r);
+    a[0] = x0r + y0r;
+    a[1] = x0i + y0i;
+    a[j1] = x0r - y0r;
+    a[j1 + 1] = x0i - y0i;
+    y0r = wn4r * (x3r - x3i);
+    y0i = wn4r * (x3i + x3r);
+    a[j2] = x1r - y0i;
+    a[j2 + 1] = x1i + y0r;
+    a[j3] = x1r + y0i;
+    a[j3 + 1] = x1i - y0r;
+    k = 0;
+    kr = 2 * m;
+    for (j = 2; j < mh; j += 2) {
+        k += 4;
+        wk1r = w[k];
+        wk1i = w[k + 1];
+        wk3r = w[k + 2];
+        wk3i = w[k + 3];
+        kr -= 4;
+        wd1i = w[kr];
+        wd1r = w[kr + 1];
+        wd3i = w[kr + 2];
+        wd3r = w[kr + 3];
+        j1 = j + m;
+        j2 = j1 + m;
+        j3 = j2 + m;
+        x0r = a[j] - a[j2 + 1];
+        x0i = a[j + 1] + a[j2];
+        x1r = a[j] + a[j2 + 1];
+        x1i = a[j + 1] - a[j2];
+        x2r = a[j1] - a[j3 + 1];
+        x2i = a[j1 + 1] + a[j3];
+        x3r = a[j1] + a[j3 + 1];
+        x3i = a[j1 + 1] - a[j3];
+        y0r = wk1r * x0r - wk1i * x0i;
+        y0i = wk1r * x0i + wk1i * x0r;
+        y2r = wd1r * x2r - wd1i * x2i;
+        y2i = wd1r * x2i + wd1i * x2r;
+        a[j] = y0r + y2r;
+        a[j + 1] = y0i + y2i;
+        a[j1] = y0r - y2r;
+        a[j1 + 1] = y0i - y2i;
+        y0r = wk3r * x1r + wk3i * x1i;
+        y0i = wk3r * x1i - wk3i * x1r;
+        y2r = wd3r * x3r + wd3i * x3i;
+        y2i = wd3r * x3i - wd3i * x3r;
+        a[j2] = y0r + y2r;
+        a[j2 + 1] = y0i + y2i;
+        a[j3] = y0r - y2r;
+        a[j3 + 1] = y0i - y2i;
+        j0 = m - j;
+        j1 = j0 + m;
+        j2 = j1 + m;
+        j3 = j2 + m;
+        x0r = a[j0] - a[j2 + 1];
+        x0i = a[j0 + 1] + a[j2];
+        x1r = a[j0] + a[j2 + 1];
+        x1i = a[j0 + 1] - a[j2];
+        x2r = a[j1] - a[j3 + 1];
+        x2i = a[j1 + 1] + a[j3];
+        x3r = a[j1] + a[j3 + 1];
+        x3i = a[j1 + 1] - a[j3];
+        y0r = wd1i * x0r - wd1r * x0i;
+        y0i = wd1i * x0i + wd1r * x0r;
+        y2r = wk1i * x2r - wk1r * x2i;
+        y2i = wk1i * x2i + wk1r * x2r;
+        a[j0] = y0r + y2r;
+        a[j0 + 1] = y0i + y2i;
+        a[j1] = y0r - y2r;
+        a[j1 + 1] = y0i - y2i;
+        y0r = wd3i * x1r + wd3r * x1i;
+        y0i = wd3i * x1i - wd3r * x1r;
+        y2r = wk3i * x3r + wk3r * x3i;
+        y2i = wk3i * x3i - wk3r * x3r;
+        a[j2] = y0r + y2r;
+        a[j2 + 1] = y0i + y2i;
+        a[j3] = y0r - y2r;
+        a[j3 + 1] = y0i - y2i;
+    }
+    wk1r = w[m];
+    wk1i = w[m + 1];
+    j0 = mh;
+    j1 = j0 + m;
+    j2 = j1 + m;
+    j3 = j2 + m;
+    x0r = a[j0] - a[j2 + 1];
+    x0i = a[j0 + 1] + a[j2];
+    x1r = a[j0] + a[j2 + 1];
+    x1i = a[j0 + 1] - a[j2];
+    x2r = a[j1] - a[j3 + 1];
+    x2i = a[j1 + 1] + a[j3];
+    x3r = a[j1] + a[j3 + 1];
+    x3i = a[j1 + 1] - a[j3];
+    y0r = wk1r * x0r - wk1i * x0i;
+    y0i = wk1r * x0i + wk1i * x0r;
+    y2r = wk1i * x2r - wk1r * x2i;
+    y2i = wk1i * x2i + wk1r * x2r;
+    a[j0] = y0r + y2r;
+    a[j0 + 1] = y0i + y2i;
+    a[j1] = y0r - y2r;
+    a[j1 + 1] = y0i - y2i;
+    y0r = wk1i * x1r - wk1r * x1i;
+    y0i = wk1i * x1i + wk1r * x1r;
+    y2r = wk1r * x3r - wk1i * x3i;
+    y2i = wk1r * x3i + wk1i * x3r;
+    a[j2] = y0r - y2r;
+    a[j2 + 1] = y0i - y2i;
+    a[j3] = y0r + y2r;
+    a[j3 + 1] = y0i + y2i;
+}
+
+
+void cftfx41(int n, double *a, int nw, double *w)
+{
+    void cftf161(double *a, double *w);
+    void cftf162(double *a, double *w);
+    void cftf081(double *a, double *w);
+    void cftf082(double *a, double *w);
+    
+    if (n == 128) {
+        cftf161(a, &w[nw - 8]);
+        cftf162(&a[32], &w[nw - 32]);
+        cftf161(&a[64], &w[nw - 8]);
+        cftf161(&a[96], &w[nw - 8]);
+    } else {
+        cftf081(a, &w[nw - 8]);
+        cftf082(&a[16], &w[nw - 8]);
+        cftf081(&a[32], &w[nw - 8]);
+        cftf081(&a[48], &w[nw - 8]);
+    }
+}
+
+
+void cftf161(double *a, double *w)
+{
+    double wn4r, wk1r, wk1i, 
+        x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i, 
+        y0r, y0i, y1r, y1i, y2r, y2i, y3r, y3i, 
+        y4r, y4i, y5r, y5i, y6r, y6i, y7r, y7i, 
+        y8r, y8i, y9r, y9i, y10r, y10i, y11r, y11i, 
+        y12r, y12i, y13r, y13i, y14r, y14i, y15r, y15i;
+    
+    wn4r = w[1];
+    wk1r = w[2];
+    wk1i = w[3];
+    x0r = a[0] + a[16];
+    x0i = a[1] + a[17];
+    x1r = a[0] - a[16];
+    x1i = a[1] - a[17];
+    x2r = a[8] + a[24];
+    x2i = a[9] + a[25];
+    x3r = a[8] - a[24];
+    x3i = a[9] - a[25];
+    y0r = x0r + x2r;
+    y0i = x0i + x2i;
+    y4r = x0r - x2r;
+    y4i = x0i - x2i;
+    y8r = x1r - x3i;
+    y8i = x1i + x3r;
+    y12r = x1r + x3i;
+    y12i = x1i - x3r;
+    x0r = a[2] + a[18];
+    x0i = a[3] + a[19];
+    x1r = a[2] - a[18];
+    x1i = a[3] - a[19];
+    x2r = a[10] + a[26];
+    x2i = a[11] + a[27];
+    x3r = a[10] - a[26];
+    x3i = a[11] - a[27];
+    y1r = x0r + x2r;
+    y1i = x0i + x2i;
+    y5r = x0r - x2r;
+    y5i = x0i - x2i;
+    x0r = x1r - x3i;
+    x0i = x1i + x3r;
+    y9r = wk1r * x0r - wk1i * x0i;
+    y9i = wk1r * x0i + wk1i * x0r;
+    x0r = x1r + x3i;
+    x0i = x1i - x3r;
+    y13r = wk1i * x0r - wk1r * x0i;
+    y13i = wk1i * x0i + wk1r * x0r;
+    x0r = a[4] + a[20];
+    x0i = a[5] + a[21];
+    x1r = a[4] - a[20];
+    x1i = a[5] - a[21];
+    x2r = a[12] + a[28];
+    x2i = a[13] + a[29];
+    x3r = a[12] - a[28];
+    x3i = a[13] - a[29];
+    y2r = x0r + x2r;
+    y2i = x0i + x2i;
+    y6r = x0r - x2r;
+    y6i = x0i - x2i;
+    x0r = x1r - x3i;
+    x0i = x1i + x3r;
+    y10r = wn4r * (x0r - x0i);
+    y10i = wn4r * (x0i + x0r);
+    x0r = x1r + x3i;
+    x0i = x1i - x3r;
+    y14r = wn4r * (x0r + x0i);
+    y14i = wn4r * (x0i - x0r);
+    x0r = a[6] + a[22];
+    x0i = a[7] + a[23];
+    x1r = a[6] - a[22];
+    x1i = a[7] - a[23];
+    x2r = a[14] + a[30];
+    x2i = a[15] + a[31];
+    x3r = a[14] - a[30];
+    x3i = a[15] - a[31];
+    y3r = x0r + x2r;
+    y3i = x0i + x2i;
+    y7r = x0r - x2r;
+    y7i = x0i - x2i;
+    x0r = x1r - x3i;
+    x0i = x1i + x3r;
+    y11r = wk1i * x0r - wk1r * x0i;
+    y11i = wk1i * x0i + wk1r * x0r;
+    x0r = x1r + x3i;
+    x0i = x1i - x3r;
+    y15r = wk1r * x0r - wk1i * x0i;
+    y15i = wk1r * x0i + wk1i * x0r;
+    x0r = y12r - y14r;
+    x0i = y12i - y14i;
+    x1r = y12r + y14r;
+    x1i = y12i + y14i;
+    x2r = y13r - y15r;
+    x2i = y13i - y15i;
+    x3r = y13r + y15r;
+    x3i = y13i + y15i;
+    a[24] = x0r + x2r;
+    a[25] = x0i + x2i;
+    a[26] = x0r - x2r;
+    a[27] = x0i - x2i;
+    a[28] = x1r - x3i;
+    a[29] = x1i + x3r;
+    a[30] = x1r + x3i;
+    a[31] = x1i - x3r;
+    x0r = y8r + y10r;
+    x0i = y8i + y10i;
+    x1r = y8r - y10r;
+    x1i = y8i - y10i;
+    x2r = y9r + y11r;
+    x2i = y9i + y11i;
+    x3r = y9r - y11r;
+    x3i = y9i - y11i;
+    a[16] = x0r + x2r;
+    a[17] = x0i + x2i;
+    a[18] = x0r - x2r;
+    a[19] = x0i - x2i;
+    a[20] = x1r - x3i;
+    a[21] = x1i + x3r;
+    a[22] = x1r + x3i;
+    a[23] = x1i - x3r;
+    x0r = y5r - y7i;
+    x0i = y5i + y7r;
+    x2r = wn4r * (x0r - x0i);
+    x2i = wn4r * (x0i + x0r);
+    x0r = y5r + y7i;
+    x0i = y5i - y7r;
+    x3r = wn4r * (x0r - x0i);
+    x3i = wn4r * (x0i + x0r);
+    x0r = y4r - y6i;
+    x0i = y4i + y6r;
+    x1r = y4r + y6i;
+    x1i = y4i - y6r;
+    a[8] = x0r + x2r;
+    a[9] = x0i + x2i;
+    a[10] = x0r - x2r;
+    a[11] = x0i - x2i;
+    a[12] = x1r - x3i;
+    a[13] = x1i + x3r;
+    a[14] = x1r + x3i;
+    a[15] = x1i - x3r;
+    x0r = y0r + y2r;
+    x0i = y0i + y2i;
+    x1r = y0r - y2r;
+    x1i = y0i - y2i;
+    x2r = y1r + y3r;
+    x2i = y1i + y3i;
+    x3r = y1r - y3r;
+    x3i = y1i - y3i;
+    a[0] = x0r + x2r;
+    a[1] = x0i + x2i;
+    a[2] = x0r - x2r;
+    a[3] = x0i - x2i;
+    a[4] = x1r - x3i;
+    a[5] = x1i + x3r;
+    a[6] = x1r + x3i;
+    a[7] = x1i - x3r;
+}
+
+
+void cftf162(double *a, double *w)
+{
+    double wn4r, wk1r, wk1i, wk2r, wk2i, wk3r, wk3i, 
+        x0r, x0i, x1r, x1i, x2r, x2i, 
+        y0r, y0i, y1r, y1i, y2r, y2i, y3r, y3i, 
+        y4r, y4i, y5r, y5i, y6r, y6i, y7r, y7i, 
+        y8r, y8i, y9r, y9i, y10r, y10i, y11r, y11i, 
+        y12r, y12i, y13r, y13i, y14r, y14i, y15r, y15i;
+    
+    wn4r = w[1];
+    wk1r = w[4];
+    wk1i = w[5];
+    wk3r = w[6];
+    wk3i = -w[7];
+    wk2r = w[8];
+    wk2i = w[9];
+    x1r = a[0] - a[17];
+    x1i = a[1] + a[16];
+    x0r = a[8] - a[25];
+    x0i = a[9] + a[24];
+    x2r = wn4r * (x0r - x0i);
+    x2i = wn4r * (x0i + x0r);
+    y0r = x1r + x2r;
+    y0i = x1i + x2i;
+    y4r = x1r - x2r;
+    y4i = x1i - x2i;
+    x1r = a[0] + a[17];
+    x1i = a[1] - a[16];
+    x0r = a[8] + a[25];
+    x0i = a[9] - a[24];
+    x2r = wn4r * (x0r - x0i);
+    x2i = wn4r * (x0i + x0r);
+    y8r = x1r - x2i;
+    y8i = x1i + x2r;
+    y12r = x1r + x2i;
+    y12i = x1i - x2r;
+    x0r = a[2] - a[19];
+    x0i = a[3] + a[18];
+    x1r = wk1r * x0r - wk1i * x0i;
+    x1i = wk1r * x0i + wk1i * x0r;
+    x0r = a[10] - a[27];
+    x0i = a[11] + a[26];
+    x2r = wk3i * x0r - wk3r * x0i;
+    x2i = wk3i * x0i + wk3r * x0r;
+    y1r = x1r + x2r;
+    y1i = x1i + x2i;
+    y5r = x1r - x2r;
+    y5i = x1i - x2i;
+    x0r = a[2] + a[19];
+    x0i = a[3] - a[18];
+    x1r = wk3r * x0r - wk3i * x0i;
+    x1i = wk3r * x0i + wk3i * x0r;
+    x0r = a[10] + a[27];
+    x0i = a[11] - a[26];
+    x2r = wk1r * x0r + wk1i * x0i;
+    x2i = wk1r * x0i - wk1i * x0r;
+    y9r = x1r - x2r;
+    y9i = x1i - x2i;
+    y13r = x1r + x2r;
+    y13i = x1i + x2i;
+    x0r = a[4] - a[21];
+    x0i = a[5] + a[20];
+    x1r = wk2r * x0r - wk2i * x0i;
+    x1i = wk2r * x0i + wk2i * x0r;
+    x0r = a[12] - a[29];
+    x0i = a[13] + a[28];
+    x2r = wk2i * x0r - wk2r * x0i;
+    x2i = wk2i * x0i + wk2r * x0r;
+    y2r = x1r + x2r;
+    y2i = x1i + x2i;
+    y6r = x1r - x2r;
+    y6i = x1i - x2i;
+    x0r = a[4] + a[21];
+    x0i = a[5] - a[20];
+    x1r = wk2i * x0r - wk2r * x0i;
+    x1i = wk2i * x0i + wk2r * x0r;
+    x0r = a[12] + a[29];
+    x0i = a[13] - a[28];
+    x2r = wk2r * x0r - wk2i * x0i;
+    x2i = wk2r * x0i + wk2i * x0r;
+    y10r = x1r - x2r;
+    y10i = x1i - x2i;
+    y14r = x1r + x2r;
+    y14i = x1i + x2i;
+    x0r = a[6] - a[23];
+    x0i = a[7] + a[22];
+    x1r = wk3r * x0r - wk3i * x0i;
+    x1i = wk3r * x0i + wk3i * x0r;
+    x0r = a[14] - a[31];
+    x0i = a[15] + a[30];
+    x2r = wk1i * x0r - wk1r * x0i;
+    x2i = wk1i * x0i + wk1r * x0r;
+    y3r = x1r + x2r;
+    y3i = x1i + x2i;
+    y7r = x1r - x2r;
+    y7i = x1i - x2i;
+    x0r = a[6] + a[23];
+    x0i = a[7] - a[22];
+    x1r = wk1i * x0r + wk1r * x0i;
+    x1i = wk1i * x0i - wk1r * x0r;
+    x0r = a[14] + a[31];
+    x0i = a[15] - a[30];
+    x2r = wk3i * x0r - wk3r * x0i;
+    x2i = wk3i * x0i + wk3r * x0r;
+    y11r = x1r + x2r;
+    y11i = x1i + x2i;
+    y15r = x1r - x2r;
+    y15i = x1i - x2i;
+    x1r = y0r + y2r;
+    x1i = y0i + y2i;
+    x2r = y1r + y3r;
+    x2i = y1i + y3i;
+    a[0] = x1r + x2r;
+    a[1] = x1i + x2i;
+    a[2] = x1r - x2r;
+    a[3] = x1i - x2i;
+    x1r = y0r - y2r;
+    x1i = y0i - y2i;
+    x2r = y1r - y3r;
+    x2i = y1i - y3i;
+    a[4] = x1r - x2i;
+    a[5] = x1i + x2r;
+    a[6] = x1r + x2i;
+    a[7] = x1i - x2r;
+    x1r = y4r - y6i;
+    x1i = y4i + y6r;
+    x0r = y5r - y7i;
+    x0i = y5i + y7r;
+    x2r = wn4r * (x0r - x0i);
+    x2i = wn4r * (x0i + x0r);
+    a[8] = x1r + x2r;
+    a[9] = x1i + x2i;
+    a[10] = x1r - x2r;
+    a[11] = x1i - x2i;
+    x1r = y4r + y6i;
+    x1i = y4i - y6r;
+    x0r = y5r + y7i;
+    x0i = y5i - y7r;
+    x2r = wn4r * (x0r - x0i);
+    x2i = wn4r * (x0i + x0r);
+    a[12] = x1r - x2i;
+    a[13] = x1i + x2r;
+    a[14] = x1r + x2i;
+    a[15] = x1i - x2r;
+    x1r = y8r + y10r;
+    x1i = y8i + y10i;
+    x2r = y9r - y11r;
+    x2i = y9i - y11i;
+    a[16] = x1r + x2r;
+    a[17] = x1i + x2i;
+    a[18] = x1r - x2r;
+    a[19] = x1i - x2i;
+    x1r = y8r - y10r;
+    x1i = y8i - y10i;
+    x2r = y9r + y11r;
+    x2i = y9i + y11i;
+    a[20] = x1r - x2i;
+    a[21] = x1i + x2r;
+    a[22] = x1r + x2i;
+    a[23] = x1i - x2r;
+    x1r = y12r - y14i;
+    x1i = y12i + y14r;
+    x0r = y13r + y15i;
+    x0i = y13i - y15r;
+    x2r = wn4r * (x0r - x0i);
+    x2i = wn4r * (x0i + x0r);
+    a[24] = x1r + x2r;
+    a[25] = x1i + x2i;
+    a[26] = x1r - x2r;
+    a[27] = x1i - x2i;
+    x1r = y12r + y14i;
+    x1i = y12i - y14r;
+    x0r = y13r - y15i;
+    x0i = y13i + y15r;
+    x2r = wn4r * (x0r - x0i);
+    x2i = wn4r * (x0i + x0r);
+    a[28] = x1r - x2i;
+    a[29] = x1i + x2r;
+    a[30] = x1r + x2i;
+    a[31] = x1i - x2r;
+}
+
+
+void cftf081(double *a, double *w)
+{
+    double wn4r, x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i, 
+        y0r, y0i, y1r, y1i, y2r, y2i, y3r, y3i, 
+        y4r, y4i, y5r, y5i, y6r, y6i, y7r, y7i;
+    
+    wn4r = w[1];
+    x0r = a[0] + a[8];
+    x0i = a[1] + a[9];
+    x1r = a[0] - a[8];
+    x1i = a[1] - a[9];
+    x2r = a[4] + a[12];
+    x2i = a[5] + a[13];
+    x3r = a[4] - a[12];
+    x3i = a[5] - a[13];
+    y0r = x0r + x2r;
+    y0i = x0i + x2i;
+    y2r = x0r - x2r;
+    y2i = x0i - x2i;
+    y1r = x1r - x3i;
+    y1i = x1i + x3r;
+    y3r = x1r + x3i;
+    y3i = x1i - x3r;
+    x0r = a[2] + a[10];
+    x0i = a[3] + a[11];
+    x1r = a[2] - a[10];
+    x1i = a[3] - a[11];
+    x2r = a[6] + a[14];
+    x2i = a[7] + a[15];
+    x3r = a[6] - a[14];
+    x3i = a[7] - a[15];
+    y4r = x0r + x2r;
+    y4i = x0i + x2i;
+    y6r = x0r - x2r;
+    y6i = x0i - x2i;
+    x0r = x1r - x3i;
+    x0i = x1i + x3r;
+    x2r = x1r + x3i;
+    x2i = x1i - x3r;
+    y5r = wn4r * (x0r - x0i);
+    y5i = wn4r * (x0r + x0i);
+    y7r = wn4r * (x2r - x2i);
+    y7i = wn4r * (x2r + x2i);
+    a[8] = y1r + y5r;
+    a[9] = y1i + y5i;
+    a[10] = y1r - y5r;
+    a[11] = y1i - y5i;
+    a[12] = y3r - y7i;
+    a[13] = y3i + y7r;
+    a[14] = y3r + y7i;
+    a[15] = y3i - y7r;
+    a[0] = y0r + y4r;
+    a[1] = y0i + y4i;
+    a[2] = y0r - y4r;
+    a[3] = y0i - y4i;
+    a[4] = y2r - y6i;
+    a[5] = y2i + y6r;
+    a[6] = y2r + y6i;
+    a[7] = y2i - y6r;
+}
+
+
+void cftf082(double *a, double *w)
+{
+    double wn4r, wk1r, wk1i, x0r, x0i, x1r, x1i, 
+        y0r, y0i, y1r, y1i, y2r, y2i, y3r, y3i, 
+        y4r, y4i, y5r, y5i, y6r, y6i, y7r, y7i;
+    
+    wn4r = w[1];
+    wk1r = w[2];
+    wk1i = w[3];
+    y0r = a[0] - a[9];
+    y0i = a[1] + a[8];
+    y1r = a[0] + a[9];
+    y1i = a[1] - a[8];
+    x0r = a[4] - a[13];
+    x0i = a[5] + a[12];
+    y2r = wn4r * (x0r - x0i);
+    y2i = wn4r * (x0i + x0r);
+    x0r = a[4] + a[13];
+    x0i = a[5] - a[12];
+    y3r = wn4r * (x0r - x0i);
+    y3i = wn4r * (x0i + x0r);
+    x0r = a[2] - a[11];
+    x0i = a[3] + a[10];
+    y4r = wk1r * x0r - wk1i * x0i;
+    y4i = wk1r * x0i + wk1i * x0r;
+    x0r = a[2] + a[11];
+    x0i = a[3] - a[10];
+    y5r = wk1i * x0r - wk1r * x0i;
+    y5i = wk1i * x0i + wk1r * x0r;
+    x0r = a[6] - a[15];
+    x0i = a[7] + a[14];
+    y6r = wk1i * x0r - wk1r * x0i;
+    y6i = wk1i * x0i + wk1r * x0r;
+    x0r = a[6] + a[15];
+    x0i = a[7] - a[14];
+    y7r = wk1r * x0r - wk1i * x0i;
+    y7i = wk1r * x0i + wk1i * x0r;
+    x0r = y0r + y2r;
+    x0i = y0i + y2i;
+    x1r = y4r + y6r;
+    x1i = y4i + y6i;
+    a[0] = x0r + x1r;
+    a[1] = x0i + x1i;
+    a[2] = x0r - x1r;
+    a[3] = x0i - x1i;
+    x0r = y0r - y2r;
+    x0i = y0i - y2i;
+    x1r = y4r - y6r;
+    x1i = y4i - y6i;
+    a[4] = x0r - x1i;
+    a[5] = x0i + x1r;
+    a[6] = x0r + x1i;
+    a[7] = x0i - x1r;
+    x0r = y1r - y3i;
+    x0i = y1i + y3r;
+    x1r = y5r - y7r;
+    x1i = y5i - y7i;
+    a[8] = x0r + x1r;
+    a[9] = x0i + x1i;
+    a[10] = x0r - x1r;
+    a[11] = x0i - x1i;
+    x0r = y1r + y3i;
+    x0i = y1i - y3r;
+    x1r = y5r + y7r;
+    x1i = y5i + y7i;
+    a[12] = x0r - x1i;
+    a[13] = x0i + x1r;
+    a[14] = x0r + x1i;
+    a[15] = x0i - x1r;
+}
+
+
+void cftf040(double *a)
+{
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i;
+    
+    x0r = a[0] + a[4];
+    x0i = a[1] + a[5];
+    x1r = a[0] - a[4];
+    x1i = a[1] - a[5];
+    x2r = a[2] + a[6];
+    x2i = a[3] + a[7];
+    x3r = a[2] - a[6];
+    x3i = a[3] - a[7];
+    a[0] = x0r + x2r;
+    a[1] = x0i + x2i;
+    a[2] = x1r - x3i;
+    a[3] = x1i + x3r;
+    a[4] = x0r - x2r;
+    a[5] = x0i - x2i;
+    a[6] = x1r + x3i;
+    a[7] = x1i - x3r;
+}
+
+
+void cftb040(double *a)
+{
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i;
+    
+    x0r = a[0] + a[4];
+    x0i = a[1] + a[5];
+    x1r = a[0] - a[4];
+    x1i = a[1] - a[5];
+    x2r = a[2] + a[6];
+    x2i = a[3] + a[7];
+    x3r = a[2] - a[6];
+    x3i = a[3] - a[7];
+    a[0] = x0r + x2r;
+    a[1] = x0i + x2i;
+    a[2] = x1r + x3i;
+    a[3] = x1i - x3r;
+    a[4] = x0r - x2r;
+    a[5] = x0i - x2i;
+    a[6] = x1r - x3i;
+    a[7] = x1i + x3r;
+}
+
+
+void cftx020(double *a)
+{
+    double x0r, x0i;
+    
+    x0r = a[0] - a[2];
+    x0i = a[1] - a[3];
+    a[0] += a[2];
+    a[1] += a[3];
+    a[2] = x0r;
+    a[3] = x0i;
+}
+
+
+void rftfsub(int n, double *a, int nc, double *c)
+{
+    int j, k, kk, ks, m;
+    double wkr, wki, xr, xi, yr, yi;
+    
+    m = n >> 1;
+    ks = 2 * nc / m;
+    kk = 0;
+    for (j = 2; j < m; j += 2) {
+        k = n - j;
+        kk += ks;
+        wkr = 0.5 - c[nc - kk];
+        wki = c[kk];
+        xr = a[j] - a[k];
+        xi = a[j + 1] + a[k + 1];
+        yr = wkr * xr - wki * xi;
+        yi = wkr * xi + wki * xr;
+        a[j] -= yr;
+        a[j + 1] -= yi;
+        a[k] += yr;
+        a[k + 1] -= yi;
+    }
+}
+
+
+void rftbsub(int n, double *a, int nc, double *c)
+{
+    int j, k, kk, ks, m;
+    double wkr, wki, xr, xi, yr, yi;
+    
+    m = n >> 1;
+    ks = 2 * nc / m;
+    kk = 0;
+    for (j = 2; j < m; j += 2) {
+        k = n - j;
+        kk += ks;
+        wkr = 0.5 - c[nc - kk];
+        wki = c[kk];
+        xr = a[j] - a[k];
+        xi = a[j + 1] + a[k + 1];
+        yr = wkr * xr + wki * xi;
+        yi = wkr * xi - wki * xr;
+        a[j] -= yr;
+        a[j + 1] -= yi;
+        a[k] += yr;
+        a[k + 1] -= yi;
+    }
+}
+
+
+void dctsub(int n, double *a, int nc, double *c)
+{
+    int j, k, kk, ks, m;
+    double wkr, wki, xr;
+    
+    m = n >> 1;
+    ks = nc / n;
+    kk = 0;
+    for (j = 1; j < m; j++) {
+        k = n - j;
+        kk += ks;
+        wkr = c[kk] - c[nc - kk];
+        wki = c[kk] + c[nc - kk];
+        xr = wki * a[j] - wkr * a[k];
+        a[j] = wkr * a[j] + wki * a[k];
+        a[k] = xr;
+    }
+    a[m] *= c[0];
+}
+
+
+void dstsub(int n, double *a, int nc, double *c)
+{
+    int j, k, kk, ks, m;
+    double wkr, wki, xr;
+    
+    m = n >> 1;
+    ks = nc / n;
+    kk = 0;
+    for (j = 1; j < m; j++) {
+        k = n - j;
+        kk += ks;
+        wkr = c[kk] - c[nc - kk];
+        wki = c[kk] + c[nc - kk];
+        xr = wki * a[k] - wkr * a[j];
+        a[k] = wkr * a[k] + wki * a[j];
+        a[j] = xr;
+    }
+    a[m] *= c[0];
+}
+
diff --git a/third_party/tflite-micro/third_party/fft2d/fftsg_h.c b/third_party/tflite-micro/third_party/fft2d/fftsg_h.c
new file mode 100644
index 00000000..e4c47c2a
--- /dev/null
+++ b/third_party/tflite-micro/third_party/fft2d/fftsg_h.c
@@ -0,0 +1,3447 @@
+/*
+Fast Fourier/Cosine/Sine Transform
+    dimension   :one
+    data length :power of 2
+    decimation  :frequency
+    radix       :split-radix
+    data        :inplace
+    table       :not use
+functions
+    cdft: Complex Discrete Fourier Transform
+    rdft: Real Discrete Fourier Transform
+    ddct: Discrete Cosine Transform
+    ddst: Discrete Sine Transform
+    dfct: Cosine Transform of RDFT (Real Symmetric DFT)
+    dfst: Sine Transform of RDFT (Real Anti-symmetric DFT)
+function prototypes
+    void cdft(int, int, double *);
+    void rdft(int, int, double *);
+    void ddct(int, int, double *);
+    void ddst(int, int, double *);
+    void dfct(int, double *);
+    void dfst(int, double *);
+macro definitions
+    USE_CDFT_PTHREADS : default=not defined
+        CDFT_THREADS_BEGIN_N  : must be >= 512, default=8192
+        CDFT_4THREADS_BEGIN_N : must be >= 512, default=65536
+    USE_CDFT_WINTHREADS : default=not defined
+        CDFT_THREADS_BEGIN_N  : must be >= 512, default=32768
+        CDFT_4THREADS_BEGIN_N : must be >= 512, default=524288
+
+
+-------- Complex DFT (Discrete Fourier Transform) --------
+    [definition]
+        <case1>
+            X[k] = sum_j=0^n-1 x[j]*exp(2*pi*i*j*k/n), 0<=k<n
+        <case2>
+            X[k] = sum_j=0^n-1 x[j]*exp(-2*pi*i*j*k/n), 0<=k<n
+        (notes: sum_j=0^n-1 is a summation from j=0 to n-1)
+    [usage]
+        <case1>
+            cdft(2*n, 1, a);
+        <case2>
+            cdft(2*n, -1, a);
+    [parameters]
+        2*n            :data length (int)
+                        n >= 1, n = power of 2
+        a[0...2*n-1]   :input/output data (double *)
+                        input data
+                            a[2*j] = Re(x[j]), 
+                            a[2*j+1] = Im(x[j]), 0<=j<n
+                        output data
+                            a[2*k] = Re(X[k]), 
+                            a[2*k+1] = Im(X[k]), 0<=k<n
+    [remark]
+        Inverse of 
+            cdft(2*n, -1, a);
+        is 
+            cdft(2*n, 1, a);
+            for (j = 0; j <= 2 * n - 1; j++) {
+                a[j] *= 1.0 / n;
+            }
+        .
+
+
+-------- Real DFT / Inverse of Real DFT --------
+    [definition]
+        <case1> RDFT
+            R[k] = sum_j=0^n-1 a[j]*cos(2*pi*j*k/n), 0<=k<=n/2
+            I[k] = sum_j=0^n-1 a[j]*sin(2*pi*j*k/n), 0<k<n/2
+        <case2> IRDFT (excluding scale)
+            a[k] = (R[0] + R[n/2]*cos(pi*k))/2 + 
+                   sum_j=1^n/2-1 R[j]*cos(2*pi*j*k/n) + 
+                   sum_j=1^n/2-1 I[j]*sin(2*pi*j*k/n), 0<=k<n
+    [usage]
+        <case1>
+            rdft(n, 1, a);
+        <case2>
+            rdft(n, -1, a);
+    [parameters]
+        n              :data length (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        <case1>
+                            output data
+                                a[2*k] = R[k], 0<=k<n/2
+                                a[2*k+1] = I[k], 0<k<n/2
+                                a[1] = R[n/2]
+                        <case2>
+                            input data
+                                a[2*j] = R[j], 0<=j<n/2
+                                a[2*j+1] = I[j], 0<j<n/2
+                                a[1] = R[n/2]
+    [remark]
+        Inverse of 
+            rdft(n, 1, a);
+        is 
+            rdft(n, -1, a);
+            for (j = 0; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- DCT (Discrete Cosine Transform) / Inverse of DCT --------
+    [definition]
+        <case1> IDCT (excluding scale)
+            C[k] = sum_j=0^n-1 a[j]*cos(pi*j*(k+1/2)/n), 0<=k<n
+        <case2> DCT
+            C[k] = sum_j=0^n-1 a[j]*cos(pi*(j+1/2)*k/n), 0<=k<n
+    [usage]
+        <case1>
+            ddct(n, 1, a);
+        <case2>
+            ddct(n, -1, a);
+    [parameters]
+        n              :data length (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        output data
+                            a[k] = C[k], 0<=k<n
+    [remark]
+        Inverse of 
+            ddct(n, -1, a);
+        is 
+            a[0] *= 0.5;
+            ddct(n, 1, a);
+            for (j = 0; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- DST (Discrete Sine Transform) / Inverse of DST --------
+    [definition]
+        <case1> IDST (excluding scale)
+            S[k] = sum_j=1^n A[j]*sin(pi*j*(k+1/2)/n), 0<=k<n
+        <case2> DST
+            S[k] = sum_j=0^n-1 a[j]*sin(pi*(j+1/2)*k/n), 0<k<=n
+    [usage]
+        <case1>
+            ddst(n, 1, a);
+        <case2>
+            ddst(n, -1, a);
+    [parameters]
+        n              :data length (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        <case1>
+                            input data
+                                a[j] = A[j], 0<j<n
+                                a[0] = A[n]
+                            output data
+                                a[k] = S[k], 0<=k<n
+                        <case2>
+                            output data
+                                a[k] = S[k], 0<k<n
+                                a[0] = S[n]
+    [remark]
+        Inverse of 
+            ddst(n, -1, a);
+        is 
+            a[0] *= 0.5;
+            ddst(n, 1, a);
+            for (j = 0; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- Cosine Transform of RDFT (Real Symmetric DFT) --------
+    [definition]
+        C[k] = sum_j=0^n a[j]*cos(pi*j*k/n), 0<=k<=n
+    [usage]
+        dfct(n, a);
+    [parameters]
+        n              :data length - 1 (int)
+                        n >= 2, n = power of 2
+        a[0...n]       :input/output data (double *)
+                        output data
+                            a[k] = C[k], 0<=k<=n
+    [remark]
+        Inverse of 
+            a[0] *= 0.5;
+            a[n] *= 0.5;
+            dfct(n, a);
+        is 
+            a[0] *= 0.5;
+            a[n] *= 0.5;
+            dfct(n, a);
+            for (j = 0; j <= n; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+
+
+-------- Sine Transform of RDFT (Real Anti-symmetric DFT) --------
+    [definition]
+        S[k] = sum_j=1^n-1 a[j]*sin(pi*j*k/n), 0<k<n
+    [usage]
+        dfst(n, a);
+    [parameters]
+        n              :data length + 1 (int)
+                        n >= 2, n = power of 2
+        a[0...n-1]     :input/output data (double *)
+                        output data
+                            a[k] = S[k], 0<k<n
+                        (a[0] is used for work area)
+    [remark]
+        Inverse of 
+            dfst(n, a);
+        is 
+            dfst(n, a);
+            for (j = 1; j <= n - 1; j++) {
+                a[j] *= 2.0 / n;
+            }
+        .
+*/
+
+
+void cdft(int n, int isgn, double *a)
+{
+    void cftfsub(int n, double *a);
+    void cftbsub(int n, double *a);
+    
+    if (isgn >= 0) {
+        cftfsub(n, a);
+    } else {
+        cftbsub(n, a);
+    }
+}
+
+
+void rdft(int n, int isgn, double *a)
+{
+    void cftfsub(int n, double *a);
+    void cftbsub(int n, double *a);
+    void rftfsub(int n, double *a);
+    void rftbsub(int n, double *a);
+    double xi;
+    
+    if (isgn >= 0) {
+        if (n > 4) {
+            cftfsub(n, a);
+            rftfsub(n, a);
+        } else if (n == 4) {
+            cftfsub(n, a);
+        }
+        xi = a[0] - a[1];
+        a[0] += a[1];
+        a[1] = xi;
+    } else {
+        a[1] = 0.5 * (a[0] - a[1]);
+        a[0] -= a[1];
+        if (n > 4) {
+            rftbsub(n, a);
+            cftbsub(n, a);
+        } else if (n == 4) {
+            cftbsub(n, a);
+        }
+    }
+}
+
+
+void ddct(int n, int isgn, double *a)
+{
+    void cftfsub(int n, double *a);
+    void cftbsub(int n, double *a);
+    void rftfsub(int n, double *a);
+    void rftbsub(int n, double *a);
+    void dctsub(int n, double *a);
+    void dctsub4(int n, double *a);
+    int j;
+    double xr;
+    
+    if (isgn < 0) {
+        xr = a[n - 1];
+        for (j = n - 2; j >= 2; j -= 2) {
+            a[j + 1] = a[j] - a[j - 1];
+            a[j] += a[j - 1];
+        }
+        a[1] = a[0] - xr;
+        a[0] += xr;
+        if (n > 4) {
+            rftbsub(n, a);
+            cftbsub(n, a);
+        } else if (n == 4) {
+            cftbsub(n, a);
+        }
+    }
+    if (n > 4) {
+        dctsub(n, a);
+    } else {
+        dctsub4(n, a);
+    }
+    if (isgn >= 0) {
+        if (n > 4) {
+            cftfsub(n, a);
+            rftfsub(n, a);
+        } else if (n == 4) {
+            cftfsub(n, a);
+        }
+        xr = a[0] - a[1];
+        a[0] += a[1];
+        for (j = 2; j < n; j += 2) {
+            a[j - 1] = a[j] - a[j + 1];
+            a[j] += a[j + 1];
+        }
+        a[n - 1] = xr;
+    }
+}
+
+
+void ddst(int n, int isgn, double *a)
+{
+    void cftfsub(int n, double *a);
+    void cftbsub(int n, double *a);
+    void rftfsub(int n, double *a);
+    void rftbsub(int n, double *a);
+    void dstsub(int n, double *a);
+    void dstsub4(int n, double *a);
+    int j;
+    double xr;
+    
+    if (isgn < 0) {
+        xr = a[n - 1];
+        for (j = n - 2; j >= 2; j -= 2) {
+            a[j + 1] = -a[j] - a[j - 1];
+            a[j] -= a[j - 1];
+        }
+        a[1] = a[0] + xr;
+        a[0] -= xr;
+        if (n > 4) {
+            rftbsub(n, a);
+            cftbsub(n, a);
+        } else if (n == 4) {
+            cftbsub(n, a);
+        }
+    }
+    if (n > 4) {
+        dstsub(n, a);
+    } else {
+        dstsub4(n, a);
+    }
+    if (isgn >= 0) {
+        if (n > 4) {
+            cftfsub(n, a);
+            rftfsub(n, a);
+        } else if (n == 4) {
+            cftfsub(n, a);
+        }
+        xr = a[0] - a[1];
+        a[0] += a[1];
+        for (j = 2; j < n; j += 2) {
+            a[j - 1] = -a[j] - a[j + 1];
+            a[j] -= a[j + 1];
+        }
+        a[n - 1] = -xr;
+    }
+}
+
+
+void dfct(int n, double *a)
+{
+    void ddct(int n, int isgn, double *a);
+    void bitrv1(int n, double *a);
+    int j, k, m, mh;
+    double xr, xi, yr, yi, an;
+    
+    m = n >> 1;
+    for (j = 0; j < m; j++) {
+        k = n - j;
+        xr = a[j] + a[k];
+        a[j] -= a[k];
+        a[k] = xr;
+    }
+    an = a[n];
+    while (m >= 2) {
+        ddct(m, 1, a);
+        if (m > 2) {
+            bitrv1(m, a);
+        }
+        mh = m >> 1;
+        xi = a[m];
+        a[m] = a[0];
+        a[0] = an - xi;
+        an += xi;
+        for (j = 1; j < mh; j++) {
+            k = m - j;
+            xr = a[m + k];
+            xi = a[m + j];
+            yr = a[j];
+            yi = a[k];
+            a[m + j] = yr;
+            a[m + k] = yi;
+            a[j] = xr - xi;
+            a[k] = xr + xi;
+        }
+        xr = a[mh];
+        a[mh] = a[m + mh];
+        a[m + mh] = xr;
+        m = mh;
+    }
+    xi = a[1];
+    a[1] = a[0];
+    a[0] = an + xi;
+    a[n] = an - xi;
+    if (n > 2) {
+        bitrv1(n, a);
+    }
+}
+
+
+void dfst(int n, double *a)
+{
+    void ddst(int n, int isgn, double *a);
+    void bitrv1(int n, double *a);
+    int j, k, m, mh;
+    double xr, xi, yr, yi;
+    
+    m = n >> 1;
+    for (j = 1; j < m; j++) {
+        k = n - j;
+        xr = a[j] - a[k];
+        a[j] += a[k];
+        a[k] = xr;
+    }
+    a[0] = a[m];
+    while (m >= 2) {
+        ddst(m, 1, a);
+        if (m > 2) {
+            bitrv1(m, a);
+        }
+        mh = m >> 1;
+        for (j = 1; j < mh; j++) {
+            k = m - j;
+            xr = a[m + k];
+            xi = a[m + j];
+            yr = a[j];
+            yi = a[k];
+            a[m + j] = yr;
+            a[m + k] = yi;
+            a[j] = xr + xi;
+            a[k] = xr - xi;
+        }
+        a[m] = a[0];
+        a[0] = a[m + mh];
+        a[m + mh] = a[mh];
+        m = mh;
+    }
+    a[1] = a[0];
+    a[0] = 0;
+    if (n > 2) {
+        bitrv1(n, a);
+    }
+}
+
+
+/* -------- child routines -------- */
+
+
+#include <math.h>
+#ifndef M_PI_2
+#define M_PI_2      1.570796326794896619231321691639751442098584699687
+#endif
+#ifndef WR5000  /* cos(M_PI_2*0.5000) */
+#define WR5000      0.707106781186547524400844362104849039284835937688
+#endif
+#ifndef WR2500  /* cos(M_PI_2*0.2500) */
+#define WR2500      0.923879532511286756128183189396788286822416625863
+#endif
+#ifndef WI2500  /* sin(M_PI_2*0.2500) */
+#define WI2500      0.382683432365089771728459984030398866761344562485
+#endif
+#ifndef WR1250  /* cos(M_PI_2*0.1250) */
+#define WR1250      0.980785280403230449126182236134239036973933730893
+#endif
+#ifndef WI1250  /* sin(M_PI_2*0.1250) */
+#define WI1250      0.195090322016128267848284868477022240927691617751
+#endif
+#ifndef WR3750  /* cos(M_PI_2*0.3750) */
+#define WR3750      0.831469612302545237078788377617905756738560811987
+#endif
+#ifndef WI3750  /* sin(M_PI_2*0.3750) */
+#define WI3750      0.555570233019602224742830813948532874374937190754
+#endif
+
+
+#ifdef USE_CDFT_PTHREADS
+#define USE_CDFT_THREADS
+#ifndef CDFT_THREADS_BEGIN_N
+#define CDFT_THREADS_BEGIN_N 8192
+#endif
+#ifndef CDFT_4THREADS_BEGIN_N
+#define CDFT_4THREADS_BEGIN_N 65536
+#endif
+#include <pthread.h>
+#include <stdio.h>
+#include <stdlib.h>
+#define cdft_thread_t pthread_t
+#define cdft_thread_create(thp,func,argp) { \
+    if (pthread_create(thp, NULL, func, (void *) argp) != 0) { \
+        fprintf(stderr, "cdft thread error\n"); \
+        exit(1); \
+    } \
+}
+#define cdft_thread_wait(th) { \
+    if (pthread_join(th, NULL) != 0) { \
+        fprintf(stderr, "cdft thread error\n"); \
+        exit(1); \
+    } \
+}
+#endif /* USE_CDFT_PTHREADS */
+
+
+#ifdef USE_CDFT_WINTHREADS
+#define USE_CDFT_THREADS
+#ifndef CDFT_THREADS_BEGIN_N
+#define CDFT_THREADS_BEGIN_N 32768
+#endif
+#ifndef CDFT_4THREADS_BEGIN_N
+#define CDFT_4THREADS_BEGIN_N 524288
+#endif
+#include <windows.h>
+#include <stdio.h>
+#include <stdlib.h>
+#define cdft_thread_t HANDLE
+#define cdft_thread_create(thp,func,argp) { \
+    DWORD thid; \
+    *(thp) = CreateThread(NULL, 0, (LPTHREAD_START_ROUTINE) func, (LPVOID) argp, 0, &thid); \
+    if (*(thp) == 0) { \
+        fprintf(stderr, "cdft thread error\n"); \
+        exit(1); \
+    } \
+}
+#define cdft_thread_wait(th) { \
+    WaitForSingleObject(th, INFINITE); \
+    CloseHandle(th); \
+}
+#endif /* USE_CDFT_WINTHREADS */
+
+
+#ifndef CDFT_LOOP_DIV  /* control of the CDFT's speed & tolerance */
+#define CDFT_LOOP_DIV 32
+#endif
+
+#ifndef RDFT_LOOP_DIV  /* control of the RDFT's speed & tolerance */
+#define RDFT_LOOP_DIV 64
+#endif
+
+#ifndef DCST_LOOP_DIV  /* control of the DCT,DST's speed & tolerance */
+#define DCST_LOOP_DIV 64
+#endif
+
+
+void cftfsub(int n, double *a)
+{
+    void bitrv2(int n, double *a);
+    void bitrv216(double *a);
+    void bitrv208(double *a);
+    void cftmdl1(int n, double *a);
+    void cftrec4(int n, double *a);
+    void cftleaf(int n, int isplt, double *a);
+    void cftfx41(int n, double *a);
+    void cftf161(double *a);
+    void cftf081(double *a);
+    void cftf040(double *a);
+    void cftx020(double *a);
+#ifdef USE_CDFT_THREADS
+    void cftrec4_th(int n, double *a);
+#endif /* USE_CDFT_THREADS */
+    
+    if (n > 8) {
+        if (n > 32) {
+            cftmdl1(n, a);
+#ifdef USE_CDFT_THREADS
+            if (n > CDFT_THREADS_BEGIN_N) {
+                cftrec4_th(n, a);
+            } else 
+#endif /* USE_CDFT_THREADS */
+            if (n > 512) {
+                cftrec4(n, a);
+            } else if (n > 128) {
+                cftleaf(n, 1, a);
+            } else {
+                cftfx41(n, a);
+            }
+            bitrv2(n, a);
+        } else if (n == 32) {
+            cftf161(a);
+            bitrv216(a);
+        } else {
+            cftf081(a);
+            bitrv208(a);
+        }
+    } else if (n == 8) {
+        cftf040(a);
+    } else if (n == 4) {
+        cftx020(a);
+    }
+}
+
+
+void cftbsub(int n, double *a)
+{
+    void bitrv2conj(int n, double *a);
+    void bitrv216neg(double *a);
+    void bitrv208neg(double *a);
+    void cftb1st(int n, double *a);
+    void cftrec4(int n, double *a);
+    void cftleaf(int n, int isplt, double *a);
+    void cftfx41(int n, double *a);
+    void cftf161(double *a);
+    void cftf081(double *a);
+    void cftb040(double *a);
+    void cftx020(double *a);
+#ifdef USE_CDFT_THREADS
+    void cftrec4_th(int n, double *a);
+#endif /* USE_CDFT_THREADS */
+    
+    if (n > 8) {
+        if (n > 32) {
+            cftb1st(n, a);
+#ifdef USE_CDFT_THREADS
+            if (n > CDFT_THREADS_BEGIN_N) {
+                cftrec4_th(n, a);
+            } else 
+#endif /* USE_CDFT_THREADS */
+            if (n > 512) {
+                cftrec4(n, a);
+            } else if (n > 128) {
+                cftleaf(n, 1, a);
+            } else {
+                cftfx41(n, a);
+            }
+            bitrv2conj(n, a);
+        } else if (n == 32) {
+            cftf161(a);
+            bitrv216neg(a);
+        } else {
+            cftf081(a);
+            bitrv208neg(a);
+        }
+    } else if (n == 8) {
+        cftb040(a);
+    } else if (n == 4) {
+        cftx020(a);
+    }
+}
+
+
+void bitrv2(int n, double *a)
+{
+    int j0, k0, j1, k1, l, m, i, j, k, nh;
+    double xr, xi, yr, yi;
+    
+    m = 4;
+    for (l = n >> 2; l > 8; l >>= 2) {
+        m <<= 1;
+    }
+    nh = n >> 1;
+    if (l == 8) {
+        j0 = 0;
+        for (k0 = 0; k0 < m; k0 += 4) {
+            k = k0;
+            for (j = j0; j < j0 + k0; j += 4) {
+                xr = a[j];
+                xi = a[j + 1];
+                yr = a[k];
+                yi = a[k + 1];
+                a[j] = yr;
+                a[j + 1] = yi;
+                a[k] = xr;
+                a[k + 1] = xi;
+                j1 = j + m;
+                k1 = k + 2 * m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m;
+                k1 -= m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m;
+                k1 += 2 * m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nh;
+                k1 += 2;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= m;
+                k1 -= 2 * m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= m;
+                k1 += m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= m;
+                k1 -= 2 * m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += 2;
+                k1 += nh;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m;
+                k1 += 2 * m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m;
+                k1 -= m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m;
+                k1 += 2 * m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nh;
+                k1 -= 2;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= m;
+                k1 -= 2 * m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= m;
+                k1 += m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= m;
+                k1 -= 2 * m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                for (i = nh >> 1; i > (k ^= i); i >>= 1);
+            }
+            k1 = j0 + k0;
+            j1 = k1 + 2;
+            k1 += nh;
+            xr = a[j1];
+            xi = a[j1 + 1];
+            yr = a[k1];
+            yi = a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            j1 += m;
+            k1 += 2 * m;
+            xr = a[j1];
+            xi = a[j1 + 1];
+            yr = a[k1];
+            yi = a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            j1 += m;
+            k1 -= m;
+            xr = a[j1];
+            xi = a[j1 + 1];
+            yr = a[k1];
+            yi = a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            j1 -= 2;
+            k1 -= nh;
+            xr = a[j1];
+            xi = a[j1 + 1];
+            yr = a[k1];
+            yi = a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            j1 += nh + 2;
+            k1 += nh + 2;
+            xr = a[j1];
+            xi = a[j1 + 1];
+            yr = a[k1];
+            yi = a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            j1 -= nh - m;
+            k1 += 2 * m - 2;
+            xr = a[j1];
+            xi = a[j1 + 1];
+            yr = a[k1];
+            yi = a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            for (i = nh >> 1; i > (j0 ^= i); i >>= 1);
+        }
+    } else {
+        j0 = 0;
+        for (k0 = 0; k0 < m; k0 += 4) {
+            k = k0;
+            for (j = j0; j < j0 + k0; j += 4) {
+                xr = a[j];
+                xi = a[j + 1];
+                yr = a[k];
+                yi = a[k + 1];
+                a[j] = yr;
+                a[j + 1] = yi;
+                a[k] = xr;
+                a[k + 1] = xi;
+                j1 = j + m;
+                k1 = k + m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nh;
+                k1 += 2;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= m;
+                k1 -= m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += 2;
+                k1 += nh;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m;
+                k1 += m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nh;
+                k1 -= 2;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= m;
+                k1 -= m;
+                xr = a[j1];
+                xi = a[j1 + 1];
+                yr = a[k1];
+                yi = a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                for (i = nh >> 1; i > (k ^= i); i >>= 1);
+            }
+            k1 = j0 + k0;
+            j1 = k1 + 2;
+            k1 += nh;
+            xr = a[j1];
+            xi = a[j1 + 1];
+            yr = a[k1];
+            yi = a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            j1 += m;
+            k1 += m;
+            xr = a[j1];
+            xi = a[j1 + 1];
+            yr = a[k1];
+            yi = a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            for (i = nh >> 1; i > (j0 ^= i); i >>= 1);
+        }
+    }
+}
+
+
+void bitrv2conj(int n, double *a)
+{
+    int j0, k0, j1, k1, l, m, i, j, k, nh;
+    double xr, xi, yr, yi;
+    
+    m = 4;
+    for (l = n >> 2; l > 8; l >>= 2) {
+        m <<= 1;
+    }
+    nh = n >> 1;
+    if (l == 8) {
+        j0 = 0;
+        for (k0 = 0; k0 < m; k0 += 4) {
+            k = k0;
+            for (j = j0; j < j0 + k0; j += 4) {
+                xr = a[j];
+                xi = -a[j + 1];
+                yr = a[k];
+                yi = -a[k + 1];
+                a[j] = yr;
+                a[j + 1] = yi;
+                a[k] = xr;
+                a[k + 1] = xi;
+                j1 = j + m;
+                k1 = k + 2 * m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m;
+                k1 -= m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m;
+                k1 += 2 * m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nh;
+                k1 += 2;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= m;
+                k1 -= 2 * m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= m;
+                k1 += m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= m;
+                k1 -= 2 * m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += 2;
+                k1 += nh;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m;
+                k1 += 2 * m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m;
+                k1 -= m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m;
+                k1 += 2 * m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nh;
+                k1 -= 2;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= m;
+                k1 -= 2 * m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= m;
+                k1 += m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= m;
+                k1 -= 2 * m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                for (i = nh >> 1; i > (k ^= i); i >>= 1);
+            }
+            k1 = j0 + k0;
+            j1 = k1 + 2;
+            k1 += nh;
+            a[j1 - 1] = -a[j1 - 1];
+            xr = a[j1];
+            xi = -a[j1 + 1];
+            yr = a[k1];
+            yi = -a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            a[k1 + 3] = -a[k1 + 3];
+            j1 += m;
+            k1 += 2 * m;
+            xr = a[j1];
+            xi = -a[j1 + 1];
+            yr = a[k1];
+            yi = -a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            j1 += m;
+            k1 -= m;
+            xr = a[j1];
+            xi = -a[j1 + 1];
+            yr = a[k1];
+            yi = -a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            j1 -= 2;
+            k1 -= nh;
+            xr = a[j1];
+            xi = -a[j1 + 1];
+            yr = a[k1];
+            yi = -a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            j1 += nh + 2;
+            k1 += nh + 2;
+            xr = a[j1];
+            xi = -a[j1 + 1];
+            yr = a[k1];
+            yi = -a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            j1 -= nh - m;
+            k1 += 2 * m - 2;
+            a[j1 - 1] = -a[j1 - 1];
+            xr = a[j1];
+            xi = -a[j1 + 1];
+            yr = a[k1];
+            yi = -a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            a[k1 + 3] = -a[k1 + 3];
+            for (i = nh >> 1; i > (j0 ^= i); i >>= 1);
+        }
+    } else {
+        j0 = 0;
+        for (k0 = 0; k0 < m; k0 += 4) {
+            k = k0;
+            for (j = j0; j < j0 + k0; j += 4) {
+                xr = a[j];
+                xi = -a[j + 1];
+                yr = a[k];
+                yi = -a[k + 1];
+                a[j] = yr;
+                a[j + 1] = yi;
+                a[k] = xr;
+                a[k + 1] = xi;
+                j1 = j + m;
+                k1 = k + m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += nh;
+                k1 += 2;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= m;
+                k1 -= m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += 2;
+                k1 += nh;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 += m;
+                k1 += m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= nh;
+                k1 -= 2;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                j1 -= m;
+                k1 -= m;
+                xr = a[j1];
+                xi = -a[j1 + 1];
+                yr = a[k1];
+                yi = -a[k1 + 1];
+                a[j1] = yr;
+                a[j1 + 1] = yi;
+                a[k1] = xr;
+                a[k1 + 1] = xi;
+                for (i = nh >> 1; i > (k ^= i); i >>= 1);
+            }
+            k1 = j0 + k0;
+            j1 = k1 + 2;
+            k1 += nh;
+            a[j1 - 1] = -a[j1 - 1];
+            xr = a[j1];
+            xi = -a[j1 + 1];
+            yr = a[k1];
+            yi = -a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            a[k1 + 3] = -a[k1 + 3];
+            j1 += m;
+            k1 += m;
+            a[j1 - 1] = -a[j1 - 1];
+            xr = a[j1];
+            xi = -a[j1 + 1];
+            yr = a[k1];
+            yi = -a[k1 + 1];
+            a[j1] = yr;
+            a[j1 + 1] = yi;
+            a[k1] = xr;
+            a[k1 + 1] = xi;
+            a[k1 + 3] = -a[k1 + 3];
+            for (i = nh >> 1; i > (j0 ^= i); i >>= 1);
+        }
+    }
+}
+
+
+void bitrv216(double *a)
+{
+    double x1r, x1i, x2r, x2i, x3r, x3i, x4r, x4i, 
+        x5r, x5i, x7r, x7i, x8r, x8i, x10r, x10i, 
+        x11r, x11i, x12r, x12i, x13r, x13i, x14r, x14i;
+    
+    x1r = a[2];
+    x1i = a[3];
+    x2r = a[4];
+    x2i = a[5];
+    x3r = a[6];
+    x3i = a[7];
+    x4r = a[8];
+    x4i = a[9];
+    x5r = a[10];
+    x5i = a[11];
+    x7r = a[14];
+    x7i = a[15];
+    x8r = a[16];
+    x8i = a[17];
+    x10r = a[20];
+    x10i = a[21];
+    x11r = a[22];
+    x11i = a[23];
+    x12r = a[24];
+    x12i = a[25];
+    x13r = a[26];
+    x13i = a[27];
+    x14r = a[28];
+    x14i = a[29];
+    a[2] = x8r;
+    a[3] = x8i;
+    a[4] = x4r;
+    a[5] = x4i;
+    a[6] = x12r;
+    a[7] = x12i;
+    a[8] = x2r;
+    a[9] = x2i;
+    a[10] = x10r;
+    a[11] = x10i;
+    a[14] = x14r;
+    a[15] = x14i;
+    a[16] = x1r;
+    a[17] = x1i;
+    a[20] = x5r;
+    a[21] = x5i;
+    a[22] = x13r;
+    a[23] = x13i;
+    a[24] = x3r;
+    a[25] = x3i;
+    a[26] = x11r;
+    a[27] = x11i;
+    a[28] = x7r;
+    a[29] = x7i;
+}
+
+
+void bitrv216neg(double *a)
+{
+    double x1r, x1i, x2r, x2i, x3r, x3i, x4r, x4i, 
+        x5r, x5i, x6r, x6i, x7r, x7i, x8r, x8i, 
+        x9r, x9i, x10r, x10i, x11r, x11i, x12r, x12i, 
+        x13r, x13i, x14r, x14i, x15r, x15i;
+    
+    x1r = a[2];
+    x1i = a[3];
+    x2r = a[4];
+    x2i = a[5];
+    x3r = a[6];
+    x3i = a[7];
+    x4r = a[8];
+    x4i = a[9];
+    x5r = a[10];
+    x5i = a[11];
+    x6r = a[12];
+    x6i = a[13];
+    x7r = a[14];
+    x7i = a[15];
+    x8r = a[16];
+    x8i = a[17];
+    x9r = a[18];
+    x9i = a[19];
+    x10r = a[20];
+    x10i = a[21];
+    x11r = a[22];
+    x11i = a[23];
+    x12r = a[24];
+    x12i = a[25];
+    x13r = a[26];
+    x13i = a[27];
+    x14r = a[28];
+    x14i = a[29];
+    x15r = a[30];
+    x15i = a[31];
+    a[2] = x15r;
+    a[3] = x15i;
+    a[4] = x7r;
+    a[5] = x7i;
+    a[6] = x11r;
+    a[7] = x11i;
+    a[8] = x3r;
+    a[9] = x3i;
+    a[10] = x13r;
+    a[11] = x13i;
+    a[12] = x5r;
+    a[13] = x5i;
+    a[14] = x9r;
+    a[15] = x9i;
+    a[16] = x1r;
+    a[17] = x1i;
+    a[18] = x14r;
+    a[19] = x14i;
+    a[20] = x6r;
+    a[21] = x6i;
+    a[22] = x10r;
+    a[23] = x10i;
+    a[24] = x2r;
+    a[25] = x2i;
+    a[26] = x12r;
+    a[27] = x12i;
+    a[28] = x4r;
+    a[29] = x4i;
+    a[30] = x8r;
+    a[31] = x8i;
+}
+
+
+void bitrv208(double *a)
+{
+    double x1r, x1i, x3r, x3i, x4r, x4i, x6r, x6i;
+    
+    x1r = a[2];
+    x1i = a[3];
+    x3r = a[6];
+    x3i = a[7];
+    x4r = a[8];
+    x4i = a[9];
+    x6r = a[12];
+    x6i = a[13];
+    a[2] = x4r;
+    a[3] = x4i;
+    a[6] = x6r;
+    a[7] = x6i;
+    a[8] = x1r;
+    a[9] = x1i;
+    a[12] = x3r;
+    a[13] = x3i;
+}
+
+
+void bitrv208neg(double *a)
+{
+    double x1r, x1i, x2r, x2i, x3r, x3i, x4r, x4i, 
+        x5r, x5i, x6r, x6i, x7r, x7i;
+    
+    x1r = a[2];
+    x1i = a[3];
+    x2r = a[4];
+    x2i = a[5];
+    x3r = a[6];
+    x3i = a[7];
+    x4r = a[8];
+    x4i = a[9];
+    x5r = a[10];
+    x5i = a[11];
+    x6r = a[12];
+    x6i = a[13];
+    x7r = a[14];
+    x7i = a[15];
+    a[2] = x7r;
+    a[3] = x7i;
+    a[4] = x3r;
+    a[5] = x3i;
+    a[6] = x5r;
+    a[7] = x5i;
+    a[8] = x1r;
+    a[9] = x1i;
+    a[10] = x6r;
+    a[11] = x6i;
+    a[12] = x2r;
+    a[13] = x2i;
+    a[14] = x4r;
+    a[15] = x4i;
+}
+
+
+void bitrv1(int n, double *a)
+{
+    int j0, k0, j1, k1, l, m, i, j, k, nh;
+    double x;
+    
+    nh = n >> 1;
+    x = a[1];
+    a[1] = a[nh];
+    a[nh] = x;
+    m = 2;
+    for (l = n >> 2; l > 2; l >>= 2) {
+        m <<= 1;
+    }
+    if (l == 2) {
+        j1 = m + 1;
+        k1 = m + nh;
+        x = a[j1];
+        a[j1] = a[k1];
+        a[k1] = x;
+        j0 = 0;
+        for (k0 = 2; k0 < m; k0 += 2) {
+            for (i = nh >> 1; i > (j0 ^= i); i >>= 1);
+            k = k0;
+            for (j = j0; j < j0 + k0; j += 2) {
+                x = a[j];
+                a[j] = a[k];
+                a[k] = x;
+                j1 = j + m;
+                k1 = k + m;
+                x = a[j1];
+                a[j1] = a[k1];
+                a[k1] = x;
+                j1 += nh;
+                k1++;
+                x = a[j1];
+                a[j1] = a[k1];
+                a[k1] = x;
+                j1 -= m;
+                k1 -= m;
+                x = a[j1];
+                a[j1] = a[k1];
+                a[k1] = x;
+                j1++;
+                k1 += nh;
+                x = a[j1];
+                a[j1] = a[k1];
+                a[k1] = x;
+                j1 += m;
+                k1 += m;
+                x = a[j1];
+                a[j1] = a[k1];
+                a[k1] = x;
+                j1 -= nh;
+                k1--;
+                x = a[j1];
+                a[j1] = a[k1];
+                a[k1] = x;
+                j1 -= m;
+                k1 -= m;
+                x = a[j1];
+                a[j1] = a[k1];
+                a[k1] = x;
+                for (i = nh >> 1; i > (k ^= i); i >>= 1);
+            }
+            k1 = j0 + k0;
+            j1 = k1 + 1;
+            k1 += nh;
+            x = a[j1];
+            a[j1] = a[k1];
+            a[k1] = x;
+            j1 += m;
+            k1 += m;
+            x = a[j1];
+            a[j1] = a[k1];
+            a[k1] = x;
+        }
+    } else {
+        j0 = 0;
+        for (k0 = 2; k0 < m; k0 += 2) {
+            for (i = nh >> 1; i > (j0 ^= i); i >>= 1);
+            k = k0;
+            for (j = j0; j < j0 + k0; j += 2) {
+                x = a[j];
+                a[j] = a[k];
+                a[k] = x;
+                j1 = j + nh;
+                k1 = k + 1;
+                x = a[j1];
+                a[j1] = a[k1];
+                a[k1] = x;
+                j1++;
+                k1 += nh;
+                x = a[j1];
+                a[j1] = a[k1];
+                a[k1] = x;
+                j1 -= nh;
+                k1--;
+                x = a[j1];
+                a[j1] = a[k1];
+                a[k1] = x;
+                for (i = nh >> 1; i > (k ^= i); i >>= 1);
+            }
+            k1 = j0 + k0;
+            j1 = k1 + 1;
+            k1 += nh;
+            x = a[j1];
+            a[j1] = a[k1];
+            a[k1] = x;
+        }
+    }
+}
+
+
+void cftb1st(int n, double *a)
+{
+    int i, i0, j, j0, j1, j2, j3, m, mh;
+    double ew, w1r, w1i, wk1r, wk1i, wk3r, wk3i, 
+        wd1r, wd1i, wd3r, wd3i, ss1, ss3;
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i;
+    
+    mh = n >> 3;
+    m = 2 * mh;
+    j1 = m;
+    j2 = j1 + m;
+    j3 = j2 + m;
+    x0r = a[0] + a[j2];
+    x0i = -a[1] - a[j2 + 1];
+    x1r = a[0] - a[j2];
+    x1i = -a[1] + a[j2 + 1];
+    x2r = a[j1] + a[j3];
+    x2i = a[j1 + 1] + a[j3 + 1];
+    x3r = a[j1] - a[j3];
+    x3i = a[j1 + 1] - a[j3 + 1];
+    a[0] = x0r + x2r;
+    a[1] = x0i - x2i;
+    a[j1] = x0r - x2r;
+    a[j1 + 1] = x0i + x2i;
+    a[j2] = x1r + x3i;
+    a[j2 + 1] = x1i + x3r;
+    a[j3] = x1r - x3i;
+    a[j3 + 1] = x1i - x3r;
+    wd1r = 1;
+    wd1i = 0;
+    wd3r = 1;
+    wd3i = 0;
+    ew = M_PI_2 / m;
+    w1r = cos(2 * ew);
+    w1i = sin(2 * ew);
+    wk1r = w1r;
+    wk1i = w1i;
+    ss1 = 2 * w1i;
+    wk3i = 2 * ss1 * wk1r;
+    wk3r = wk1r - wk3i * wk1i;
+    wk3i = wk1i - wk3i * wk1r;
+    ss3 = 2 * wk3i;
+    i = 0;
+    for (;;) {
+        i0 = i + 4 * CDFT_LOOP_DIV;
+        if (i0 > mh - 4) {
+            i0 = mh - 4;
+        }
+        for (j = i + 2; j < i0; j += 4) {
+            wd1r -= ss1 * wk1i;
+            wd1i += ss1 * wk1r;
+            wd3r -= ss3 * wk3i;
+            wd3i += ss3 * wk3r;
+            j1 = j + m;
+            j2 = j1 + m;
+            j3 = j2 + m;
+            x0r = a[j] + a[j2];
+            x0i = -a[j + 1] - a[j2 + 1];
+            x1r = a[j] - a[j2];
+            x1i = -a[j + 1] + a[j2 + 1];
+            x2r = a[j1] + a[j3];
+            x2i = a[j1 + 1] + a[j3 + 1];
+            x3r = a[j1] - a[j3];
+            x3i = a[j1 + 1] - a[j3 + 1];
+            a[j] = x0r + x2r;
+            a[j + 1] = x0i - x2i;
+            a[j1] = x0r - x2r;
+            a[j1 + 1] = x0i + x2i;
+            x0r = x1r + x3i;
+            x0i = x1i + x3r;
+            a[j2] = wk1r * x0r - wk1i * x0i;
+            a[j2 + 1] = wk1r * x0i + wk1i * x0r;
+            x0r = x1r - x3i;
+            x0i = x1i - x3r;
+            a[j3] = wk3r * x0r + wk3i * x0i;
+            a[j3 + 1] = wk3r * x0i - wk3i * x0r;
+            x0r = a[j + 2] + a[j2 + 2];
+            x0i = -a[j + 3] - a[j2 + 3];
+            x1r = a[j + 2] - a[j2 + 2];
+            x1i = -a[j + 3] + a[j2 + 3];
+            x2r = a[j1 + 2] + a[j3 + 2];
+            x2i = a[j1 + 3] + a[j3 + 3];
+            x3r = a[j1 + 2] - a[j3 + 2];
+            x3i = a[j1 + 3] - a[j3 + 3];
+            a[j + 2] = x0r + x2r;
+            a[j + 3] = x0i - x2i;
+            a[j1 + 2] = x0r - x2r;
+            a[j1 + 3] = x0i + x2i;
+            x0r = x1r + x3i;
+            x0i = x1i + x3r;
+            a[j2 + 2] = wd1r * x0r - wd1i * x0i;
+            a[j2 + 3] = wd1r * x0i + wd1i * x0r;
+            x0r = x1r - x3i;
+            x0i = x1i - x3r;
+            a[j3 + 2] = wd3r * x0r + wd3i * x0i;
+            a[j3 + 3] = wd3r * x0i - wd3i * x0r;
+            j0 = m - j;
+            j1 = j0 + m;
+            j2 = j1 + m;
+            j3 = j2 + m;
+            x0r = a[j0] + a[j2];
+            x0i = -a[j0 + 1] - a[j2 + 1];
+            x1r = a[j0] - a[j2];
+            x1i = -a[j0 + 1] + a[j2 + 1];
+            x2r = a[j1] + a[j3];
+            x2i = a[j1 + 1] + a[j3 + 1];
+            x3r = a[j1] - a[j3];
+            x3i = a[j1 + 1] - a[j3 + 1];
+            a[j0] = x0r + x2r;
+            a[j0 + 1] = x0i - x2i;
+            a[j1] = x0r - x2r;
+            a[j1 + 1] = x0i + x2i;
+            x0r = x1r + x3i;
+            x0i = x1i + x3r;
+            a[j2] = wk1i * x0r - wk1r * x0i;
+            a[j2 + 1] = wk1i * x0i + wk1r * x0r;
+            x0r = x1r - x3i;
+            x0i = x1i - x3r;
+            a[j3] = wk3i * x0r + wk3r * x0i;
+            a[j3 + 1] = wk3i * x0i - wk3r * x0r;
+            x0r = a[j0 - 2] + a[j2 - 2];
+            x0i = -a[j0 - 1] - a[j2 - 1];
+            x1r = a[j0 - 2] - a[j2 - 2];
+            x1i = -a[j0 - 1] + a[j2 - 1];
+            x2r = a[j1 - 2] + a[j3 - 2];
+            x2i = a[j1 - 1] + a[j3 - 1];
+            x3r = a[j1 - 2] - a[j3 - 2];
+            x3i = a[j1 - 1] - a[j3 - 1];
+            a[j0 - 2] = x0r + x2r;
+            a[j0 - 1] = x0i - x2i;
+            a[j1 - 2] = x0r - x2r;
+            a[j1 - 1] = x0i + x2i;
+            x0r = x1r + x3i;
+            x0i = x1i + x3r;
+            a[j2 - 2] = wd1i * x0r - wd1r * x0i;
+            a[j2 - 1] = wd1i * x0i + wd1r * x0r;
+            x0r = x1r - x3i;
+            x0i = x1i - x3r;
+            a[j3 - 2] = wd3i * x0r + wd3r * x0i;
+            a[j3 - 1] = wd3i * x0i - wd3r * x0r;
+            wk1r -= ss1 * wd1i;
+            wk1i += ss1 * wd1r;
+            wk3r -= ss3 * wd3i;
+            wk3i += ss3 * wd3r;
+        }
+        if (i0 == mh - 4) {
+            break;
+        }
+        wd1r = cos(ew * i0);
+        wd1i = sin(ew * i0);
+        wd3i = 4 * wd1i * wd1r;
+        wd3r = wd1r - wd3i * wd1i;
+        wd3i = wd1i - wd3i * wd1r;
+        wk1r = w1r * wd1r - w1i * wd1i;
+        wk1i = w1r * wd1i + w1i * wd1r;
+        wk3i = 4 * wk1i * wk1r;
+        wk3r = wk1r - wk3i * wk1i;
+        wk3i = wk1i - wk3i * wk1r;
+        i = i0;
+    }
+    wd1r = WR5000;
+    j0 = mh;
+    j1 = j0 + m;
+    j2 = j1 + m;
+    j3 = j2 + m;
+    x0r = a[j0 - 2] + a[j2 - 2];
+    x0i = -a[j0 - 1] - a[j2 - 1];
+    x1r = a[j0 - 2] - a[j2 - 2];
+    x1i = -a[j0 - 1] + a[j2 - 1];
+    x2r = a[j1 - 2] + a[j3 - 2];
+    x2i = a[j1 - 1] + a[j3 - 1];
+    x3r = a[j1 - 2] - a[j3 - 2];
+    x3i = a[j1 - 1] - a[j3 - 1];
+    a[j0 - 2] = x0r + x2r;
+    a[j0 - 1] = x0i - x2i;
+    a[j1 - 2] = x0r - x2r;
+    a[j1 - 1] = x0i + x2i;
+    x0r = x1r + x3i;
+    x0i = x1i + x3r;
+    a[j2 - 2] = wk1r * x0r - wk1i * x0i;
+    a[j2 - 1] = wk1r * x0i + wk1i * x0r;
+    x0r = x1r - x3i;
+    x0i = x1i - x3r;
+    a[j3 - 2] = wk3r * x0r + wk3i * x0i;
+    a[j3 - 1] = wk3r * x0i - wk3i * x0r;
+    x0r = a[j0] + a[j2];
+    x0i = -a[j0 + 1] - a[j2 + 1];
+    x1r = a[j0] - a[j2];
+    x1i = -a[j0 + 1] + a[j2 + 1];
+    x2r = a[j1] + a[j3];
+    x2i = a[j1 + 1] + a[j3 + 1];
+    x3r = a[j1] - a[j3];
+    x3i = a[j1 + 1] - a[j3 + 1];
+    a[j0] = x0r + x2r;
+    a[j0 + 1] = x0i - x2i;
+    a[j1] = x0r - x2r;
+    a[j1 + 1] = x0i + x2i;
+    x0r = x1r + x3i;
+    x0i = x1i + x3r;
+    a[j2] = wd1r * (x0r - x0i);
+    a[j2 + 1] = wd1r * (x0i + x0r);
+    x0r = x1r - x3i;
+    x0i = x1i - x3r;
+    a[j3] = -wd1r * (x0r + x0i);
+    a[j3 + 1] = -wd1r * (x0i - x0r);
+    x0r = a[j0 + 2] + a[j2 + 2];
+    x0i = -a[j0 + 3] - a[j2 + 3];
+    x1r = a[j0 + 2] - a[j2 + 2];
+    x1i = -a[j0 + 3] + a[j2 + 3];
+    x2r = a[j1 + 2] + a[j3 + 2];
+    x2i = a[j1 + 3] + a[j3 + 3];
+    x3r = a[j1 + 2] - a[j3 + 2];
+    x3i = a[j1 + 3] - a[j3 + 3];
+    a[j0 + 2] = x0r + x2r;
+    a[j0 + 3] = x0i - x2i;
+    a[j1 + 2] = x0r - x2r;
+    a[j1 + 3] = x0i + x2i;
+    x0r = x1r + x3i;
+    x0i = x1i + x3r;
+    a[j2 + 2] = wk1i * x0r - wk1r * x0i;
+    a[j2 + 3] = wk1i * x0i + wk1r * x0r;
+    x0r = x1r - x3i;
+    x0i = x1i - x3r;
+    a[j3 + 2] = wk3i * x0r + wk3r * x0i;
+    a[j3 + 3] = wk3i * x0i - wk3r * x0r;
+}
+
+
+#ifdef USE_CDFT_THREADS
+struct cdft_arg_st {
+    int n0;
+    int n;
+    double *a;
+};
+typedef struct cdft_arg_st cdft_arg_t;
+
+
+void cftrec4_th(int n, double *a)
+{
+    void *cftrec1_th(void *p);
+    void *cftrec2_th(void *p);
+    int i, idiv4, m, nthread;
+    cdft_thread_t th[4];
+    cdft_arg_t ag[4];
+    
+    nthread = 2;
+    idiv4 = 0;
+    m = n >> 1;
+    if (n > CDFT_4THREADS_BEGIN_N) {
+        nthread = 4;
+        idiv4 = 1;
+        m >>= 1;
+    }
+    for (i = 0; i < nthread; i++) {
+        ag[i].n0 = n;
+        ag[i].n = m;
+        ag[i].a = &a[i * m];
+        if (i != idiv4) {
+            cdft_thread_create(&th[i], cftrec1_th, &ag[i]);
+        } else {
+            cdft_thread_create(&th[i], cftrec2_th, &ag[i]);
+        }
+    }
+    for (i = 0; i < nthread; i++) {
+        cdft_thread_wait(th[i]);
+    }
+}
+
+
+void *cftrec1_th(void *p)
+{
+    int cfttree(int n, int j, int k, double *a);
+    void cftleaf(int n, int isplt, double *a);
+    void cftmdl1(int n, double *a);
+    int isplt, j, k, m, n, n0;
+    double *a;
+    
+    n0 = ((cdft_arg_t *) p)->n0;
+    n = ((cdft_arg_t *) p)->n;
+    a = ((cdft_arg_t *) p)->a;
+    m = n0;
+    while (m > 512) {
+        m >>= 2;
+        cftmdl1(m, &a[n - m]);
+    }
+    cftleaf(m, 1, &a[n - m]);
+    k = 0;
+    for (j = n - m; j > 0; j -= m) {
+        k++;
+        isplt = cfttree(m, j, k, a);
+        cftleaf(m, isplt, &a[j - m]);
+    }
+    return (void *) 0;
+}
+
+
+void *cftrec2_th(void *p)
+{
+    int cfttree(int n, int j, int k, double *a);
+    void cftleaf(int n, int isplt, double *a);
+    void cftmdl2(int n, double *a);
+    int isplt, j, k, m, n, n0;
+    double *a;
+    
+    n0 = ((cdft_arg_t *) p)->n0;
+    n = ((cdft_arg_t *) p)->n;
+    a = ((cdft_arg_t *) p)->a;
+    k = 1;
+    m = n0;
+    while (m > 512) {
+        m >>= 2;
+        k <<= 2;
+        cftmdl2(m, &a[n - m]);
+    }
+    cftleaf(m, 0, &a[n - m]);
+    k >>= 1;
+    for (j = n - m; j > 0; j -= m) {
+        k++;
+        isplt = cfttree(m, j, k, a);
+        cftleaf(m, isplt, &a[j - m]);
+    }
+    return (void *) 0;
+}
+#endif /* USE_CDFT_THREADS */
+
+
+void cftrec4(int n, double *a)
+{
+    int cfttree(int n, int j, int k, double *a);
+    void cftleaf(int n, int isplt, double *a);
+    void cftmdl1(int n, double *a);
+    int isplt, j, k, m;
+    
+    m = n;
+    while (m > 512) {
+        m >>= 2;
+        cftmdl1(m, &a[n - m]);
+    }
+    cftleaf(m, 1, &a[n - m]);
+    k = 0;
+    for (j = n - m; j > 0; j -= m) {
+        k++;
+        isplt = cfttree(m, j, k, a);
+        cftleaf(m, isplt, &a[j - m]);
+    }
+}
+
+
+int cfttree(int n, int j, int k, double *a)
+{
+    void cftmdl1(int n, double *a);
+    void cftmdl2(int n, double *a);
+    int i, isplt, m;
+    
+    if ((k & 3) != 0) {
+        isplt = k & 1;
+        if (isplt != 0) {
+            cftmdl1(n, &a[j - n]);
+        } else {
+            cftmdl2(n, &a[j - n]);
+        }
+    } else {
+        m = n;
+        for (i = k; (i & 3) == 0; i >>= 2) {
+            m <<= 2;
+        }
+        isplt = i & 1;
+        if (isplt != 0) {
+            while (m > 128) {
+                cftmdl1(m, &a[j - m]);
+                m >>= 2;
+            }
+        } else {
+            while (m > 128) {
+                cftmdl2(m, &a[j - m]);
+                m >>= 2;
+            }
+        }
+    }
+    return isplt;
+}
+
+
+void cftleaf(int n, int isplt, double *a)
+{
+    void cftmdl1(int n, double *a);
+    void cftmdl2(int n, double *a);
+    void cftf161(double *a);
+    void cftf162(double *a);
+    void cftf081(double *a);
+    void cftf082(double *a);
+    
+    if (n == 512) {
+        cftmdl1(128, a);
+        cftf161(a);
+        cftf162(&a[32]);
+        cftf161(&a[64]);
+        cftf161(&a[96]);
+        cftmdl2(128, &a[128]);
+        cftf161(&a[128]);
+        cftf162(&a[160]);
+        cftf161(&a[192]);
+        cftf162(&a[224]);
+        cftmdl1(128, &a[256]);
+        cftf161(&a[256]);
+        cftf162(&a[288]);
+        cftf161(&a[320]);
+        cftf161(&a[352]);
+        if (isplt != 0) {
+            cftmdl1(128, &a[384]);
+            cftf161(&a[480]);
+        } else {
+            cftmdl2(128, &a[384]);
+            cftf162(&a[480]);
+        }
+        cftf161(&a[384]);
+        cftf162(&a[416]);
+        cftf161(&a[448]);
+    } else {
+        cftmdl1(64, a);
+        cftf081(a);
+        cftf082(&a[16]);
+        cftf081(&a[32]);
+        cftf081(&a[48]);
+        cftmdl2(64, &a[64]);
+        cftf081(&a[64]);
+        cftf082(&a[80]);
+        cftf081(&a[96]);
+        cftf082(&a[112]);
+        cftmdl1(64, &a[128]);
+        cftf081(&a[128]);
+        cftf082(&a[144]);
+        cftf081(&a[160]);
+        cftf081(&a[176]);
+        if (isplt != 0) {
+            cftmdl1(64, &a[192]);
+            cftf081(&a[240]);
+        } else {
+            cftmdl2(64, &a[192]);
+            cftf082(&a[240]);
+        }
+        cftf081(&a[192]);
+        cftf082(&a[208]);
+        cftf081(&a[224]);
+    }
+}
+
+
+void cftmdl1(int n, double *a)
+{
+    int i, i0, j, j0, j1, j2, j3, m, mh;
+    double ew, w1r, w1i, wk1r, wk1i, wk3r, wk3i, 
+        wd1r, wd1i, wd3r, wd3i, ss1, ss3;
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i;
+    
+    mh = n >> 3;
+    m = 2 * mh;
+    j1 = m;
+    j2 = j1 + m;
+    j3 = j2 + m;
+    x0r = a[0] + a[j2];
+    x0i = a[1] + a[j2 + 1];
+    x1r = a[0] - a[j2];
+    x1i = a[1] - a[j2 + 1];
+    x2r = a[j1] + a[j3];
+    x2i = a[j1 + 1] + a[j3 + 1];
+    x3r = a[j1] - a[j3];
+    x3i = a[j1 + 1] - a[j3 + 1];
+    a[0] = x0r + x2r;
+    a[1] = x0i + x2i;
+    a[j1] = x0r - x2r;
+    a[j1 + 1] = x0i - x2i;
+    a[j2] = x1r - x3i;
+    a[j2 + 1] = x1i + x3r;
+    a[j3] = x1r + x3i;
+    a[j3 + 1] = x1i - x3r;
+    wd1r = 1;
+    wd1i = 0;
+    wd3r = 1;
+    wd3i = 0;
+    ew = M_PI_2 / m;
+    w1r = cos(2 * ew);
+    w1i = sin(2 * ew);
+    wk1r = w1r;
+    wk1i = w1i;
+    ss1 = 2 * w1i;
+    wk3i = 2 * ss1 * wk1r;
+    wk3r = wk1r - wk3i * wk1i;
+    wk3i = wk1i - wk3i * wk1r;
+    ss3 = 2 * wk3i;
+    i = 0;
+    for (;;) {
+        i0 = i + 4 * CDFT_LOOP_DIV;
+        if (i0 > mh - 4) {
+            i0 = mh - 4;
+        }
+        for (j = i + 2; j < i0; j += 4) {
+            wd1r -= ss1 * wk1i;
+            wd1i += ss1 * wk1r;
+            wd3r -= ss3 * wk3i;
+            wd3i += ss3 * wk3r;
+            j1 = j + m;
+            j2 = j1 + m;
+            j3 = j2 + m;
+            x0r = a[j] + a[j2];
+            x0i = a[j + 1] + a[j2 + 1];
+            x1r = a[j] - a[j2];
+            x1i = a[j + 1] - a[j2 + 1];
+            x2r = a[j1] + a[j3];
+            x2i = a[j1 + 1] + a[j3 + 1];
+            x3r = a[j1] - a[j3];
+            x3i = a[j1 + 1] - a[j3 + 1];
+            a[j] = x0r + x2r;
+            a[j + 1] = x0i + x2i;
+            a[j1] = x0r - x2r;
+            a[j1 + 1] = x0i - x2i;
+            x0r = x1r - x3i;
+            x0i = x1i + x3r;
+            a[j2] = wk1r * x0r - wk1i * x0i;
+            a[j2 + 1] = wk1r * x0i + wk1i * x0r;
+            x0r = x1r + x3i;
+            x0i = x1i - x3r;
+            a[j3] = wk3r * x0r + wk3i * x0i;
+            a[j3 + 1] = wk3r * x0i - wk3i * x0r;
+            x0r = a[j + 2] + a[j2 + 2];
+            x0i = a[j + 3] + a[j2 + 3];
+            x1r = a[j + 2] - a[j2 + 2];
+            x1i = a[j + 3] - a[j2 + 3];
+            x2r = a[j1 + 2] + a[j3 + 2];
+            x2i = a[j1 + 3] + a[j3 + 3];
+            x3r = a[j1 + 2] - a[j3 + 2];
+            x3i = a[j1 + 3] - a[j3 + 3];
+            a[j + 2] = x0r + x2r;
+            a[j + 3] = x0i + x2i;
+            a[j1 + 2] = x0r - x2r;
+            a[j1 + 3] = x0i - x2i;
+            x0r = x1r - x3i;
+            x0i = x1i + x3r;
+            a[j2 + 2] = wd1r * x0r - wd1i * x0i;
+            a[j2 + 3] = wd1r * x0i + wd1i * x0r;
+            x0r = x1r + x3i;
+            x0i = x1i - x3r;
+            a[j3 + 2] = wd3r * x0r + wd3i * x0i;
+            a[j3 + 3] = wd3r * x0i - wd3i * x0r;
+            j0 = m - j;
+            j1 = j0 + m;
+            j2 = j1 + m;
+            j3 = j2 + m;
+            x0r = a[j0] + a[j2];
+            x0i = a[j0 + 1] + a[j2 + 1];
+            x1r = a[j0] - a[j2];
+            x1i = a[j0 + 1] - a[j2 + 1];
+            x2r = a[j1] + a[j3];
+            x2i = a[j1 + 1] + a[j3 + 1];
+            x3r = a[j1] - a[j3];
+            x3i = a[j1 + 1] - a[j3 + 1];
+            a[j0] = x0r + x2r;
+            a[j0 + 1] = x0i + x2i;
+            a[j1] = x0r - x2r;
+            a[j1 + 1] = x0i - x2i;
+            x0r = x1r - x3i;
+            x0i = x1i + x3r;
+            a[j2] = wk1i * x0r - wk1r * x0i;
+            a[j2 + 1] = wk1i * x0i + wk1r * x0r;
+            x0r = x1r + x3i;
+            x0i = x1i - x3r;
+            a[j3] = wk3i * x0r + wk3r * x0i;
+            a[j3 + 1] = wk3i * x0i - wk3r * x0r;
+            x0r = a[j0 - 2] + a[j2 - 2];
+            x0i = a[j0 - 1] + a[j2 - 1];
+            x1r = a[j0 - 2] - a[j2 - 2];
+            x1i = a[j0 - 1] - a[j2 - 1];
+            x2r = a[j1 - 2] + a[j3 - 2];
+            x2i = a[j1 - 1] + a[j3 - 1];
+            x3r = a[j1 - 2] - a[j3 - 2];
+            x3i = a[j1 - 1] - a[j3 - 1];
+            a[j0 - 2] = x0r + x2r;
+            a[j0 - 1] = x0i + x2i;
+            a[j1 - 2] = x0r - x2r;
+            a[j1 - 1] = x0i - x2i;
+            x0r = x1r - x3i;
+            x0i = x1i + x3r;
+            a[j2 - 2] = wd1i * x0r - wd1r * x0i;
+            a[j2 - 1] = wd1i * x0i + wd1r * x0r;
+            x0r = x1r + x3i;
+            x0i = x1i - x3r;
+            a[j3 - 2] = wd3i * x0r + wd3r * x0i;
+            a[j3 - 1] = wd3i * x0i - wd3r * x0r;
+            wk1r -= ss1 * wd1i;
+            wk1i += ss1 * wd1r;
+            wk3r -= ss3 * wd3i;
+            wk3i += ss3 * wd3r;
+        }
+        if (i0 == mh - 4) {
+            break;
+        }
+        wd1r = cos(ew * i0);
+        wd1i = sin(ew * i0);
+        wd3i = 4 * wd1i * wd1r;
+        wd3r = wd1r - wd3i * wd1i;
+        wd3i = wd1i - wd3i * wd1r;
+        wk1r = w1r * wd1r - w1i * wd1i;
+        wk1i = w1r * wd1i + w1i * wd1r;
+        wk3i = 4 * wk1i * wk1r;
+        wk3r = wk1r - wk3i * wk1i;
+        wk3i = wk1i - wk3i * wk1r;
+        i = i0;
+    }
+    wd1r = WR5000;
+    j0 = mh;
+    j1 = j0 + m;
+    j2 = j1 + m;
+    j3 = j2 + m;
+    x0r = a[j0 - 2] + a[j2 - 2];
+    x0i = a[j0 - 1] + a[j2 - 1];
+    x1r = a[j0 - 2] - a[j2 - 2];
+    x1i = a[j0 - 1] - a[j2 - 1];
+    x2r = a[j1 - 2] + a[j3 - 2];
+    x2i = a[j1 - 1] + a[j3 - 1];
+    x3r = a[j1 - 2] - a[j3 - 2];
+    x3i = a[j1 - 1] - a[j3 - 1];
+    a[j0 - 2] = x0r + x2r;
+    a[j0 - 1] = x0i + x2i;
+    a[j1 - 2] = x0r - x2r;
+    a[j1 - 1] = x0i - x2i;
+    x0r = x1r - x3i;
+    x0i = x1i + x3r;
+    a[j2 - 2] = wk1r * x0r - wk1i * x0i;
+    a[j2 - 1] = wk1r * x0i + wk1i * x0r;
+    x0r = x1r + x3i;
+    x0i = x1i - x3r;
+    a[j3 - 2] = wk3r * x0r + wk3i * x0i;
+    a[j3 - 1] = wk3r * x0i - wk3i * x0r;
+    x0r = a[j0] + a[j2];
+    x0i = a[j0 + 1] + a[j2 + 1];
+    x1r = a[j0] - a[j2];
+    x1i = a[j0 + 1] - a[j2 + 1];
+    x2r = a[j1] + a[j3];
+    x2i = a[j1 + 1] + a[j3 + 1];
+    x3r = a[j1] - a[j3];
+    x3i = a[j1 + 1] - a[j3 + 1];
+    a[j0] = x0r + x2r;
+    a[j0 + 1] = x0i + x2i;
+    a[j1] = x0r - x2r;
+    a[j1 + 1] = x0i - x2i;
+    x0r = x1r - x3i;
+    x0i = x1i + x3r;
+    a[j2] = wd1r * (x0r - x0i);
+    a[j2 + 1] = wd1r * (x0i + x0r);
+    x0r = x1r + x3i;
+    x0i = x1i - x3r;
+    a[j3] = -wd1r * (x0r + x0i);
+    a[j3 + 1] = -wd1r * (x0i - x0r);
+    x0r = a[j0 + 2] + a[j2 + 2];
+    x0i = a[j0 + 3] + a[j2 + 3];
+    x1r = a[j0 + 2] - a[j2 + 2];
+    x1i = a[j0 + 3] - a[j2 + 3];
+    x2r = a[j1 + 2] + a[j3 + 2];
+    x2i = a[j1 + 3] + a[j3 + 3];
+    x3r = a[j1 + 2] - a[j3 + 2];
+    x3i = a[j1 + 3] - a[j3 + 3];
+    a[j0 + 2] = x0r + x2r;
+    a[j0 + 3] = x0i + x2i;
+    a[j1 + 2] = x0r - x2r;
+    a[j1 + 3] = x0i - x2i;
+    x0r = x1r - x3i;
+    x0i = x1i + x3r;
+    a[j2 + 2] = wk1i * x0r - wk1r * x0i;
+    a[j2 + 3] = wk1i * x0i + wk1r * x0r;
+    x0r = x1r + x3i;
+    x0i = x1i - x3r;
+    a[j3 + 2] = wk3i * x0r + wk3r * x0i;
+    a[j3 + 3] = wk3i * x0i - wk3r * x0r;
+}
+
+
+void cftmdl2(int n, double *a)
+{
+    int i, i0, j, j0, j1, j2, j3, m, mh;
+    double ew, w1r, w1i, wn4r, wk1r, wk1i, wk3r, wk3i, 
+        wl1r, wl1i, wl3r, wl3i, wd1r, wd1i, wd3r, wd3i, 
+        we1r, we1i, we3r, we3i, ss1, ss3;
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i, y0r, y0i, y2r, y2i;
+    
+    mh = n >> 3;
+    m = 2 * mh;
+    wn4r = WR5000;
+    j1 = m;
+    j2 = j1 + m;
+    j3 = j2 + m;
+    x0r = a[0] - a[j2 + 1];
+    x0i = a[1] + a[j2];
+    x1r = a[0] + a[j2 + 1];
+    x1i = a[1] - a[j2];
+    x2r = a[j1] - a[j3 + 1];
+    x2i = a[j1 + 1] + a[j3];
+    x3r = a[j1] + a[j3 + 1];
+    x3i = a[j1 + 1] - a[j3];
+    y0r = wn4r * (x2r - x2i);
+    y0i = wn4r * (x2i + x2r);
+    a[0] = x0r + y0r;
+    a[1] = x0i + y0i;
+    a[j1] = x0r - y0r;
+    a[j1 + 1] = x0i - y0i;
+    y0r = wn4r * (x3r - x3i);
+    y0i = wn4r * (x3i + x3r);
+    a[j2] = x1r - y0i;
+    a[j2 + 1] = x1i + y0r;
+    a[j3] = x1r + y0i;
+    a[j3 + 1] = x1i - y0r;
+    wl1r = 1;
+    wl1i = 0;
+    wl3r = 1;
+    wl3i = 0;
+    we1r = wn4r;
+    we1i = wn4r;
+    we3r = -wn4r;
+    we3i = -wn4r;
+    ew = M_PI_2 / (2 * m);
+    w1r = cos(2 * ew);
+    w1i = sin(2 * ew);
+    wk1r = w1r;
+    wk1i = w1i;
+    wd1r = wn4r * (w1r - w1i);
+    wd1i = wn4r * (w1i + w1r);
+    ss1 = 2 * w1i;
+    wk3i = 2 * ss1 * wk1r;
+    wk3r = wk1r - wk3i * wk1i;
+    wk3i = wk1i - wk3i * wk1r;
+    ss3 = 2 * wk3i;
+    wd3r = -wn4r * (wk3r - wk3i);
+    wd3i = -wn4r * (wk3i + wk3r);
+    i = 0;
+    for (;;) {
+        i0 = i + 4 * CDFT_LOOP_DIV;
+        if (i0 > mh - 4) {
+            i0 = mh - 4;
+        }
+        for (j = i + 2; j < i0; j += 4) {
+            wl1r -= ss1 * wk1i;
+            wl1i += ss1 * wk1r;
+            wl3r -= ss3 * wk3i;
+            wl3i += ss3 * wk3r;
+            we1r -= ss1 * wd1i;
+            we1i += ss1 * wd1r;
+            we3r -= ss3 * wd3i;
+            we3i += ss3 * wd3r;
+            j1 = j + m;
+            j2 = j1 + m;
+            j3 = j2 + m;
+            x0r = a[j] - a[j2 + 1];
+            x0i = a[j + 1] + a[j2];
+            x1r = a[j] + a[j2 + 1];
+            x1i = a[j + 1] - a[j2];
+            x2r = a[j1] - a[j3 + 1];
+            x2i = a[j1 + 1] + a[j3];
+            x3r = a[j1] + a[j3 + 1];
+            x3i = a[j1 + 1] - a[j3];
+            y0r = wk1r * x0r - wk1i * x0i;
+            y0i = wk1r * x0i + wk1i * x0r;
+            y2r = wd1r * x2r - wd1i * x2i;
+            y2i = wd1r * x2i + wd1i * x2r;
+            a[j] = y0r + y2r;
+            a[j + 1] = y0i + y2i;
+            a[j1] = y0r - y2r;
+            a[j1 + 1] = y0i - y2i;
+            y0r = wk3r * x1r + wk3i * x1i;
+            y0i = wk3r * x1i - wk3i * x1r;
+            y2r = wd3r * x3r + wd3i * x3i;
+            y2i = wd3r * x3i - wd3i * x3r;
+            a[j2] = y0r + y2r;
+            a[j2 + 1] = y0i + y2i;
+            a[j3] = y0r - y2r;
+            a[j3 + 1] = y0i - y2i;
+            x0r = a[j + 2] - a[j2 + 3];
+            x0i = a[j + 3] + a[j2 + 2];
+            x1r = a[j + 2] + a[j2 + 3];
+            x1i = a[j + 3] - a[j2 + 2];
+            x2r = a[j1 + 2] - a[j3 + 3];
+            x2i = a[j1 + 3] + a[j3 + 2];
+            x3r = a[j1 + 2] + a[j3 + 3];
+            x3i = a[j1 + 3] - a[j3 + 2];
+            y0r = wl1r * x0r - wl1i * x0i;
+            y0i = wl1r * x0i + wl1i * x0r;
+            y2r = we1r * x2r - we1i * x2i;
+            y2i = we1r * x2i + we1i * x2r;
+            a[j + 2] = y0r + y2r;
+            a[j + 3] = y0i + y2i;
+            a[j1 + 2] = y0r - y2r;
+            a[j1 + 3] = y0i - y2i;
+            y0r = wl3r * x1r + wl3i * x1i;
+            y0i = wl3r * x1i - wl3i * x1r;
+            y2r = we3r * x3r + we3i * x3i;
+            y2i = we3r * x3i - we3i * x3r;
+            a[j2 + 2] = y0r + y2r;
+            a[j2 + 3] = y0i + y2i;
+            a[j3 + 2] = y0r - y2r;
+            a[j3 + 3] = y0i - y2i;
+            j0 = m - j;
+            j1 = j0 + m;
+            j2 = j1 + m;
+            j3 = j2 + m;
+            x0r = a[j0] - a[j2 + 1];
+            x0i = a[j0 + 1] + a[j2];
+            x1r = a[j0] + a[j2 + 1];
+            x1i = a[j0 + 1] - a[j2];
+            x2r = a[j1] - a[j3 + 1];
+            x2i = a[j1 + 1] + a[j3];
+            x3r = a[j1] + a[j3 + 1];
+            x3i = a[j1 + 1] - a[j3];
+            y0r = wd1i * x0r - wd1r * x0i;
+            y0i = wd1i * x0i + wd1r * x0r;
+            y2r = wk1i * x2r - wk1r * x2i;
+            y2i = wk1i * x2i + wk1r * x2r;
+            a[j0] = y0r + y2r;
+            a[j0 + 1] = y0i + y2i;
+            a[j1] = y0r - y2r;
+            a[j1 + 1] = y0i - y2i;
+            y0r = wd3i * x1r + wd3r * x1i;
+            y0i = wd3i * x1i - wd3r * x1r;
+            y2r = wk3i * x3r + wk3r * x3i;
+            y2i = wk3i * x3i - wk3r * x3r;
+            a[j2] = y0r + y2r;
+            a[j2 + 1] = y0i + y2i;
+            a[j3] = y0r - y2r;
+            a[j3 + 1] = y0i - y2i;
+            x0r = a[j0 - 2] - a[j2 - 1];
+            x0i = a[j0 - 1] + a[j2 - 2];
+            x1r = a[j0 - 2] + a[j2 - 1];
+            x1i = a[j0 - 1] - a[j2 - 2];
+            x2r = a[j1 - 2] - a[j3 - 1];
+            x2i = a[j1 - 1] + a[j3 - 2];
+            x3r = a[j1 - 2] + a[j3 - 1];
+            x3i = a[j1 - 1] - a[j3 - 2];
+            y0r = we1i * x0r - we1r * x0i;
+            y0i = we1i * x0i + we1r * x0r;
+            y2r = wl1i * x2r - wl1r * x2i;
+            y2i = wl1i * x2i + wl1r * x2r;
+            a[j0 - 2] = y0r + y2r;
+            a[j0 - 1] = y0i + y2i;
+            a[j1 - 2] = y0r - y2r;
+            a[j1 - 1] = y0i - y2i;
+            y0r = we3i * x1r + we3r * x1i;
+            y0i = we3i * x1i - we3r * x1r;
+            y2r = wl3i * x3r + wl3r * x3i;
+            y2i = wl3i * x3i - wl3r * x3r;
+            a[j2 - 2] = y0r + y2r;
+            a[j2 - 1] = y0i + y2i;
+            a[j3 - 2] = y0r - y2r;
+            a[j3 - 1] = y0i - y2i;
+            wk1r -= ss1 * wl1i;
+            wk1i += ss1 * wl1r;
+            wk3r -= ss3 * wl3i;
+            wk3i += ss3 * wl3r;
+            wd1r -= ss1 * we1i;
+            wd1i += ss1 * we1r;
+            wd3r -= ss3 * we3i;
+            wd3i += ss3 * we3r;
+        }
+        if (i0 == mh - 4) {
+            break;
+        }
+        wl1r = cos(ew * i0);
+        wl1i = sin(ew * i0);
+        wl3i = 4 * wl1i * wl1r;
+        wl3r = wl1r - wl3i * wl1i;
+        wl3i = wl1i - wl3i * wl1r;
+        we1r = wn4r * (wl1r - wl1i);
+        we1i = wn4r * (wl1i + wl1r);
+        we3r = -wn4r * (wl3r - wl3i);
+        we3i = -wn4r * (wl3i + wl3r);
+        wk1r = w1r * wl1r - w1i * wl1i;
+        wk1i = w1r * wl1i + w1i * wl1r;
+        wk3i = 4 * wk1i * wk1r;
+        wk3r = wk1r - wk3i * wk1i;
+        wk3i = wk1i - wk3i * wk1r;
+        wd1r = wn4r * (wk1r - wk1i);
+        wd1i = wn4r * (wk1i + wk1r);
+        wd3r = -wn4r * (wk3r - wk3i);
+        wd3i = -wn4r * (wk3i + wk3r);
+        i = i0;
+    }
+    wl1r = WR2500;
+    wl1i = WI2500;
+    j0 = mh;
+    j1 = j0 + m;
+    j2 = j1 + m;
+    j3 = j2 + m;
+    x0r = a[j0 - 2] - a[j2 - 1];
+    x0i = a[j0 - 1] + a[j2 - 2];
+    x1r = a[j0 - 2] + a[j2 - 1];
+    x1i = a[j0 - 1] - a[j2 - 2];
+    x2r = a[j1 - 2] - a[j3 - 1];
+    x2i = a[j1 - 1] + a[j3 - 2];
+    x3r = a[j1 - 2] + a[j3 - 1];
+    x3i = a[j1 - 1] - a[j3 - 2];
+    y0r = wk1r * x0r - wk1i * x0i;
+    y0i = wk1r * x0i + wk1i * x0r;
+    y2r = wd1r * x2r - wd1i * x2i;
+    y2i = wd1r * x2i + wd1i * x2r;
+    a[j0 - 2] = y0r + y2r;
+    a[j0 - 1] = y0i + y2i;
+    a[j1 - 2] = y0r - y2r;
+    a[j1 - 1] = y0i - y2i;
+    y0r = wk3r * x1r + wk3i * x1i;
+    y0i = wk3r * x1i - wk3i * x1r;
+    y2r = wd3r * x3r + wd3i * x3i;
+    y2i = wd3r * x3i - wd3i * x3r;
+    a[j2 - 2] = y0r + y2r;
+    a[j2 - 1] = y0i + y2i;
+    a[j3 - 2] = y0r - y2r;
+    a[j3 - 1] = y0i - y2i;
+    x0r = a[j0] - a[j2 + 1];
+    x0i = a[j0 + 1] + a[j2];
+    x1r = a[j0] + a[j2 + 1];
+    x1i = a[j0 + 1] - a[j2];
+    x2r = a[j1] - a[j3 + 1];
+    x2i = a[j1 + 1] + a[j3];
+    x3r = a[j1] + a[j3 + 1];
+    x3i = a[j1 + 1] - a[j3];
+    y0r = wl1r * x0r - wl1i * x0i;
+    y0i = wl1r * x0i + wl1i * x0r;
+    y2r = wl1i * x2r - wl1r * x2i;
+    y2i = wl1i * x2i + wl1r * x2r;
+    a[j0] = y0r + y2r;
+    a[j0 + 1] = y0i + y2i;
+    a[j1] = y0r - y2r;
+    a[j1 + 1] = y0i - y2i;
+    y0r = wl1i * x1r - wl1r * x1i;
+    y0i = wl1i * x1i + wl1r * x1r;
+    y2r = wl1r * x3r - wl1i * x3i;
+    y2i = wl1r * x3i + wl1i * x3r;
+    a[j2] = y0r - y2r;
+    a[j2 + 1] = y0i - y2i;
+    a[j3] = y0r + y2r;
+    a[j3 + 1] = y0i + y2i;
+    x0r = a[j0 + 2] - a[j2 + 3];
+    x0i = a[j0 + 3] + a[j2 + 2];
+    x1r = a[j0 + 2] + a[j2 + 3];
+    x1i = a[j0 + 3] - a[j2 + 2];
+    x2r = a[j1 + 2] - a[j3 + 3];
+    x2i = a[j1 + 3] + a[j3 + 2];
+    x3r = a[j1 + 2] + a[j3 + 3];
+    x3i = a[j1 + 3] - a[j3 + 2];
+    y0r = wd1i * x0r - wd1r * x0i;
+    y0i = wd1i * x0i + wd1r * x0r;
+    y2r = wk1i * x2r - wk1r * x2i;
+    y2i = wk1i * x2i + wk1r * x2r;
+    a[j0 + 2] = y0r + y2r;
+    a[j0 + 3] = y0i + y2i;
+    a[j1 + 2] = y0r - y2r;
+    a[j1 + 3] = y0i - y2i;
+    y0r = wd3i * x1r + wd3r * x1i;
+    y0i = wd3i * x1i - wd3r * x1r;
+    y2r = wk3i * x3r + wk3r * x3i;
+    y2i = wk3i * x3i - wk3r * x3r;
+    a[j2 + 2] = y0r + y2r;
+    a[j2 + 3] = y0i + y2i;
+    a[j3 + 2] = y0r - y2r;
+    a[j3 + 3] = y0i - y2i;
+}
+
+
+void cftfx41(int n, double *a)
+{
+    void cftf161(double *a);
+    void cftf162(double *a);
+    void cftf081(double *a);
+    void cftf082(double *a);
+    
+    if (n == 128) {
+        cftf161(a);
+        cftf162(&a[32]);
+        cftf161(&a[64]);
+        cftf161(&a[96]);
+    } else {
+        cftf081(a);
+        cftf082(&a[16]);
+        cftf081(&a[32]);
+        cftf081(&a[48]);
+    }
+}
+
+
+void cftf161(double *a)
+{
+    double wn4r, wk1r, wk1i, 
+        x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i, 
+        y0r, y0i, y1r, y1i, y2r, y2i, y3r, y3i, 
+        y4r, y4i, y5r, y5i, y6r, y6i, y7r, y7i, 
+        y8r, y8i, y9r, y9i, y10r, y10i, y11r, y11i, 
+        y12r, y12i, y13r, y13i, y14r, y14i, y15r, y15i;
+    
+    wn4r = WR5000;
+    wk1r = WR2500;
+    wk1i = WI2500;
+    x0r = a[0] + a[16];
+    x0i = a[1] + a[17];
+    x1r = a[0] - a[16];
+    x1i = a[1] - a[17];
+    x2r = a[8] + a[24];
+    x2i = a[9] + a[25];
+    x3r = a[8] - a[24];
+    x3i = a[9] - a[25];
+    y0r = x0r + x2r;
+    y0i = x0i + x2i;
+    y4r = x0r - x2r;
+    y4i = x0i - x2i;
+    y8r = x1r - x3i;
+    y8i = x1i + x3r;
+    y12r = x1r + x3i;
+    y12i = x1i - x3r;
+    x0r = a[2] + a[18];
+    x0i = a[3] + a[19];
+    x1r = a[2] - a[18];
+    x1i = a[3] - a[19];
+    x2r = a[10] + a[26];
+    x2i = a[11] + a[27];
+    x3r = a[10] - a[26];
+    x3i = a[11] - a[27];
+    y1r = x0r + x2r;
+    y1i = x0i + x2i;
+    y5r = x0r - x2r;
+    y5i = x0i - x2i;
+    x0r = x1r - x3i;
+    x0i = x1i + x3r;
+    y9r = wk1r * x0r - wk1i * x0i;
+    y9i = wk1r * x0i + wk1i * x0r;
+    x0r = x1r + x3i;
+    x0i = x1i - x3r;
+    y13r = wk1i * x0r - wk1r * x0i;
+    y13i = wk1i * x0i + wk1r * x0r;
+    x0r = a[4] + a[20];
+    x0i = a[5] + a[21];
+    x1r = a[4] - a[20];
+    x1i = a[5] - a[21];
+    x2r = a[12] + a[28];
+    x2i = a[13] + a[29];
+    x3r = a[12] - a[28];
+    x3i = a[13] - a[29];
+    y2r = x0r + x2r;
+    y2i = x0i + x2i;
+    y6r = x0r - x2r;
+    y6i = x0i - x2i;
+    x0r = x1r - x3i;
+    x0i = x1i + x3r;
+    y10r = wn4r * (x0r - x0i);
+    y10i = wn4r * (x0i + x0r);
+    x0r = x1r + x3i;
+    x0i = x1i - x3r;
+    y14r = wn4r * (x0r + x0i);
+    y14i = wn4r * (x0i - x0r);
+    x0r = a[6] + a[22];
+    x0i = a[7] + a[23];
+    x1r = a[6] - a[22];
+    x1i = a[7] - a[23];
+    x2r = a[14] + a[30];
+    x2i = a[15] + a[31];
+    x3r = a[14] - a[30];
+    x3i = a[15] - a[31];
+    y3r = x0r + x2r;
+    y3i = x0i + x2i;
+    y7r = x0r - x2r;
+    y7i = x0i - x2i;
+    x0r = x1r - x3i;
+    x0i = x1i + x3r;
+    y11r = wk1i * x0r - wk1r * x0i;
+    y11i = wk1i * x0i + wk1r * x0r;
+    x0r = x1r + x3i;
+    x0i = x1i - x3r;
+    y15r = wk1r * x0r - wk1i * x0i;
+    y15i = wk1r * x0i + wk1i * x0r;
+    x0r = y12r - y14r;
+    x0i = y12i - y14i;
+    x1r = y12r + y14r;
+    x1i = y12i + y14i;
+    x2r = y13r - y15r;
+    x2i = y13i - y15i;
+    x3r = y13r + y15r;
+    x3i = y13i + y15i;
+    a[24] = x0r + x2r;
+    a[25] = x0i + x2i;
+    a[26] = x0r - x2r;
+    a[27] = x0i - x2i;
+    a[28] = x1r - x3i;
+    a[29] = x1i + x3r;
+    a[30] = x1r + x3i;
+    a[31] = x1i - x3r;
+    x0r = y8r + y10r;
+    x0i = y8i + y10i;
+    x1r = y8r - y10r;
+    x1i = y8i - y10i;
+    x2r = y9r + y11r;
+    x2i = y9i + y11i;
+    x3r = y9r - y11r;
+    x3i = y9i - y11i;
+    a[16] = x0r + x2r;
+    a[17] = x0i + x2i;
+    a[18] = x0r - x2r;
+    a[19] = x0i - x2i;
+    a[20] = x1r - x3i;
+    a[21] = x1i + x3r;
+    a[22] = x1r + x3i;
+    a[23] = x1i - x3r;
+    x0r = y5r - y7i;
+    x0i = y5i + y7r;
+    x2r = wn4r * (x0r - x0i);
+    x2i = wn4r * (x0i + x0r);
+    x0r = y5r + y7i;
+    x0i = y5i - y7r;
+    x3r = wn4r * (x0r - x0i);
+    x3i = wn4r * (x0i + x0r);
+    x0r = y4r - y6i;
+    x0i = y4i + y6r;
+    x1r = y4r + y6i;
+    x1i = y4i - y6r;
+    a[8] = x0r + x2r;
+    a[9] = x0i + x2i;
+    a[10] = x0r - x2r;
+    a[11] = x0i - x2i;
+    a[12] = x1r - x3i;
+    a[13] = x1i + x3r;
+    a[14] = x1r + x3i;
+    a[15] = x1i - x3r;
+    x0r = y0r + y2r;
+    x0i = y0i + y2i;
+    x1r = y0r - y2r;
+    x1i = y0i - y2i;
+    x2r = y1r + y3r;
+    x2i = y1i + y3i;
+    x3r = y1r - y3r;
+    x3i = y1i - y3i;
+    a[0] = x0r + x2r;
+    a[1] = x0i + x2i;
+    a[2] = x0r - x2r;
+    a[3] = x0i - x2i;
+    a[4] = x1r - x3i;
+    a[5] = x1i + x3r;
+    a[6] = x1r + x3i;
+    a[7] = x1i - x3r;
+}
+
+
+void cftf162(double *a)
+{
+    double wn4r, wk1r, wk1i, wk2r, wk2i, wk3r, wk3i, 
+        x0r, x0i, x1r, x1i, x2r, x2i, 
+        y0r, y0i, y1r, y1i, y2r, y2i, y3r, y3i, 
+        y4r, y4i, y5r, y5i, y6r, y6i, y7r, y7i, 
+        y8r, y8i, y9r, y9i, y10r, y10i, y11r, y11i, 
+        y12r, y12i, y13r, y13i, y14r, y14i, y15r, y15i;
+    
+    wn4r = WR5000;
+    wk1r = WR1250;
+    wk1i = WI1250;
+    wk2r = WR2500;
+    wk2i = WI2500;
+    wk3r = WR3750;
+    wk3i = WI3750;
+    x1r = a[0] - a[17];
+    x1i = a[1] + a[16];
+    x0r = a[8] - a[25];
+    x0i = a[9] + a[24];
+    x2r = wn4r * (x0r - x0i);
+    x2i = wn4r * (x0i + x0r);
+    y0r = x1r + x2r;
+    y0i = x1i + x2i;
+    y4r = x1r - x2r;
+    y4i = x1i - x2i;
+    x1r = a[0] + a[17];
+    x1i = a[1] - a[16];
+    x0r = a[8] + a[25];
+    x0i = a[9] - a[24];
+    x2r = wn4r * (x0r - x0i);
+    x2i = wn4r * (x0i + x0r);
+    y8r = x1r - x2i;
+    y8i = x1i + x2r;
+    y12r = x1r + x2i;
+    y12i = x1i - x2r;
+    x0r = a[2] - a[19];
+    x0i = a[3] + a[18];
+    x1r = wk1r * x0r - wk1i * x0i;
+    x1i = wk1r * x0i + wk1i * x0r;
+    x0r = a[10] - a[27];
+    x0i = a[11] + a[26];
+    x2r = wk3i * x0r - wk3r * x0i;
+    x2i = wk3i * x0i + wk3r * x0r;
+    y1r = x1r + x2r;
+    y1i = x1i + x2i;
+    y5r = x1r - x2r;
+    y5i = x1i - x2i;
+    x0r = a[2] + a[19];
+    x0i = a[3] - a[18];
+    x1r = wk3r * x0r - wk3i * x0i;
+    x1i = wk3r * x0i + wk3i * x0r;
+    x0r = a[10] + a[27];
+    x0i = a[11] - a[26];
+    x2r = wk1r * x0r + wk1i * x0i;
+    x2i = wk1r * x0i - wk1i * x0r;
+    y9r = x1r - x2r;
+    y9i = x1i - x2i;
+    y13r = x1r + x2r;
+    y13i = x1i + x2i;
+    x0r = a[4] - a[21];
+    x0i = a[5] + a[20];
+    x1r = wk2r * x0r - wk2i * x0i;
+    x1i = wk2r * x0i + wk2i * x0r;
+    x0r = a[12] - a[29];
+    x0i = a[13] + a[28];
+    x2r = wk2i * x0r - wk2r * x0i;
+    x2i = wk2i * x0i + wk2r * x0r;
+    y2r = x1r + x2r;
+    y2i = x1i + x2i;
+    y6r = x1r - x2r;
+    y6i = x1i - x2i;
+    x0r = a[4] + a[21];
+    x0i = a[5] - a[20];
+    x1r = wk2i * x0r - wk2r * x0i;
+    x1i = wk2i * x0i + wk2r * x0r;
+    x0r = a[12] + a[29];
+    x0i = a[13] - a[28];
+    x2r = wk2r * x0r - wk2i * x0i;
+    x2i = wk2r * x0i + wk2i * x0r;
+    y10r = x1r - x2r;
+    y10i = x1i - x2i;
+    y14r = x1r + x2r;
+    y14i = x1i + x2i;
+    x0r = a[6] - a[23];
+    x0i = a[7] + a[22];
+    x1r = wk3r * x0r - wk3i * x0i;
+    x1i = wk3r * x0i + wk3i * x0r;
+    x0r = a[14] - a[31];
+    x0i = a[15] + a[30];
+    x2r = wk1i * x0r - wk1r * x0i;
+    x2i = wk1i * x0i + wk1r * x0r;
+    y3r = x1r + x2r;
+    y3i = x1i + x2i;
+    y7r = x1r - x2r;
+    y7i = x1i - x2i;
+    x0r = a[6] + a[23];
+    x0i = a[7] - a[22];
+    x1r = wk1i * x0r + wk1r * x0i;
+    x1i = wk1i * x0i - wk1r * x0r;
+    x0r = a[14] + a[31];
+    x0i = a[15] - a[30];
+    x2r = wk3i * x0r - wk3r * x0i;
+    x2i = wk3i * x0i + wk3r * x0r;
+    y11r = x1r + x2r;
+    y11i = x1i + x2i;
+    y15r = x1r - x2r;
+    y15i = x1i - x2i;
+    x1r = y0r + y2r;
+    x1i = y0i + y2i;
+    x2r = y1r + y3r;
+    x2i = y1i + y3i;
+    a[0] = x1r + x2r;
+    a[1] = x1i + x2i;
+    a[2] = x1r - x2r;
+    a[3] = x1i - x2i;
+    x1r = y0r - y2r;
+    x1i = y0i - y2i;
+    x2r = y1r - y3r;
+    x2i = y1i - y3i;
+    a[4] = x1r - x2i;
+    a[5] = x1i + x2r;
+    a[6] = x1r + x2i;
+    a[7] = x1i - x2r;
+    x1r = y4r - y6i;
+    x1i = y4i + y6r;
+    x0r = y5r - y7i;
+    x0i = y5i + y7r;
+    x2r = wn4r * (x0r - x0i);
+    x2i = wn4r * (x0i + x0r);
+    a[8] = x1r + x2r;
+    a[9] = x1i + x2i;
+    a[10] = x1r - x2r;
+    a[11] = x1i - x2i;
+    x1r = y4r + y6i;
+    x1i = y4i - y6r;
+    x0r = y5r + y7i;
+    x0i = y5i - y7r;
+    x2r = wn4r * (x0r - x0i);
+    x2i = wn4r * (x0i + x0r);
+    a[12] = x1r - x2i;
+    a[13] = x1i + x2r;
+    a[14] = x1r + x2i;
+    a[15] = x1i - x2r;
+    x1r = y8r + y10r;
+    x1i = y8i + y10i;
+    x2r = y9r - y11r;
+    x2i = y9i - y11i;
+    a[16] = x1r + x2r;
+    a[17] = x1i + x2i;
+    a[18] = x1r - x2r;
+    a[19] = x1i - x2i;
+    x1r = y8r - y10r;
+    x1i = y8i - y10i;
+    x2r = y9r + y11r;
+    x2i = y9i + y11i;
+    a[20] = x1r - x2i;
+    a[21] = x1i + x2r;
+    a[22] = x1r + x2i;
+    a[23] = x1i - x2r;
+    x1r = y12r - y14i;
+    x1i = y12i + y14r;
+    x0r = y13r + y15i;
+    x0i = y13i - y15r;
+    x2r = wn4r * (x0r - x0i);
+    x2i = wn4r * (x0i + x0r);
+    a[24] = x1r + x2r;
+    a[25] = x1i + x2i;
+    a[26] = x1r - x2r;
+    a[27] = x1i - x2i;
+    x1r = y12r + y14i;
+    x1i = y12i - y14r;
+    x0r = y13r - y15i;
+    x0i = y13i + y15r;
+    x2r = wn4r * (x0r - x0i);
+    x2i = wn4r * (x0i + x0r);
+    a[28] = x1r - x2i;
+    a[29] = x1i + x2r;
+    a[30] = x1r + x2i;
+    a[31] = x1i - x2r;
+}
+
+
+void cftf081(double *a)
+{
+    double wn4r, x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i, 
+        y0r, y0i, y1r, y1i, y2r, y2i, y3r, y3i, 
+        y4r, y4i, y5r, y5i, y6r, y6i, y7r, y7i;
+    
+    wn4r = WR5000;
+    x0r = a[0] + a[8];
+    x0i = a[1] + a[9];
+    x1r = a[0] - a[8];
+    x1i = a[1] - a[9];
+    x2r = a[4] + a[12];
+    x2i = a[5] + a[13];
+    x3r = a[4] - a[12];
+    x3i = a[5] - a[13];
+    y0r = x0r + x2r;
+    y0i = x0i + x2i;
+    y2r = x0r - x2r;
+    y2i = x0i - x2i;
+    y1r = x1r - x3i;
+    y1i = x1i + x3r;
+    y3r = x1r + x3i;
+    y3i = x1i - x3r;
+    x0r = a[2] + a[10];
+    x0i = a[3] + a[11];
+    x1r = a[2] - a[10];
+    x1i = a[3] - a[11];
+    x2r = a[6] + a[14];
+    x2i = a[7] + a[15];
+    x3r = a[6] - a[14];
+    x3i = a[7] - a[15];
+    y4r = x0r + x2r;
+    y4i = x0i + x2i;
+    y6r = x0r - x2r;
+    y6i = x0i - x2i;
+    x0r = x1r - x3i;
+    x0i = x1i + x3r;
+    x2r = x1r + x3i;
+    x2i = x1i - x3r;
+    y5r = wn4r * (x0r - x0i);
+    y5i = wn4r * (x0r + x0i);
+    y7r = wn4r * (x2r - x2i);
+    y7i = wn4r * (x2r + x2i);
+    a[8] = y1r + y5r;
+    a[9] = y1i + y5i;
+    a[10] = y1r - y5r;
+    a[11] = y1i - y5i;
+    a[12] = y3r - y7i;
+    a[13] = y3i + y7r;
+    a[14] = y3r + y7i;
+    a[15] = y3i - y7r;
+    a[0] = y0r + y4r;
+    a[1] = y0i + y4i;
+    a[2] = y0r - y4r;
+    a[3] = y0i - y4i;
+    a[4] = y2r - y6i;
+    a[5] = y2i + y6r;
+    a[6] = y2r + y6i;
+    a[7] = y2i - y6r;
+}
+
+
+void cftf082(double *a)
+{
+    double wn4r, wk1r, wk1i, x0r, x0i, x1r, x1i, 
+        y0r, y0i, y1r, y1i, y2r, y2i, y3r, y3i, 
+        y4r, y4i, y5r, y5i, y6r, y6i, y7r, y7i;
+    
+    wn4r = WR5000;
+    wk1r = WR2500;
+    wk1i = WI2500;
+    y0r = a[0] - a[9];
+    y0i = a[1] + a[8];
+    y1r = a[0] + a[9];
+    y1i = a[1] - a[8];
+    x0r = a[4] - a[13];
+    x0i = a[5] + a[12];
+    y2r = wn4r * (x0r - x0i);
+    y2i = wn4r * (x0i + x0r);
+    x0r = a[4] + a[13];
+    x0i = a[5] - a[12];
+    y3r = wn4r * (x0r - x0i);
+    y3i = wn4r * (x0i + x0r);
+    x0r = a[2] - a[11];
+    x0i = a[3] + a[10];
+    y4r = wk1r * x0r - wk1i * x0i;
+    y4i = wk1r * x0i + wk1i * x0r;
+    x0r = a[2] + a[11];
+    x0i = a[3] - a[10];
+    y5r = wk1i * x0r - wk1r * x0i;
+    y5i = wk1i * x0i + wk1r * x0r;
+    x0r = a[6] - a[15];
+    x0i = a[7] + a[14];
+    y6r = wk1i * x0r - wk1r * x0i;
+    y6i = wk1i * x0i + wk1r * x0r;
+    x0r = a[6] + a[15];
+    x0i = a[7] - a[14];
+    y7r = wk1r * x0r - wk1i * x0i;
+    y7i = wk1r * x0i + wk1i * x0r;
+    x0r = y0r + y2r;
+    x0i = y0i + y2i;
+    x1r = y4r + y6r;
+    x1i = y4i + y6i;
+    a[0] = x0r + x1r;
+    a[1] = x0i + x1i;
+    a[2] = x0r - x1r;
+    a[3] = x0i - x1i;
+    x0r = y0r - y2r;
+    x0i = y0i - y2i;
+    x1r = y4r - y6r;
+    x1i = y4i - y6i;
+    a[4] = x0r - x1i;
+    a[5] = x0i + x1r;
+    a[6] = x0r + x1i;
+    a[7] = x0i - x1r;
+    x0r = y1r - y3i;
+    x0i = y1i + y3r;
+    x1r = y5r - y7r;
+    x1i = y5i - y7i;
+    a[8] = x0r + x1r;
+    a[9] = x0i + x1i;
+    a[10] = x0r - x1r;
+    a[11] = x0i - x1i;
+    x0r = y1r + y3i;
+    x0i = y1i - y3r;
+    x1r = y5r + y7r;
+    x1i = y5i + y7i;
+    a[12] = x0r - x1i;
+    a[13] = x0i + x1r;
+    a[14] = x0r + x1i;
+    a[15] = x0i - x1r;
+}
+
+
+void cftf040(double *a)
+{
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i;
+    
+    x0r = a[0] + a[4];
+    x0i = a[1] + a[5];
+    x1r = a[0] - a[4];
+    x1i = a[1] - a[5];
+    x2r = a[2] + a[6];
+    x2i = a[3] + a[7];
+    x3r = a[2] - a[6];
+    x3i = a[3] - a[7];
+    a[0] = x0r + x2r;
+    a[1] = x0i + x2i;
+    a[2] = x1r - x3i;
+    a[3] = x1i + x3r;
+    a[4] = x0r - x2r;
+    a[5] = x0i - x2i;
+    a[6] = x1r + x3i;
+    a[7] = x1i - x3r;
+}
+
+
+void cftb040(double *a)
+{
+    double x0r, x0i, x1r, x1i, x2r, x2i, x3r, x3i;
+    
+    x0r = a[0] + a[4];
+    x0i = a[1] + a[5];
+    x1r = a[0] - a[4];
+    x1i = a[1] - a[5];
+    x2r = a[2] + a[6];
+    x2i = a[3] + a[7];
+    x3r = a[2] - a[6];
+    x3i = a[3] - a[7];
+    a[0] = x0r + x2r;
+    a[1] = x0i + x2i;
+    a[2] = x1r + x3i;
+    a[3] = x1i - x3r;
+    a[4] = x0r - x2r;
+    a[5] = x0i - x2i;
+    a[6] = x1r - x3i;
+    a[7] = x1i + x3r;
+}
+
+
+void cftx020(double *a)
+{
+    double x0r, x0i;
+    
+    x0r = a[0] - a[2];
+    x0i = a[1] - a[3];
+    a[0] += a[2];
+    a[1] += a[3];
+    a[2] = x0r;
+    a[3] = x0i;
+}
+
+
+void rftfsub(int n, double *a)
+{
+    int i, i0, j, k;
+    double ec, w1r, w1i, wkr, wki, wdr, wdi, ss, xr, xi, yr, yi;
+    
+    ec = 2 * M_PI_2 / n;
+    wkr = 0;
+    wki = 0;
+    wdi = cos(ec);
+    wdr = sin(ec);
+    wdi *= wdr;
+    wdr *= wdr;
+    w1r = 1 - 2 * wdr;
+    w1i = 2 * wdi;
+    ss = 2 * w1i;
+    i = n >> 1;
+    for (;;) {
+        i0 = i - 4 * RDFT_LOOP_DIV;
+        if (i0 < 4) {
+            i0 = 4;
+        }
+        for (j = i - 4; j >= i0; j -= 4) {
+            k = n - j;
+            xr = a[j + 2] - a[k - 2];
+            xi = a[j + 3] + a[k - 1];
+            yr = wdr * xr - wdi * xi;
+            yi = wdr * xi + wdi * xr;
+            a[j + 2] -= yr;
+            a[j + 3] -= yi;
+            a[k - 2] += yr;
+            a[k - 1] -= yi;
+            wkr += ss * wdi;
+            wki += ss * (0.5 - wdr);
+            xr = a[j] - a[k];
+            xi = a[j + 1] + a[k + 1];
+            yr = wkr * xr - wki * xi;
+            yi = wkr * xi + wki * xr;
+            a[j] -= yr;
+            a[j + 1] -= yi;
+            a[k] += yr;
+            a[k + 1] -= yi;
+            wdr += ss * wki;
+            wdi += ss * (0.5 - wkr);
+        }
+        if (i0 == 4) {
+            break;
+        }
+        wkr = 0.5 * sin(ec * i0);
+        wki = 0.5 * cos(ec * i0);
+        wdr = 0.5 - (wkr * w1r - wki * w1i);
+        wdi = wkr * w1i + wki * w1r;
+        wkr = 0.5 - wkr;
+        i = i0;
+    }
+    xr = a[2] - a[n - 2];
+    xi = a[3] + a[n - 1];
+    yr = wdr * xr - wdi * xi;
+    yi = wdr * xi + wdi * xr;
+    a[2] -= yr;
+    a[3] -= yi;
+    a[n - 2] += yr;
+    a[n - 1] -= yi;
+}
+
+
+void rftbsub(int n, double *a)
+{
+    int i, i0, j, k;
+    double ec, w1r, w1i, wkr, wki, wdr, wdi, ss, xr, xi, yr, yi;
+    
+    ec = 2 * M_PI_2 / n;
+    wkr = 0;
+    wki = 0;
+    wdi = cos(ec);
+    wdr = sin(ec);
+    wdi *= wdr;
+    wdr *= wdr;
+    w1r = 1 - 2 * wdr;
+    w1i = 2 * wdi;
+    ss = 2 * w1i;
+    i = n >> 1;
+    for (;;) {
+        i0 = i - 4 * RDFT_LOOP_DIV;
+        if (i0 < 4) {
+            i0 = 4;
+        }
+        for (j = i - 4; j >= i0; j -= 4) {
+            k = n - j;
+            xr = a[j + 2] - a[k - 2];
+            xi = a[j + 3] + a[k - 1];
+            yr = wdr * xr + wdi * xi;
+            yi = wdr * xi - wdi * xr;
+            a[j + 2] -= yr;
+            a[j + 3] -= yi;
+            a[k - 2] += yr;
+            a[k - 1] -= yi;
+            wkr += ss * wdi;
+            wki += ss * (0.5 - wdr);
+            xr = a[j] - a[k];
+            xi = a[j + 1] + a[k + 1];
+            yr = wkr * xr + wki * xi;
+            yi = wkr * xi - wki * xr;
+            a[j] -= yr;
+            a[j + 1] -= yi;
+            a[k] += yr;
+            a[k + 1] -= yi;
+            wdr += ss * wki;
+            wdi += ss * (0.5 - wkr);
+        }
+        if (i0 == 4) {
+            break;
+        }
+        wkr = 0.5 * sin(ec * i0);
+        wki = 0.5 * cos(ec * i0);
+        wdr = 0.5 - (wkr * w1r - wki * w1i);
+        wdi = wkr * w1i + wki * w1r;
+        wkr = 0.5 - wkr;
+        i = i0;
+    }
+    xr = a[2] - a[n - 2];
+    xi = a[3] + a[n - 1];
+    yr = wdr * xr + wdi * xi;
+    yi = wdr * xi - wdi * xr;
+    a[2] -= yr;
+    a[3] -= yi;
+    a[n - 2] += yr;
+    a[n - 1] -= yi;
+}
+
+
+void dctsub(int n, double *a)
+{
+    int i, i0, j, k, m;
+    double ec, w1r, w1i, wkr, wki, wdr, wdi, ss, xr, xi, yr, yi;
+    
+    ec = M_PI_2 / n;
+    wkr = 0.5;
+    wki = 0.5;
+    w1r = cos(ec);
+    w1i = sin(ec);
+    wdr = 0.5 * (w1r - w1i);
+    wdi = 0.5 * (w1r + w1i);
+    ss = 2 * w1i;
+    m = n >> 1;
+    i = 0;
+    for (;;) {
+        i0 = i + 2 * DCST_LOOP_DIV;
+        if (i0 > m - 2) {
+            i0 = m - 2;
+        }
+        for (j = i + 2; j <= i0; j += 2) {
+            k = n - j;
+            xr = wdi * a[j - 1] - wdr * a[k + 1];
+            xi = wdr * a[j - 1] + wdi * a[k + 1];
+            wkr -= ss * wdi;
+            wki += ss * wdr;
+            yr = wki * a[j] - wkr * a[k];
+            yi = wkr * a[j] + wki * a[k];
+            wdr -= ss * wki;
+            wdi += ss * wkr;
+            a[k + 1] = xr;
+            a[k] = yr;
+            a[j - 1] = xi;
+            a[j] = yi;
+        }
+        if (i0 == m - 2) {
+            break;
+        }
+        wdr = cos(ec * i0);
+        wdi = sin(ec * i0);
+        wkr = 0.5 * (wdr - wdi);
+        wki = 0.5 * (wdr + wdi);
+        wdr = wkr * w1r - wki * w1i;
+        wdi = wkr * w1i + wki * w1r;
+        i = i0;
+    }
+    xr = wdi * a[m - 1] - wdr * a[m + 1];
+    a[m - 1] = wdr * a[m - 1] + wdi * a[m + 1];
+    a[m + 1] = xr;
+    a[m] *= WR5000;
+}
+
+
+void dstsub(int n, double *a)
+{
+    int i, i0, j, k, m;
+    double ec, w1r, w1i, wkr, wki, wdr, wdi, ss, xr, xi, yr, yi;
+    
+    ec = M_PI_2 / n;
+    wkr = 0.5;
+    wki = 0.5;
+    w1r = cos(ec);
+    w1i = sin(ec);
+    wdr = 0.5 * (w1r - w1i);
+    wdi = 0.5 * (w1r + w1i);
+    ss = 2 * w1i;
+    m = n >> 1;
+    i = 0;
+    for (;;) {
+        i0 = i + 2 * DCST_LOOP_DIV;
+        if (i0 > m - 2) {
+            i0 = m - 2;
+        }
+        for (j = i + 2; j <= i0; j += 2) {
+            k = n - j;
+            xr = wdi * a[k + 1] - wdr * a[j - 1];
+            xi = wdr * a[k + 1] + wdi * a[j - 1];
+            wkr -= ss * wdi;
+            wki += ss * wdr;
+            yr = wki * a[k] - wkr * a[j];
+            yi = wkr * a[k] + wki * a[j];
+            wdr -= ss * wki;
+            wdi += ss * wkr;
+            a[j - 1] = xr;
+            a[j] = yr;
+            a[k + 1] = xi;
+            a[k] = yi;
+        }
+        if (i0 == m - 2) {
+            break;
+        }
+        wdr = cos(ec * i0);
+        wdi = sin(ec * i0);
+        wkr = 0.5 * (wdr - wdi);
+        wki = 0.5 * (wdr + wdi);
+        wdr = wkr * w1r - wki * w1i;
+        wdi = wkr * w1i + wki * w1r;
+        i = i0;
+    }
+    xr = wdi * a[m + 1] - wdr * a[m - 1];
+    a[m + 1] = wdr * a[m + 1] + wdi * a[m - 1];
+    a[m - 1] = xr;
+    a[m] *= WR5000;
+}
+
+
+void dctsub4(int n, double *a)
+{
+    int m;
+    double wki, wdr, wdi, xr;
+    
+    wki = WR5000;
+    m = n >> 1;
+    if (m == 2) {
+        wdr = wki * WI2500;
+        wdi = wki * WR2500;
+        xr = wdi * a[1] - wdr * a[3];
+        a[1] = wdr * a[1] + wdi * a[3];
+        a[3] = xr;
+    }
+    a[m] *= wki;
+}
+
+
+void dstsub4(int n, double *a)
+{
+    int m;
+    double wki, wdr, wdi, xr;
+    
+    wki = WR5000;
+    m = n >> 1;
+    if (m == 2) {
+        wdr = wki * WI2500;
+        wdi = wki * WR2500;
+        xr = wdi * a[3] - wdr * a[1];
+        a[3] = wdr * a[3] + wdi * a[1];
+        a[1] = xr;
+    }
+    a[m] *= wki;
+}
+
diff --git a/third_party/tflite-micro/third_party/fft2d/readme.txt b/third_party/tflite-micro/third_party/fft2d/readme.txt
new file mode 100644
index 00000000..145a081b
--- /dev/null
+++ b/third_party/tflite-micro/third_party/fft2d/readme.txt
@@ -0,0 +1,167 @@
+General Purpose FFT (Fast Fourier/Cosine/Sine Transform) Package
+
+Description:
+    A package to calculate Discrete Fourier/Cosine/Sine Transforms of 
+    1-dimensional sequences of length 2^N.
+
+Files:
+    fft4g.c    : FFT Package in C       - Fast Version   I   (radix 4,2)
+    fft4g.f    : FFT Package in Fortran - Fast Version   I   (radix 4,2)
+    fft4g_h.c  : FFT Package in C       - Simple Version I   (radix 4,2)
+    fft8g.c    : FFT Package in C       - Fast Version   II  (radix 8,4,2)
+    fft8g.f    : FFT Package in Fortran - Fast Version   II  (radix 8,4,2)
+    fft8g_h.c  : FFT Package in C       - Simple Version II  (radix 8,4,2)
+    fftsg.c    : FFT Package in C       - Fast Version   III (Split-Radix)
+    fftsg.f    : FFT Package in Fortran - Fast Version   III (Split-Radix)
+    fftsg_h.c  : FFT Package in C       - Simple Version III (Split-Radix)
+    readme.txt : Readme File
+    sample1/   : Test Directory
+        Makefile    : for gcc, cc
+        Makefile.f77: for Fortran
+        testxg.c    : Test Program for "fft*g.c"
+        testxg.f    : Test Program for "fft*g.f"
+        testxg_h.c  : Test Program for "fft*g_h.c"
+    sample2/   : Benchmark Directory
+        Makefile    : for gcc, cc
+        Makefile.pth: POSIX Thread version
+        pi_fft.c    : PI(= 3.1415926535897932384626...) Calculation Program
+                      for a Benchmark Test for "fft*g.c"
+
+Difference of the Files:
+    C and Fortran versions are equal and 
+    the same routines are in each version.
+    "fft4g*.*" are optimized for most machines.
+    "fft8g*.*" are fast on the UltraSPARC.
+    "fftsg*.*" are optimized for the machines that 
+    have the multi-level (L1,L2,etc) cache.
+    The simple versions "fft*g_h.c" use no work area, but 
+    the fast versions "fft*g.*" use work areas.
+    The fast versions "fft*g.*" have the same specification.
+
+Routines in the Package:
+    cdft: Complex Discrete Fourier Transform
+    rdft: Real Discrete Fourier Transform
+    ddct: Discrete Cosine Transform
+    ddst: Discrete Sine Transform
+    dfct: Cosine Transform of RDFT (Real Symmetric DFT)
+    dfst: Sine Transform of RDFT (Real Anti-symmetric DFT)
+
+Usage:
+    Please refer to the comments in the "fft**.*" file which 
+    you want to use. Brief explanations are in the block 
+    comments of each package. The examples are also given in 
+    the test programs.
+
+Method:
+    -------- cdft --------
+    fft4g*.*, fft8g*.*:
+        A method of in-place, radix 2^M, Sande-Tukey (decimation in 
+        frequency). Index of the butterfly loop is in bit 
+        reverse order to keep continuous memory access.
+    fftsg*.*:
+        A method of in-place, Split-Radix, recursive fast 
+        algorithm.
+    -------- rdft --------
+    A method with a following butterfly operation appended to "cdft".
+    In forward transform :
+        A[k] = sum_j=0^n-1 a[j]*W(n)^(j*k), 0<=k<=n/2, 
+            W(n) = exp(2*pi*i/n), 
+    this routine makes an array x[] :
+        x[j] = a[2*j] + i*a[2*j+1], 0<=j<n/2
+    and calls "cdft" of length n/2 :
+        X[k] = sum_j=0^n/2-1 x[j] * W(n/2)^(j*k), 0<=k<n.
+    The result A[k] are :
+        A[k]     = X[k]     - (1+i*W(n)^k)/2 * (X[k]-conjg(X[n/2-k])), 
+        A[n/2-k] = X[n/2-k] + 
+                        conjg((1+i*W(n)^k)/2 * (X[k]-conjg(X[n/2-k]))), 
+            0<=k<=n/2
+        (notes: conjg() is a complex conjugate, X[n/2]=X[0]).
+    -------- ddct --------
+    A method with a following butterfly operation appended to "rdft".
+    In backward transform :
+        C[k] = sum_j=0^n-1 a[j]*cos(pi*j*(k+1/2)/n), 0<=k<n, 
+    this routine makes an array r[] :
+        r[0] = a[0], 
+        r[j]   = Re((a[j] - i*a[n-j]) * W(4*n)^j*(1+i)/2), 
+        r[n-j] = Im((a[j] - i*a[n-j]) * W(4*n)^j*(1+i)/2), 
+            0<j<=n/2
+    and calls "rdft" of length n :
+        A[k] = sum_j=0^n-1 r[j]*W(n)^(j*k), 0<=k<=n/2, 
+            W(n) = exp(2*pi*i/n).
+    The result C[k] are :
+        C[2*k]   =  Re(A[k] * (1-i)), 
+        C[2*k-1] = -Im(A[k] * (1-i)).
+    -------- ddst --------
+    A method with a following butterfly operation appended to "rdft".
+    In backward transform :
+        S[k] = sum_j=1^n A[j]*sin(pi*j*(k+1/2)/n), 0<=k<n, 
+    this routine makes an array r[] :
+        r[0] = a[0], 
+        r[j]   = Im((a[n-j] - i*a[j]) * W(4*n)^j*(1+i)/2), 
+        r[n-j] = Re((a[n-j] - i*a[j]) * W(4*n)^j*(1+i)/2), 
+            0<j<=n/2
+    and calls "rdft" of length n :
+        A[k] = sum_j=0^n-1 r[j]*W(n)^(j*k), 0<=k<=n/2, 
+            W(n) = exp(2*pi*i/n).
+    The result S[k] are :
+        S[2*k]   =  Re(A[k] * (1+i)), 
+        S[2*k-1] = -Im(A[k] * (1+i)).
+    -------- dfct --------
+    A method to split into "dfct" and "ddct" of half length.
+    The transform :
+        C[k] = sum_j=0^n a[j]*cos(pi*j*k/n), 0<=k<=n
+    is divided into :
+        C[2*k]   = sum'_j=0^n/2  (a[j]+a[n-j])*cos(pi*j*k/(n/2)), 
+        C[2*k+1] = sum_j=0^n/2-1 (a[j]-a[n-j])*cos(pi*j*(k+1/2)/(n/2))
+        (sum' is a summation whose last term multiplies 1/2).
+    This routine uses "ddct" recursively.
+    To keep the in-place operation, the data in fft*g_h.*
+    are sorted in bit reversal order.
+    -------- dfst --------
+    A method to split into "dfst" and "ddst" of half length.
+    The transform :
+        S[k] = sum_j=1^n-1 a[j]*sin(pi*j*k/n), 0<k<n
+    is divided into :
+        S[2*k]   = sum_j=1^n/2-1 (a[j]-a[n-j])*sin(pi*j*k/(n/2)), 
+        S[2*k+1] = sum'_j=1^n/2  (a[j]+a[n-j])*sin(pi*j*(k+1/2)/(n/2))
+        (sum' is a summation whose last term multiplies 1/2).
+    This routine uses "ddst" recursively.
+    To keep the in-place operation, the data in fft*g_h.*
+    are sorted in bit reversal order.
+
+Reference:
+    * Masatake MORI, Makoto NATORI, Tatuo TORII: Suchikeisan, 
+      Iwanamikouzajyouhoukagaku18, Iwanami, 1982 (Japanese)
+    * Henri J. Nussbaumer: Fast Fourier Transform and Convolution 
+      Algorithms, Springer Verlag, 1982
+    * C. S. Burrus, Notes on the FFT (with large FFT paper list)
+      http://www-dsp.rice.edu/research/fft/fftnote.asc
+
+Copyright:
+    Copyright(C) 1996-2001 Takuya OOURA
+    email: ooura@mmm.t.u-tokyo.ac.jp
+    download: http://momonga.t.u-tokyo.ac.jp/~ooura/fft.html
+    You may use, copy, modify this code for any purpose and 
+    without fee. You may distribute this ORIGINAL package.
+
+History:
+    ...
+    Dec. 1995  : Edit the General Purpose FFT
+    Mar. 1996  : Change the specification
+    Jun. 1996  : Change the method of trigonometric function table
+    Sep. 1996  : Modify the documents
+    Feb. 1997  : Change the butterfly loops
+    Dec. 1997  : Modify the documents
+    Dec. 1997  : Add "fft4g.*"
+    Jul. 1998  : Fix some bugs in the documents
+    Jul. 1998  : Add "fft8g.*" and delete "fft4f.*"
+    Jul. 1998  : Add a benchmark program "pi_fft.c"
+    Jul. 1999  : Add a simple version "fft*g_h.c"
+    Jul. 1999  : Add a Split-Radix FFT package "fftsg*.c"
+    Sep. 1999  : Reduce the memory operation (minor optimization)
+    Oct. 1999  : Change the butterfly structure of "fftsg*.c"
+    Oct. 1999  : Save the code size
+    Sep. 2001  : Add "fftsg.f"
+    Sep. 2001  : Add Pthread & Win32thread routines to "fftsg*.c"
+    Dec. 2006  : Fix a minor bug in "fftsg.f"
+
diff --git a/third_party/tflite-micro/third_party/flatbuffers/include/flatbuffers/flexbuffers.h b/third_party/tflite-micro/third_party/flatbuffers/include/flatbuffers/flexbuffers.h
index 1a109bbd..b4b0332b 100644
--- a/third_party/tflite-micro/third_party/flatbuffers/include/flatbuffers/flexbuffers.h
+++ b/third_party/tflite-micro/third_party/flatbuffers/include/flatbuffers/flexbuffers.h
@@ -156,7 +156,6 @@ inline uint64_t ReadUInt64(const uint8_t *data, uint8_t byte_width) {
   // TODO: GCC apparently replaces memcpy by a rep movsb, but only if count is a
   // constant, which here it isn't. Test if memcpy is still faster than
   // the conditionals in ReadSizedScalar. Can also use inline asm.
-
   // clang-format off
   #if defined(_MSC_VER) && defined(_M_X64) && !defined(_M_ARM64EC)
   // This is 64-bit Windows only, __movsb does not work on 32-bit Windows.
@@ -494,24 +493,9 @@ class Reference {
           return static_cast<double>(ReadUInt64(Indirect(), byte_width_));
         case FBT_NULL: return 0.0;
         case FBT_STRING: {
-#if 1
-#if !defined( _MSC_VER)
-#pragma GCC diagnostic push
-#pragma GCC diagnostic ignored "-Wnull-dereference"
-#endif
-          // See b/173239141 for additional context. Patched via
-          // micro/tools/make/flexbuffers_download.sh
-          // Introduce a segfault for an unsupported code path for TFLM.
-          return *(static_cast<double*>(nullptr));
-#if !defined( _MSC_VER)
-#pragma GCC diagnostic pop
-#endif
-#else
-          // This is the original code
           double d;
           flatbuffers::StringToNumber(AsString().c_str(), &d);
           return d;
-#endif
         }
         case FBT_VECTOR: return static_cast<double>(AsVector().size());
         case FBT_BOOL:
@@ -594,7 +578,7 @@ class Reference {
           // unquoted if it looks like an "identifier":
           const char *p = keys[i].AsKey();
           if (!flatbuffers::is_alpha(*p) && *p != '_') {
-            kq = true;
+              kq = true;
           } else {
             while (*++p) {
               if (!flatbuffers::is_alnum(*p) && *p != '_') {
@@ -1438,12 +1422,10 @@ class Builder FLATBUFFERS_FINAL_CLASS {
 
   template<typename T> static Type GetScalarType() {
     static_assert(flatbuffers::is_scalar<T>::value, "Unrelated types");
-    return flatbuffers::is_floating_point<T>::value
-               ? FBT_FLOAT
-               : flatbuffers::is_same<T, bool>::value
-                     ? FBT_BOOL
-                     : (flatbuffers::is_unsigned<T>::value ? FBT_UINT
-                                                           : FBT_INT);
+    return flatbuffers::is_floating_point<T>::value ? FBT_FLOAT
+           : flatbuffers::is_same<T, bool>::value
+               ? FBT_BOOL
+               : (flatbuffers::is_unsigned<T>::value ? FBT_UINT : FBT_INT);
   }
 
  public:
@@ -1755,9 +1737,9 @@ class Verifier FLATBUFFERS_FINAL_CLASS {
     if (!Check(depth_ <= max_depth_ && num_vectors_ <= max_vectors_))
       return false;
     auto size_byte_width = r.byte_width_;
-    if (!VerifyBeforePointer(p, size_byte_width)) return false;
-    FLEX_CHECK_VERIFIED(p - size_byte_width,
+    FLEX_CHECK_VERIFIED(p,
                         PackedType(Builder::WidthB(size_byte_width), r.type_));
+    if (!VerifyBeforePointer(p, size_byte_width)) return false;
     auto sized = Sized(p, size_byte_width);
     auto num_elems = sized.size();
     auto elem_byte_width = r.type_ == FBT_STRING || r.type_ == FBT_BLOB
@@ -1894,6 +1876,18 @@ inline bool VerifyBuffer(const uint8_t *buf, size_t buf_len,
   return verifier.VerifyBuffer();
 }
 
+#ifdef FLATBUFFERS_H_
+// This is a verifier utility function that works together with the
+// FlatBuffers verifier, which should only be present if flatbuffer.h
+// has been included (which it typically is in generated code).
+inline bool VerifyNestedFlexBuffer(const flatbuffers::Vector<uint8_t> *nv,
+                                   flatbuffers::Verifier &verifier) {
+  if (!nv) return true;
+  return verifier.Check(flexbuffers::VerifyBuffer(
+      nv->data(), nv->size(), verifier.GetFlexReuseTracker()));
+}
+#endif
+
 }  // namespace flexbuffers
 
 #if defined(_MSC_VER)
diff --git a/third_party/tflite-micro/third_party/flatbuffers/include/flatbuffers/flexbuffers.h.orig b/third_party/tflite-micro/third_party/flatbuffers/include/flatbuffers/flexbuffers.h.orig
new file mode 100644
index 00000000..1a109bbd
--- /dev/null
+++ b/third_party/tflite-micro/third_party/flatbuffers/include/flatbuffers/flexbuffers.h.orig
@@ -0,0 +1,1903 @@
+/*
+ * Copyright 2017 Google Inc. All rights reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FLATBUFFERS_FLEXBUFFERS_H_
+#define FLATBUFFERS_FLEXBUFFERS_H_
+
+#include <map>
+// Used to select STL variant.
+#include "flatbuffers/base.h"
+// We use the basic binary writing functions from the regular FlatBuffers.
+#include "flatbuffers/util.h"
+
+#ifdef _MSC_VER
+#  include <intrin.h>
+#endif
+
+#if defined(_MSC_VER)
+#  pragma warning(push)
+#  pragma warning(disable : 4127)  // C4127: conditional expression is constant
+#endif
+
+namespace flexbuffers {
+
+class Reference;
+class Map;
+
+// These are used in the lower 2 bits of a type field to determine the size of
+// the elements (and or size field) of the item pointed to (e.g. vector).
+enum BitWidth {
+  BIT_WIDTH_8 = 0,
+  BIT_WIDTH_16 = 1,
+  BIT_WIDTH_32 = 2,
+  BIT_WIDTH_64 = 3,
+};
+
+// These are used as the upper 6 bits of a type field to indicate the actual
+// type.
+enum Type {
+  FBT_NULL = 0,
+  FBT_INT = 1,
+  FBT_UINT = 2,
+  FBT_FLOAT = 3,
+  // Types above stored inline, types below (except FBT_BOOL) store an offset.
+  FBT_KEY = 4,
+  FBT_STRING = 5,
+  FBT_INDIRECT_INT = 6,
+  FBT_INDIRECT_UINT = 7,
+  FBT_INDIRECT_FLOAT = 8,
+  FBT_MAP = 9,
+  FBT_VECTOR = 10,      // Untyped.
+  FBT_VECTOR_INT = 11,  // Typed any size (stores no type table).
+  FBT_VECTOR_UINT = 12,
+  FBT_VECTOR_FLOAT = 13,
+  FBT_VECTOR_KEY = 14,
+  // DEPRECATED, use FBT_VECTOR or FBT_VECTOR_KEY instead.
+  // Read test.cpp/FlexBuffersDeprecatedTest() for details on why.
+  FBT_VECTOR_STRING_DEPRECATED = 15,
+  FBT_VECTOR_INT2 = 16,  // Typed tuple (no type table, no size field).
+  FBT_VECTOR_UINT2 = 17,
+  FBT_VECTOR_FLOAT2 = 18,
+  FBT_VECTOR_INT3 = 19,  // Typed triple (no type table, no size field).
+  FBT_VECTOR_UINT3 = 20,
+  FBT_VECTOR_FLOAT3 = 21,
+  FBT_VECTOR_INT4 = 22,  // Typed quad (no type table, no size field).
+  FBT_VECTOR_UINT4 = 23,
+  FBT_VECTOR_FLOAT4 = 24,
+  FBT_BLOB = 25,
+  FBT_BOOL = 26,
+  FBT_VECTOR_BOOL =
+      36,  // To Allow the same type of conversion of type to vector type
+
+  FBT_MAX_TYPE = 37
+};
+
+inline bool IsInline(Type t) { return t <= FBT_FLOAT || t == FBT_BOOL; }
+
+inline bool IsTypedVectorElementType(Type t) {
+  return (t >= FBT_INT && t <= FBT_STRING) || t == FBT_BOOL;
+}
+
+inline bool IsTypedVector(Type t) {
+  return (t >= FBT_VECTOR_INT && t <= FBT_VECTOR_STRING_DEPRECATED) ||
+         t == FBT_VECTOR_BOOL;
+}
+
+inline bool IsFixedTypedVector(Type t) {
+  return t >= FBT_VECTOR_INT2 && t <= FBT_VECTOR_FLOAT4;
+}
+
+inline Type ToTypedVector(Type t, size_t fixed_len = 0) {
+  FLATBUFFERS_ASSERT(IsTypedVectorElementType(t));
+  switch (fixed_len) {
+    case 0: return static_cast<Type>(t - FBT_INT + FBT_VECTOR_INT);
+    case 2: return static_cast<Type>(t - FBT_INT + FBT_VECTOR_INT2);
+    case 3: return static_cast<Type>(t - FBT_INT + FBT_VECTOR_INT3);
+    case 4: return static_cast<Type>(t - FBT_INT + FBT_VECTOR_INT4);
+    default: FLATBUFFERS_ASSERT(0); return FBT_NULL;
+  }
+}
+
+inline Type ToTypedVectorElementType(Type t) {
+  FLATBUFFERS_ASSERT(IsTypedVector(t));
+  return static_cast<Type>(t - FBT_VECTOR_INT + FBT_INT);
+}
+
+inline Type ToFixedTypedVectorElementType(Type t, uint8_t *len) {
+  FLATBUFFERS_ASSERT(IsFixedTypedVector(t));
+  auto fixed_type = t - FBT_VECTOR_INT2;
+  *len = static_cast<uint8_t>(fixed_type / 3 +
+                              2);  // 3 types each, starting from length 2.
+  return static_cast<Type>(fixed_type % 3 + FBT_INT);
+}
+
+// TODO: implement proper support for 8/16bit floats, or decide not to
+// support them.
+typedef int16_t half;
+typedef int8_t quarter;
+
+// TODO: can we do this without conditionals using intrinsics or inline asm
+// on some platforms? Given branch prediction the method below should be
+// decently quick, but it is the most frequently executed function.
+// We could do an (unaligned) 64-bit read if we ifdef out the platforms for
+// which that doesn't work (or where we'd read into un-owned memory).
+template<typename R, typename T1, typename T2, typename T4, typename T8>
+R ReadSizedScalar(const uint8_t *data, uint8_t byte_width) {
+  return byte_width < 4
+             ? (byte_width < 2
+                    ? static_cast<R>(flatbuffers::ReadScalar<T1>(data))
+                    : static_cast<R>(flatbuffers::ReadScalar<T2>(data)))
+             : (byte_width < 8
+                    ? static_cast<R>(flatbuffers::ReadScalar<T4>(data))
+                    : static_cast<R>(flatbuffers::ReadScalar<T8>(data)));
+}
+
+inline int64_t ReadInt64(const uint8_t *data, uint8_t byte_width) {
+  return ReadSizedScalar<int64_t, int8_t, int16_t, int32_t, int64_t>(
+      data, byte_width);
+}
+
+inline uint64_t ReadUInt64(const uint8_t *data, uint8_t byte_width) {
+  // This is the "hottest" function (all offset lookups use this), so worth
+  // optimizing if possible.
+  // TODO: GCC apparently replaces memcpy by a rep movsb, but only if count is a
+  // constant, which here it isn't. Test if memcpy is still faster than
+  // the conditionals in ReadSizedScalar. Can also use inline asm.
+
+  // clang-format off
+  #if defined(_MSC_VER) && defined(_M_X64) && !defined(_M_ARM64EC)
+  // This is 64-bit Windows only, __movsb does not work on 32-bit Windows.
+    uint64_t u = 0;
+    __movsb(reinterpret_cast<uint8_t *>(&u),
+            reinterpret_cast<const uint8_t *>(data), byte_width);
+    return flatbuffers::EndianScalar(u);
+  #else
+    return ReadSizedScalar<uint64_t, uint8_t, uint16_t, uint32_t, uint64_t>(
+             data, byte_width);
+  #endif
+  // clang-format on
+}
+
+inline double ReadDouble(const uint8_t *data, uint8_t byte_width) {
+  return ReadSizedScalar<double, quarter, half, float, double>(data,
+                                                               byte_width);
+}
+
+inline const uint8_t *Indirect(const uint8_t *offset, uint8_t byte_width) {
+  return offset - ReadUInt64(offset, byte_width);
+}
+
+template<typename T> const uint8_t *Indirect(const uint8_t *offset) {
+  return offset - flatbuffers::ReadScalar<T>(offset);
+}
+
+inline BitWidth WidthU(uint64_t u) {
+#define FLATBUFFERS_GET_FIELD_BIT_WIDTH(value, width)                   \
+  {                                                                     \
+    if (!((u) & ~((1ULL << (width)) - 1ULL))) return BIT_WIDTH_##width; \
+  }
+  FLATBUFFERS_GET_FIELD_BIT_WIDTH(u, 8);
+  FLATBUFFERS_GET_FIELD_BIT_WIDTH(u, 16);
+  FLATBUFFERS_GET_FIELD_BIT_WIDTH(u, 32);
+#undef FLATBUFFERS_GET_FIELD_BIT_WIDTH
+  return BIT_WIDTH_64;
+}
+
+inline BitWidth WidthI(int64_t i) {
+  auto u = static_cast<uint64_t>(i) << 1;
+  return WidthU(i >= 0 ? u : ~u);
+}
+
+inline BitWidth WidthF(double f) {
+  return static_cast<double>(static_cast<float>(f)) == f ? BIT_WIDTH_32
+                                                         : BIT_WIDTH_64;
+}
+
+// Base class of all types below.
+// Points into the data buffer and allows access to one type.
+class Object {
+ public:
+  Object(const uint8_t *data, uint8_t byte_width)
+      : data_(data), byte_width_(byte_width) {}
+
+ protected:
+  const uint8_t *data_;
+  uint8_t byte_width_;
+};
+
+// Object that has a size, obtained either from size prefix, or elsewhere.
+class Sized : public Object {
+ public:
+  // Size prefix.
+  Sized(const uint8_t *data, uint8_t byte_width)
+      : Object(data, byte_width), size_(read_size()) {}
+  // Manual size.
+  Sized(const uint8_t *data, uint8_t byte_width, size_t sz)
+      : Object(data, byte_width), size_(sz) {}
+  size_t size() const { return size_; }
+  // Access size stored in `byte_width_` bytes before data_ pointer.
+  size_t read_size() const {
+    return static_cast<size_t>(ReadUInt64(data_ - byte_width_, byte_width_));
+  }
+
+ protected:
+  size_t size_;
+};
+
+class String : public Sized {
+ public:
+  // Size prefix.
+  String(const uint8_t *data, uint8_t byte_width) : Sized(data, byte_width) {}
+  // Manual size.
+  String(const uint8_t *data, uint8_t byte_width, size_t sz)
+      : Sized(data, byte_width, sz) {}
+
+  size_t length() const { return size(); }
+  const char *c_str() const { return reinterpret_cast<const char *>(data_); }
+  std::string str() const { return std::string(c_str(), size()); }
+
+  static String EmptyString() {
+    static const char *empty_string = "";
+    return String(reinterpret_cast<const uint8_t *>(empty_string), 1, 0);
+  }
+  bool IsTheEmptyString() const { return data_ == EmptyString().data_; }
+};
+
+class Blob : public Sized {
+ public:
+  Blob(const uint8_t *data_buf, uint8_t byte_width)
+      : Sized(data_buf, byte_width) {}
+
+  static Blob EmptyBlob() {
+    static const uint8_t empty_blob[] = { 0 /*len*/ };
+    return Blob(empty_blob + 1, 1);
+  }
+  bool IsTheEmptyBlob() const { return data_ == EmptyBlob().data_; }
+  const uint8_t *data() const { return data_; }
+};
+
+class Vector : public Sized {
+ public:
+  Vector(const uint8_t *data, uint8_t byte_width) : Sized(data, byte_width) {}
+
+  Reference operator[](size_t i) const;
+
+  static Vector EmptyVector() {
+    static const uint8_t empty_vector[] = { 0 /*len*/ };
+    return Vector(empty_vector + 1, 1);
+  }
+  bool IsTheEmptyVector() const { return data_ == EmptyVector().data_; }
+};
+
+class TypedVector : public Sized {
+ public:
+  TypedVector(const uint8_t *data, uint8_t byte_width, Type element_type)
+      : Sized(data, byte_width), type_(element_type) {}
+
+  Reference operator[](size_t i) const;
+
+  static TypedVector EmptyTypedVector() {
+    static const uint8_t empty_typed_vector[] = { 0 /*len*/ };
+    return TypedVector(empty_typed_vector + 1, 1, FBT_INT);
+  }
+  bool IsTheEmptyVector() const {
+    return data_ == TypedVector::EmptyTypedVector().data_;
+  }
+
+  Type ElementType() { return type_; }
+
+  friend Reference;
+
+ private:
+  Type type_;
+
+  friend Map;
+};
+
+class FixedTypedVector : public Object {
+ public:
+  FixedTypedVector(const uint8_t *data, uint8_t byte_width, Type element_type,
+                   uint8_t len)
+      : Object(data, byte_width), type_(element_type), len_(len) {}
+
+  Reference operator[](size_t i) const;
+
+  static FixedTypedVector EmptyFixedTypedVector() {
+    static const uint8_t fixed_empty_vector[] = { 0 /* unused */ };
+    return FixedTypedVector(fixed_empty_vector, 1, FBT_INT, 0);
+  }
+  bool IsTheEmptyFixedTypedVector() const {
+    return data_ == FixedTypedVector::EmptyFixedTypedVector().data_;
+  }
+
+  Type ElementType() const { return type_; }
+  uint8_t size() const { return len_; }
+
+ private:
+  Type type_;
+  uint8_t len_;
+};
+
+class Map : public Vector {
+ public:
+  Map(const uint8_t *data, uint8_t byte_width) : Vector(data, byte_width) {}
+
+  Reference operator[](const char *key) const;
+  Reference operator[](const std::string &key) const;
+
+  Vector Values() const { return Vector(data_, byte_width_); }
+
+  TypedVector Keys() const {
+    const size_t num_prefixed_fields = 3;
+    auto keys_offset = data_ - byte_width_ * num_prefixed_fields;
+    return TypedVector(Indirect(keys_offset, byte_width_),
+                       static_cast<uint8_t>(
+                           ReadUInt64(keys_offset + byte_width_, byte_width_)),
+                       FBT_KEY);
+  }
+
+  static Map EmptyMap() {
+    static const uint8_t empty_map[] = {
+      0 /*keys_len*/, 0 /*keys_offset*/, 1 /*keys_width*/, 0 /*len*/
+    };
+    return Map(empty_map + 4, 1);
+  }
+
+  bool IsTheEmptyMap() const { return data_ == EmptyMap().data_; }
+};
+
+template<typename T>
+void AppendToString(std::string &s, T &&v, bool keys_quoted) {
+  s += "[ ";
+  for (size_t i = 0; i < v.size(); i++) {
+    if (i) s += ", ";
+    v[i].ToString(true, keys_quoted, s);
+  }
+  s += " ]";
+}
+
+class Reference {
+ public:
+  Reference()
+      : data_(nullptr), parent_width_(0), byte_width_(0), type_(FBT_NULL) {}
+
+  Reference(const uint8_t *data, uint8_t parent_width, uint8_t byte_width,
+            Type type)
+      : data_(data),
+        parent_width_(parent_width),
+        byte_width_(byte_width),
+        type_(type) {}
+
+  Reference(const uint8_t *data, uint8_t parent_width, uint8_t packed_type)
+      : data_(data), parent_width_(parent_width) {
+    byte_width_ = 1U << static_cast<BitWidth>(packed_type & 3);
+    type_ = static_cast<Type>(packed_type >> 2);
+  }
+
+  Type GetType() const { return type_; }
+
+  bool IsNull() const { return type_ == FBT_NULL; }
+  bool IsBool() const { return type_ == FBT_BOOL; }
+  bool IsInt() const { return type_ == FBT_INT || type_ == FBT_INDIRECT_INT; }
+  bool IsUInt() const {
+    return type_ == FBT_UINT || type_ == FBT_INDIRECT_UINT;
+  }
+  bool IsIntOrUint() const { return IsInt() || IsUInt(); }
+  bool IsFloat() const {
+    return type_ == FBT_FLOAT || type_ == FBT_INDIRECT_FLOAT;
+  }
+  bool IsNumeric() const { return IsIntOrUint() || IsFloat(); }
+  bool IsString() const { return type_ == FBT_STRING; }
+  bool IsKey() const { return type_ == FBT_KEY; }
+  bool IsVector() const { return type_ == FBT_VECTOR || type_ == FBT_MAP; }
+  bool IsUntypedVector() const { return type_ == FBT_VECTOR; }
+  bool IsTypedVector() const { return flexbuffers::IsTypedVector(type_); }
+  bool IsFixedTypedVector() const {
+    return flexbuffers::IsFixedTypedVector(type_);
+  }
+  bool IsAnyVector() const {
+    return (IsTypedVector() || IsFixedTypedVector() || IsVector());
+  }
+  bool IsMap() const { return type_ == FBT_MAP; }
+  bool IsBlob() const { return type_ == FBT_BLOB; }
+  bool AsBool() const {
+    return (type_ == FBT_BOOL ? ReadUInt64(data_, parent_width_)
+                              : AsUInt64()) != 0;
+  }
+
+  // Reads any type as a int64_t. Never fails, does most sensible conversion.
+  // Truncates floats, strings are attempted to be parsed for a number,
+  // vectors/maps return their size. Returns 0 if all else fails.
+  int64_t AsInt64() const {
+    if (type_ == FBT_INT) {
+      // A fast path for the common case.
+      return ReadInt64(data_, parent_width_);
+    } else
+      switch (type_) {
+        case FBT_INDIRECT_INT: return ReadInt64(Indirect(), byte_width_);
+        case FBT_UINT: return ReadUInt64(data_, parent_width_);
+        case FBT_INDIRECT_UINT: return ReadUInt64(Indirect(), byte_width_);
+        case FBT_FLOAT:
+          return static_cast<int64_t>(ReadDouble(data_, parent_width_));
+        case FBT_INDIRECT_FLOAT:
+          return static_cast<int64_t>(ReadDouble(Indirect(), byte_width_));
+        case FBT_NULL: return 0;
+        case FBT_STRING: return flatbuffers::StringToInt(AsString().c_str());
+        case FBT_VECTOR: return static_cast<int64_t>(AsVector().size());
+        case FBT_BOOL: return ReadInt64(data_, parent_width_);
+        default:
+          // Convert other things to int.
+          return 0;
+      }
+  }
+
+  // TODO: could specialize these to not use AsInt64() if that saves
+  // extension ops in generated code, and use a faster op than ReadInt64.
+  int32_t AsInt32() const { return static_cast<int32_t>(AsInt64()); }
+  int16_t AsInt16() const { return static_cast<int16_t>(AsInt64()); }
+  int8_t AsInt8() const { return static_cast<int8_t>(AsInt64()); }
+
+  uint64_t AsUInt64() const {
+    if (type_ == FBT_UINT) {
+      // A fast path for the common case.
+      return ReadUInt64(data_, parent_width_);
+    } else
+      switch (type_) {
+        case FBT_INDIRECT_UINT: return ReadUInt64(Indirect(), byte_width_);
+        case FBT_INT: return ReadInt64(data_, parent_width_);
+        case FBT_INDIRECT_INT: return ReadInt64(Indirect(), byte_width_);
+        case FBT_FLOAT:
+          return static_cast<uint64_t>(ReadDouble(data_, parent_width_));
+        case FBT_INDIRECT_FLOAT:
+          return static_cast<uint64_t>(ReadDouble(Indirect(), byte_width_));
+        case FBT_NULL: return 0;
+        case FBT_STRING: return flatbuffers::StringToUInt(AsString().c_str());
+        case FBT_VECTOR: return static_cast<uint64_t>(AsVector().size());
+        case FBT_BOOL: return ReadUInt64(data_, parent_width_);
+        default:
+          // Convert other things to uint.
+          return 0;
+      }
+  }
+
+  uint32_t AsUInt32() const { return static_cast<uint32_t>(AsUInt64()); }
+  uint16_t AsUInt16() const { return static_cast<uint16_t>(AsUInt64()); }
+  uint8_t AsUInt8() const { return static_cast<uint8_t>(AsUInt64()); }
+
+  double AsDouble() const {
+    if (type_ == FBT_FLOAT) {
+      // A fast path for the common case.
+      return ReadDouble(data_, parent_width_);
+    } else
+      switch (type_) {
+        case FBT_INDIRECT_FLOAT: return ReadDouble(Indirect(), byte_width_);
+        case FBT_INT:
+          return static_cast<double>(ReadInt64(data_, parent_width_));
+        case FBT_UINT:
+          return static_cast<double>(ReadUInt64(data_, parent_width_));
+        case FBT_INDIRECT_INT:
+          return static_cast<double>(ReadInt64(Indirect(), byte_width_));
+        case FBT_INDIRECT_UINT:
+          return static_cast<double>(ReadUInt64(Indirect(), byte_width_));
+        case FBT_NULL: return 0.0;
+        case FBT_STRING: {
+#if 1
+#if !defined( _MSC_VER)
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wnull-dereference"
+#endif
+          // See b/173239141 for additional context. Patched via
+          // micro/tools/make/flexbuffers_download.sh
+          // Introduce a segfault for an unsupported code path for TFLM.
+          return *(static_cast<double*>(nullptr));
+#if !defined( _MSC_VER)
+#pragma GCC diagnostic pop
+#endif
+#else
+          // This is the original code
+          double d;
+          flatbuffers::StringToNumber(AsString().c_str(), &d);
+          return d;
+#endif
+        }
+        case FBT_VECTOR: return static_cast<double>(AsVector().size());
+        case FBT_BOOL:
+          return static_cast<double>(ReadUInt64(data_, parent_width_));
+        default:
+          // Convert strings and other things to float.
+          return 0;
+      }
+  }
+
+  float AsFloat() const { return static_cast<float>(AsDouble()); }
+
+  const char *AsKey() const {
+    if (type_ == FBT_KEY || type_ == FBT_STRING) {
+      return reinterpret_cast<const char *>(Indirect());
+    } else {
+      return "";
+    }
+  }
+
+  // This function returns the empty string if you try to read something that
+  // is not a string or key.
+  String AsString() const {
+    if (type_ == FBT_STRING) {
+      return String(Indirect(), byte_width_);
+    } else if (type_ == FBT_KEY) {
+      auto key = Indirect();
+      return String(key, byte_width_,
+                    strlen(reinterpret_cast<const char *>(key)));
+    } else {
+      return String::EmptyString();
+    }
+  }
+
+  // Unlike AsString(), this will convert any type to a std::string.
+  std::string ToString() const {
+    std::string s;
+    ToString(false, false, s);
+    return s;
+  }
+
+  // Convert any type to a JSON-like string. strings_quoted determines if
+  // string values at the top level receive "" quotes (inside other values
+  // they always do). keys_quoted determines if keys are quoted, at any level.
+  // TODO(wvo): add further options to have indentation/newlines.
+  void ToString(bool strings_quoted, bool keys_quoted, std::string &s) const {
+    if (type_ == FBT_STRING) {
+      String str(Indirect(), byte_width_);
+      if (strings_quoted) {
+        flatbuffers::EscapeString(str.c_str(), str.length(), &s, true, false);
+      } else {
+        s.append(str.c_str(), str.length());
+      }
+    } else if (IsKey()) {
+      auto str = AsKey();
+      if (keys_quoted) {
+        flatbuffers::EscapeString(str, strlen(str), &s, true, false);
+      } else {
+        s += str;
+      }
+    } else if (IsInt()) {
+      s += flatbuffers::NumToString(AsInt64());
+    } else if (IsUInt()) {
+      s += flatbuffers::NumToString(AsUInt64());
+    } else if (IsFloat()) {
+      s += flatbuffers::NumToString(AsDouble());
+    } else if (IsNull()) {
+      s += "null";
+    } else if (IsBool()) {
+      s += AsBool() ? "true" : "false";
+    } else if (IsMap()) {
+      s += "{ ";
+      auto m = AsMap();
+      auto keys = m.Keys();
+      auto vals = m.Values();
+      for (size_t i = 0; i < keys.size(); i++) {
+        bool kq = keys_quoted;
+        if (!kq) {
+          // FlexBuffers keys may contain arbitrary characters, only allow
+          // unquoted if it looks like an "identifier":
+          const char *p = keys[i].AsKey();
+          if (!flatbuffers::is_alpha(*p) && *p != '_') {
+            kq = true;
+          } else {
+            while (*++p) {
+              if (!flatbuffers::is_alnum(*p) && *p != '_') {
+                kq = true;
+                break;
+              }
+            }
+          }
+        }
+        keys[i].ToString(true, kq, s);
+        s += ": ";
+        vals[i].ToString(true, keys_quoted, s);
+        if (i < keys.size() - 1) s += ", ";
+      }
+      s += " }";
+    } else if (IsVector()) {
+      AppendToString<Vector>(s, AsVector(), keys_quoted);
+    } else if (IsTypedVector()) {
+      AppendToString<TypedVector>(s, AsTypedVector(), keys_quoted);
+    } else if (IsFixedTypedVector()) {
+      AppendToString<FixedTypedVector>(s, AsFixedTypedVector(), keys_quoted);
+    } else if (IsBlob()) {
+      auto blob = AsBlob();
+      flatbuffers::EscapeString(reinterpret_cast<const char *>(blob.data()),
+                                blob.size(), &s, true, false);
+    } else {
+      s += "(?)";
+    }
+  }
+
+  // This function returns the empty blob if you try to read a not-blob.
+  // Strings can be viewed as blobs too.
+  Blob AsBlob() const {
+    if (type_ == FBT_BLOB || type_ == FBT_STRING) {
+      return Blob(Indirect(), byte_width_);
+    } else {
+      return Blob::EmptyBlob();
+    }
+  }
+
+  // This function returns the empty vector if you try to read a not-vector.
+  // Maps can be viewed as vectors too.
+  Vector AsVector() const {
+    if (type_ == FBT_VECTOR || type_ == FBT_MAP) {
+      return Vector(Indirect(), byte_width_);
+    } else {
+      return Vector::EmptyVector();
+    }
+  }
+
+  TypedVector AsTypedVector() const {
+    if (IsTypedVector()) {
+      auto tv =
+          TypedVector(Indirect(), byte_width_, ToTypedVectorElementType(type_));
+      if (tv.type_ == FBT_STRING) {
+        // These can't be accessed as strings, since we don't know the bit-width
+        // of the size field, see the declaration of
+        // FBT_VECTOR_STRING_DEPRECATED above for details.
+        // We change the type here to be keys, which are a subtype of strings,
+        // and will ignore the size field. This will truncate strings with
+        // embedded nulls.
+        tv.type_ = FBT_KEY;
+      }
+      return tv;
+    } else {
+      return TypedVector::EmptyTypedVector();
+    }
+  }
+
+  FixedTypedVector AsFixedTypedVector() const {
+    if (IsFixedTypedVector()) {
+      uint8_t len = 0;
+      auto vtype = ToFixedTypedVectorElementType(type_, &len);
+      return FixedTypedVector(Indirect(), byte_width_, vtype, len);
+    } else {
+      return FixedTypedVector::EmptyFixedTypedVector();
+    }
+  }
+
+  Map AsMap() const {
+    if (type_ == FBT_MAP) {
+      return Map(Indirect(), byte_width_);
+    } else {
+      return Map::EmptyMap();
+    }
+  }
+
+  template<typename T> T As() const;
+
+  // Experimental: Mutation functions.
+  // These allow scalars in an already created buffer to be updated in-place.
+  // Since by default scalars are stored in the smallest possible space,
+  // the new value may not fit, in which case these functions return false.
+  // To avoid this, you can construct the values you intend to mutate using
+  // Builder::ForceMinimumBitWidth.
+  bool MutateInt(int64_t i) {
+    if (type_ == FBT_INT) {
+      return Mutate(data_, i, parent_width_, WidthI(i));
+    } else if (type_ == FBT_INDIRECT_INT) {
+      return Mutate(Indirect(), i, byte_width_, WidthI(i));
+    } else if (type_ == FBT_UINT) {
+      auto u = static_cast<uint64_t>(i);
+      return Mutate(data_, u, parent_width_, WidthU(u));
+    } else if (type_ == FBT_INDIRECT_UINT) {
+      auto u = static_cast<uint64_t>(i);
+      return Mutate(Indirect(), u, byte_width_, WidthU(u));
+    } else {
+      return false;
+    }
+  }
+
+  bool MutateBool(bool b) {
+    return type_ == FBT_BOOL && Mutate(data_, b, parent_width_, BIT_WIDTH_8);
+  }
+
+  bool MutateUInt(uint64_t u) {
+    if (type_ == FBT_UINT) {
+      return Mutate(data_, u, parent_width_, WidthU(u));
+    } else if (type_ == FBT_INDIRECT_UINT) {
+      return Mutate(Indirect(), u, byte_width_, WidthU(u));
+    } else if (type_ == FBT_INT) {
+      auto i = static_cast<int64_t>(u);
+      return Mutate(data_, i, parent_width_, WidthI(i));
+    } else if (type_ == FBT_INDIRECT_INT) {
+      auto i = static_cast<int64_t>(u);
+      return Mutate(Indirect(), i, byte_width_, WidthI(i));
+    } else {
+      return false;
+    }
+  }
+
+  bool MutateFloat(float f) {
+    if (type_ == FBT_FLOAT) {
+      return MutateF(data_, f, parent_width_, BIT_WIDTH_32);
+    } else if (type_ == FBT_INDIRECT_FLOAT) {
+      return MutateF(Indirect(), f, byte_width_, BIT_WIDTH_32);
+    } else {
+      return false;
+    }
+  }
+
+  bool MutateFloat(double d) {
+    if (type_ == FBT_FLOAT) {
+      return MutateF(data_, d, parent_width_, WidthF(d));
+    } else if (type_ == FBT_INDIRECT_FLOAT) {
+      return MutateF(Indirect(), d, byte_width_, WidthF(d));
+    } else {
+      return false;
+    }
+  }
+
+  bool MutateString(const char *str, size_t len) {
+    auto s = AsString();
+    if (s.IsTheEmptyString()) return false;
+    // This is very strict, could allow shorter strings, but that creates
+    // garbage.
+    if (s.length() != len) return false;
+    memcpy(const_cast<char *>(s.c_str()), str, len);
+    return true;
+  }
+  bool MutateString(const char *str) { return MutateString(str, strlen(str)); }
+  bool MutateString(const std::string &str) {
+    return MutateString(str.data(), str.length());
+  }
+
+ private:
+  const uint8_t *Indirect() const {
+    return flexbuffers::Indirect(data_, parent_width_);
+  }
+
+  template<typename T>
+  bool Mutate(const uint8_t *dest, T t, size_t byte_width,
+              BitWidth value_width) {
+    auto fits = static_cast<size_t>(static_cast<size_t>(1U) << value_width) <=
+                byte_width;
+    if (fits) {
+      t = flatbuffers::EndianScalar(t);
+      memcpy(const_cast<uint8_t *>(dest), &t, byte_width);
+    }
+    return fits;
+  }
+
+  template<typename T>
+  bool MutateF(const uint8_t *dest, T t, size_t byte_width,
+               BitWidth value_width) {
+    if (byte_width == sizeof(double))
+      return Mutate(dest, static_cast<double>(t), byte_width, value_width);
+    if (byte_width == sizeof(float))
+      return Mutate(dest, static_cast<float>(t), byte_width, value_width);
+    FLATBUFFERS_ASSERT(false);
+    return false;
+  }
+
+  friend class Verifier;
+
+  const uint8_t *data_;
+  uint8_t parent_width_;
+  uint8_t byte_width_;
+  Type type_;
+};
+
+// Template specialization for As().
+template<> inline bool Reference::As<bool>() const { return AsBool(); }
+
+template<> inline int8_t Reference::As<int8_t>() const { return AsInt8(); }
+template<> inline int16_t Reference::As<int16_t>() const { return AsInt16(); }
+template<> inline int32_t Reference::As<int32_t>() const { return AsInt32(); }
+template<> inline int64_t Reference::As<int64_t>() const { return AsInt64(); }
+
+template<> inline uint8_t Reference::As<uint8_t>() const { return AsUInt8(); }
+template<> inline uint16_t Reference::As<uint16_t>() const {
+  return AsUInt16();
+}
+template<> inline uint32_t Reference::As<uint32_t>() const {
+  return AsUInt32();
+}
+template<> inline uint64_t Reference::As<uint64_t>() const {
+  return AsUInt64();
+}
+
+template<> inline double Reference::As<double>() const { return AsDouble(); }
+template<> inline float Reference::As<float>() const { return AsFloat(); }
+
+template<> inline String Reference::As<String>() const { return AsString(); }
+template<> inline std::string Reference::As<std::string>() const {
+  return AsString().str();
+}
+
+template<> inline Blob Reference::As<Blob>() const { return AsBlob(); }
+template<> inline Vector Reference::As<Vector>() const { return AsVector(); }
+template<> inline TypedVector Reference::As<TypedVector>() const {
+  return AsTypedVector();
+}
+template<> inline FixedTypedVector Reference::As<FixedTypedVector>() const {
+  return AsFixedTypedVector();
+}
+template<> inline Map Reference::As<Map>() const { return AsMap(); }
+
+inline uint8_t PackedType(BitWidth bit_width, Type type) {
+  return static_cast<uint8_t>(bit_width | (type << 2));
+}
+
+inline uint8_t NullPackedType() { return PackedType(BIT_WIDTH_8, FBT_NULL); }
+
+// Vector accessors.
+// Note: if you try to access outside of bounds, you get a Null value back
+// instead. Normally this would be an assert, but since this is "dynamically
+// typed" data, you may not want that (someone sends you a 2d vector and you
+// wanted 3d).
+// The Null converts seamlessly into a default value for any other type.
+// TODO(wvo): Could introduce an #ifdef that makes this into an assert?
+inline Reference Vector::operator[](size_t i) const {
+  auto len = size();
+  if (i >= len) return Reference(nullptr, 1, NullPackedType());
+  auto packed_type = (data_ + len * byte_width_)[i];
+  auto elem = data_ + i * byte_width_;
+  return Reference(elem, byte_width_, packed_type);
+}
+
+inline Reference TypedVector::operator[](size_t i) const {
+  auto len = size();
+  if (i >= len) return Reference(nullptr, 1, NullPackedType());
+  auto elem = data_ + i * byte_width_;
+  return Reference(elem, byte_width_, 1, type_);
+}
+
+inline Reference FixedTypedVector::operator[](size_t i) const {
+  if (i >= len_) return Reference(nullptr, 1, NullPackedType());
+  auto elem = data_ + i * byte_width_;
+  return Reference(elem, byte_width_, 1, type_);
+}
+
+template<typename T> int KeyCompare(const void *key, const void *elem) {
+  auto str_elem = reinterpret_cast<const char *>(
+      Indirect<T>(reinterpret_cast<const uint8_t *>(elem)));
+  auto skey = reinterpret_cast<const char *>(key);
+  return strcmp(skey, str_elem);
+}
+
+inline Reference Map::operator[](const char *key) const {
+  auto keys = Keys();
+  // We can't pass keys.byte_width_ to the comparison function, so we have
+  // to pick the right one ahead of time.
+  int (*comp)(const void *, const void *) = nullptr;
+  switch (keys.byte_width_) {
+    case 1: comp = KeyCompare<uint8_t>; break;
+    case 2: comp = KeyCompare<uint16_t>; break;
+    case 4: comp = KeyCompare<uint32_t>; break;
+    case 8: comp = KeyCompare<uint64_t>; break;
+    default: FLATBUFFERS_ASSERT(false); return Reference();
+  }
+  auto res = std::bsearch(key, keys.data_, keys.size(), keys.byte_width_, comp);
+  if (!res) return Reference(nullptr, 1, NullPackedType());
+  auto i = (reinterpret_cast<uint8_t *>(res) - keys.data_) / keys.byte_width_;
+  return (*static_cast<const Vector *>(this))[i];
+}
+
+inline Reference Map::operator[](const std::string &key) const {
+  return (*this)[key.c_str()];
+}
+
+inline Reference GetRoot(const uint8_t *buffer, size_t size) {
+  // See Finish() below for the serialization counterpart of this.
+  // The root starts at the end of the buffer, so we parse backwards from there.
+  auto end = buffer + size;
+  auto byte_width = *--end;
+  auto packed_type = *--end;
+  end -= byte_width;  // The root data item.
+  return Reference(end, byte_width, packed_type);
+}
+
+inline Reference GetRoot(const std::vector<uint8_t> &buffer) {
+  return GetRoot(buffer.data(), buffer.size());
+}
+
+// Flags that configure how the Builder behaves.
+// The "Share" flags determine if the Builder automatically tries to pool
+// this type. Pooling can reduce the size of serialized data if there are
+// multiple maps of the same kind, at the expense of slightly slower
+// serialization (the cost of lookups) and more memory use (std::set).
+// By default this is on for keys, but off for strings.
+// Turn keys off if you have e.g. only one map.
+// Turn strings on if you expect many non-unique string values.
+// Additionally, sharing key vectors can save space if you have maps with
+// identical field populations.
+enum BuilderFlag {
+  BUILDER_FLAG_NONE = 0,
+  BUILDER_FLAG_SHARE_KEYS = 1,
+  BUILDER_FLAG_SHARE_STRINGS = 2,
+  BUILDER_FLAG_SHARE_KEYS_AND_STRINGS = 3,
+  BUILDER_FLAG_SHARE_KEY_VECTORS = 4,
+  BUILDER_FLAG_SHARE_ALL = 7,
+};
+
+class Builder FLATBUFFERS_FINAL_CLASS {
+ public:
+  Builder(size_t initial_size = 256,
+          BuilderFlag flags = BUILDER_FLAG_SHARE_KEYS)
+      : buf_(initial_size),
+        finished_(false),
+        has_duplicate_keys_(false),
+        flags_(flags),
+        force_min_bit_width_(BIT_WIDTH_8),
+        key_pool(KeyOffsetCompare(buf_)),
+        string_pool(StringOffsetCompare(buf_)) {
+    buf_.clear();
+  }
+
+#ifdef FLATBUFFERS_DEFAULT_DECLARATION
+  Builder(Builder &&) = default;
+  Builder &operator=(Builder &&) = default;
+#endif
+
+  /// @brief Get the serialized buffer (after you call `Finish()`).
+  /// @return Returns a vector owned by this class.
+  const std::vector<uint8_t> &GetBuffer() const {
+    Finished();
+    return buf_;
+  }
+
+  // Size of the buffer. Does not include unfinished values.
+  size_t GetSize() const { return buf_.size(); }
+
+  // Reset all state so we can re-use the buffer.
+  void Clear() {
+    buf_.clear();
+    stack_.clear();
+    finished_ = false;
+    // flags_ remains as-is;
+    force_min_bit_width_ = BIT_WIDTH_8;
+    key_pool.clear();
+    string_pool.clear();
+  }
+
+  // All value constructing functions below have two versions: one that
+  // takes a key (for placement inside a map) and one that doesn't (for inside
+  // vectors and elsewhere).
+
+  void Null() { stack_.push_back(Value()); }
+  void Null(const char *key) {
+    Key(key);
+    Null();
+  }
+
+  void Int(int64_t i) { stack_.push_back(Value(i, FBT_INT, WidthI(i))); }
+  void Int(const char *key, int64_t i) {
+    Key(key);
+    Int(i);
+  }
+
+  void UInt(uint64_t u) { stack_.push_back(Value(u, FBT_UINT, WidthU(u))); }
+  void UInt(const char *key, uint64_t u) {
+    Key(key);
+    UInt(u);
+  }
+
+  void Float(float f) { stack_.push_back(Value(f)); }
+  void Float(const char *key, float f) {
+    Key(key);
+    Float(f);
+  }
+
+  void Double(double f) { stack_.push_back(Value(f)); }
+  void Double(const char *key, double d) {
+    Key(key);
+    Double(d);
+  }
+
+  void Bool(bool b) { stack_.push_back(Value(b)); }
+  void Bool(const char *key, bool b) {
+    Key(key);
+    Bool(b);
+  }
+
+  void IndirectInt(int64_t i) { PushIndirect(i, FBT_INDIRECT_INT, WidthI(i)); }
+  void IndirectInt(const char *key, int64_t i) {
+    Key(key);
+    IndirectInt(i);
+  }
+
+  void IndirectUInt(uint64_t u) {
+    PushIndirect(u, FBT_INDIRECT_UINT, WidthU(u));
+  }
+  void IndirectUInt(const char *key, uint64_t u) {
+    Key(key);
+    IndirectUInt(u);
+  }
+
+  void IndirectFloat(float f) {
+    PushIndirect(f, FBT_INDIRECT_FLOAT, BIT_WIDTH_32);
+  }
+  void IndirectFloat(const char *key, float f) {
+    Key(key);
+    IndirectFloat(f);
+  }
+
+  void IndirectDouble(double f) {
+    PushIndirect(f, FBT_INDIRECT_FLOAT, WidthF(f));
+  }
+  void IndirectDouble(const char *key, double d) {
+    Key(key);
+    IndirectDouble(d);
+  }
+
+  size_t Key(const char *str, size_t len) {
+    auto sloc = buf_.size();
+    WriteBytes(str, len + 1);
+    if (flags_ & BUILDER_FLAG_SHARE_KEYS) {
+      auto it = key_pool.find(sloc);
+      if (it != key_pool.end()) {
+        // Already in the buffer. Remove key we just serialized, and use
+        // existing offset instead.
+        buf_.resize(sloc);
+        sloc = *it;
+      } else {
+        key_pool.insert(sloc);
+      }
+    }
+    stack_.push_back(Value(static_cast<uint64_t>(sloc), FBT_KEY, BIT_WIDTH_8));
+    return sloc;
+  }
+
+  size_t Key(const char *str) { return Key(str, strlen(str)); }
+  size_t Key(const std::string &str) { return Key(str.c_str(), str.size()); }
+
+  size_t String(const char *str, size_t len) {
+    auto reset_to = buf_.size();
+    auto sloc = CreateBlob(str, len, 1, FBT_STRING);
+    if (flags_ & BUILDER_FLAG_SHARE_STRINGS) {
+      StringOffset so(sloc, len);
+      auto it = string_pool.find(so);
+      if (it != string_pool.end()) {
+        // Already in the buffer. Remove string we just serialized, and use
+        // existing offset instead.
+        buf_.resize(reset_to);
+        sloc = it->first;
+        stack_.back().u_ = sloc;
+      } else {
+        string_pool.insert(so);
+      }
+    }
+    return sloc;
+  }
+  size_t String(const char *str) { return String(str, strlen(str)); }
+  size_t String(const std::string &str) {
+    return String(str.c_str(), str.size());
+  }
+  void String(const flexbuffers::String &str) {
+    String(str.c_str(), str.length());
+  }
+
+  void String(const char *key, const char *str) {
+    Key(key);
+    String(str);
+  }
+  void String(const char *key, const std::string &str) {
+    Key(key);
+    String(str);
+  }
+  void String(const char *key, const flexbuffers::String &str) {
+    Key(key);
+    String(str);
+  }
+
+  size_t Blob(const void *data, size_t len) {
+    return CreateBlob(data, len, 0, FBT_BLOB);
+  }
+  size_t Blob(const std::vector<uint8_t> &v) {
+    return CreateBlob(v.data(), v.size(), 0, FBT_BLOB);
+  }
+
+  void Blob(const char *key, const void *data, size_t len) {
+    Key(key);
+    Blob(data, len);
+  }
+  void Blob(const char *key, const std::vector<uint8_t> &v) {
+    Key(key);
+    Blob(v);
+  }
+
+  // TODO(wvo): support all the FlexBuffer types (like flexbuffers::String),
+  // e.g. Vector etc. Also in overloaded versions.
+  // Also some FlatBuffers types?
+
+  size_t StartVector() { return stack_.size(); }
+  size_t StartVector(const char *key) {
+    Key(key);
+    return stack_.size();
+  }
+  size_t StartMap() { return stack_.size(); }
+  size_t StartMap(const char *key) {
+    Key(key);
+    return stack_.size();
+  }
+
+  // TODO(wvo): allow this to specify an alignment greater than the natural
+  // alignment.
+  size_t EndVector(size_t start, bool typed, bool fixed) {
+    auto vec = CreateVector(start, stack_.size() - start, 1, typed, fixed);
+    // Remove temp elements and return vector.
+    stack_.resize(start);
+    stack_.push_back(vec);
+    return static_cast<size_t>(vec.u_);
+  }
+
+  size_t EndMap(size_t start) {
+    // We should have interleaved keys and values on the stack.
+    // Make sure it is an even number:
+    auto len = stack_.size() - start;
+    FLATBUFFERS_ASSERT(!(len & 1));
+    len /= 2;
+    // Make sure keys are all strings:
+    for (auto key = start; key < stack_.size(); key += 2) {
+      FLATBUFFERS_ASSERT(stack_[key].type_ == FBT_KEY);
+    }
+    // Now sort values, so later we can do a binary search lookup.
+    // We want to sort 2 array elements at a time.
+    struct TwoValue {
+      Value key;
+      Value val;
+    };
+    // TODO(wvo): strict aliasing?
+    // TODO(wvo): allow the caller to indicate the data is already sorted
+    // for maximum efficiency? With an assert to check sortedness to make sure
+    // we're not breaking binary search.
+    // Or, we can track if the map is sorted as keys are added which would be
+    // be quite cheap (cheaper than checking it here), so we can skip this
+    // step automatically when appliccable, and encourage people to write in
+    // sorted fashion.
+    // std::sort is typically already a lot faster on sorted data though.
+    auto dict = reinterpret_cast<TwoValue *>(stack_.data() + start);
+    std::sort(
+        dict, dict + len, [&](const TwoValue &a, const TwoValue &b) -> bool {
+          auto as = reinterpret_cast<const char *>(buf_.data() + a.key.u_);
+          auto bs = reinterpret_cast<const char *>(buf_.data() + b.key.u_);
+          auto comp = strcmp(as, bs);
+          // We want to disallow duplicate keys, since this results in a
+          // map where values cannot be found.
+          // But we can't assert here (since we don't want to fail on
+          // random JSON input) or have an error mechanism.
+          // Instead, we set has_duplicate_keys_ in the builder to
+          // signal this.
+          // TODO: Have to check for pointer equality, as some sort
+          // implementation apparently call this function with the same
+          // element?? Why?
+          if (!comp && &a != &b) has_duplicate_keys_ = true;
+          return comp < 0;
+        });
+    // First create a vector out of all keys.
+    // TODO(wvo): if kBuilderFlagShareKeyVectors is true, see if we can share
+    // the first vector.
+    auto keys = CreateVector(start, len, 2, true, false);
+    auto vec = CreateVector(start + 1, len, 2, false, false, &keys);
+    // Remove temp elements and return map.
+    stack_.resize(start);
+    stack_.push_back(vec);
+    return static_cast<size_t>(vec.u_);
+  }
+
+  // Call this after EndMap to see if the map had any duplicate keys.
+  // Any map with such keys won't be able to retrieve all values.
+  bool HasDuplicateKeys() const { return has_duplicate_keys_; }
+
+  template<typename F> size_t Vector(F f) {
+    auto start = StartVector();
+    f();
+    return EndVector(start, false, false);
+  }
+  template<typename F, typename T> size_t Vector(F f, T &state) {
+    auto start = StartVector();
+    f(state);
+    return EndVector(start, false, false);
+  }
+  template<typename F> size_t Vector(const char *key, F f) {
+    auto start = StartVector(key);
+    f();
+    return EndVector(start, false, false);
+  }
+  template<typename F, typename T>
+  size_t Vector(const char *key, F f, T &state) {
+    auto start = StartVector(key);
+    f(state);
+    return EndVector(start, false, false);
+  }
+
+  template<typename T> void Vector(const T *elems, size_t len) {
+    if (flatbuffers::is_scalar<T>::value) {
+      // This path should be a lot quicker and use less space.
+      ScalarVector(elems, len, false);
+    } else {
+      auto start = StartVector();
+      for (size_t i = 0; i < len; i++) Add(elems[i]);
+      EndVector(start, false, false);
+    }
+  }
+  template<typename T>
+  void Vector(const char *key, const T *elems, size_t len) {
+    Key(key);
+    Vector(elems, len);
+  }
+  template<typename T> void Vector(const std::vector<T> &vec) {
+    Vector(vec.data(), vec.size());
+  }
+
+  template<typename F> size_t TypedVector(F f) {
+    auto start = StartVector();
+    f();
+    return EndVector(start, true, false);
+  }
+  template<typename F, typename T> size_t TypedVector(F f, T &state) {
+    auto start = StartVector();
+    f(state);
+    return EndVector(start, true, false);
+  }
+  template<typename F> size_t TypedVector(const char *key, F f) {
+    auto start = StartVector(key);
+    f();
+    return EndVector(start, true, false);
+  }
+  template<typename F, typename T>
+  size_t TypedVector(const char *key, F f, T &state) {
+    auto start = StartVector(key);
+    f(state);
+    return EndVector(start, true, false);
+  }
+
+  template<typename T> size_t FixedTypedVector(const T *elems, size_t len) {
+    // We only support a few fixed vector lengths. Anything bigger use a
+    // regular typed vector.
+    FLATBUFFERS_ASSERT(len >= 2 && len <= 4);
+    // And only scalar values.
+    static_assert(flatbuffers::is_scalar<T>::value, "Unrelated types");
+    return ScalarVector(elems, len, true);
+  }
+
+  template<typename T>
+  size_t FixedTypedVector(const char *key, const T *elems, size_t len) {
+    Key(key);
+    return FixedTypedVector(elems, len);
+  }
+
+  template<typename F> size_t Map(F f) {
+    auto start = StartMap();
+    f();
+    return EndMap(start);
+  }
+  template<typename F, typename T> size_t Map(F f, T &state) {
+    auto start = StartMap();
+    f(state);
+    return EndMap(start);
+  }
+  template<typename F> size_t Map(const char *key, F f) {
+    auto start = StartMap(key);
+    f();
+    return EndMap(start);
+  }
+  template<typename F, typename T> size_t Map(const char *key, F f, T &state) {
+    auto start = StartMap(key);
+    f(state);
+    return EndMap(start);
+  }
+  template<typename T> void Map(const std::map<std::string, T> &map) {
+    auto start = StartMap();
+    for (auto it = map.begin(); it != map.end(); ++it)
+      Add(it->first.c_str(), it->second);
+    EndMap(start);
+  }
+
+  // If you wish to share a value explicitly (a value not shared automatically
+  // through one of the BUILDER_FLAG_SHARE_* flags) you can do so with these
+  // functions. Or if you wish to turn those flags off for performance reasons
+  // and still do some explicit sharing. For example:
+  // builder.IndirectDouble(M_PI);
+  // auto id = builder.LastValue();  // Remember where we stored it.
+  // .. more code goes here ..
+  // builder.ReuseValue(id);  // Refers to same double by offset.
+  // LastValue works regardless of whether the value has a key or not.
+  // Works on any data type.
+  struct Value;
+  Value LastValue() { return stack_.back(); }
+  void ReuseValue(Value v) { stack_.push_back(v); }
+  void ReuseValue(const char *key, Value v) {
+    Key(key);
+    ReuseValue(v);
+  }
+
+  // Overloaded Add that tries to call the correct function above.
+  void Add(int8_t i) { Int(i); }
+  void Add(int16_t i) { Int(i); }
+  void Add(int32_t i) { Int(i); }
+  void Add(int64_t i) { Int(i); }
+  void Add(uint8_t u) { UInt(u); }
+  void Add(uint16_t u) { UInt(u); }
+  void Add(uint32_t u) { UInt(u); }
+  void Add(uint64_t u) { UInt(u); }
+  void Add(float f) { Float(f); }
+  void Add(double d) { Double(d); }
+  void Add(bool b) { Bool(b); }
+  void Add(const char *str) { String(str); }
+  void Add(const std::string &str) { String(str); }
+  void Add(const flexbuffers::String &str) { String(str); }
+
+  template<typename T> void Add(const std::vector<T> &vec) { Vector(vec); }
+
+  template<typename T> void Add(const char *key, const T &t) {
+    Key(key);
+    Add(t);
+  }
+
+  template<typename T> void Add(const std::map<std::string, T> &map) {
+    Map(map);
+  }
+
+  template<typename T> void operator+=(const T &t) { Add(t); }
+
+  // This function is useful in combination with the Mutate* functions above.
+  // It forces elements of vectors and maps to have a minimum size, such that
+  // they can later be updated without failing.
+  // Call with no arguments to reset.
+  void ForceMinimumBitWidth(BitWidth bw = BIT_WIDTH_8) {
+    force_min_bit_width_ = bw;
+  }
+
+  void Finish() {
+    // If you hit this assert, you likely have objects that were never included
+    // in a parent. You need to have exactly one root to finish a buffer.
+    // Check your Start/End calls are matched, and all objects are inside
+    // some other object.
+    FLATBUFFERS_ASSERT(stack_.size() == 1);
+
+    // Write root value.
+    auto byte_width = Align(stack_[0].ElemWidth(buf_.size(), 0));
+    WriteAny(stack_[0], byte_width);
+    // Write root type.
+    Write(stack_[0].StoredPackedType(), 1);
+    // Write root size. Normally determined by parent, but root has no parent :)
+    Write(byte_width, 1);
+
+    finished_ = true;
+  }
+
+ private:
+  void Finished() const {
+    // If you get this assert, you're attempting to get access a buffer
+    // which hasn't been finished yet. Be sure to call
+    // Builder::Finish with your root object.
+    FLATBUFFERS_ASSERT(finished_);
+  }
+
+  // Align to prepare for writing a scalar with a certain size.
+  uint8_t Align(BitWidth alignment) {
+    auto byte_width = 1U << alignment;
+    buf_.insert(buf_.end(), flatbuffers::PaddingBytes(buf_.size(), byte_width),
+                0);
+    return static_cast<uint8_t>(byte_width);
+  }
+
+  void WriteBytes(const void *val, size_t size) {
+    buf_.insert(buf_.end(), reinterpret_cast<const uint8_t *>(val),
+                reinterpret_cast<const uint8_t *>(val) + size);
+  }
+
+  template<typename T> void Write(T val, size_t byte_width) {
+    FLATBUFFERS_ASSERT(sizeof(T) >= byte_width);
+    val = flatbuffers::EndianScalar(val);
+    WriteBytes(&val, byte_width);
+  }
+
+  void WriteDouble(double f, uint8_t byte_width) {
+    switch (byte_width) {
+      case 8: Write(f, byte_width); break;
+      case 4: Write(static_cast<float>(f), byte_width); break;
+      // case 2: Write(static_cast<half>(f), byte_width); break;
+      // case 1: Write(static_cast<quarter>(f), byte_width); break;
+      default: FLATBUFFERS_ASSERT(0);
+    }
+  }
+
+  void WriteOffset(uint64_t o, uint8_t byte_width) {
+    auto reloff = buf_.size() - o;
+    FLATBUFFERS_ASSERT(byte_width == 8 || reloff < 1ULL << (byte_width * 8));
+    Write(reloff, byte_width);
+  }
+
+  template<typename T> void PushIndirect(T val, Type type, BitWidth bit_width) {
+    auto byte_width = Align(bit_width);
+    auto iloc = buf_.size();
+    Write(val, byte_width);
+    stack_.push_back(Value(static_cast<uint64_t>(iloc), type, bit_width));
+  }
+
+  static BitWidth WidthB(size_t byte_width) {
+    switch (byte_width) {
+      case 1: return BIT_WIDTH_8;
+      case 2: return BIT_WIDTH_16;
+      case 4: return BIT_WIDTH_32;
+      case 8: return BIT_WIDTH_64;
+      default: FLATBUFFERS_ASSERT(false); return BIT_WIDTH_64;
+    }
+  }
+
+  template<typename T> static Type GetScalarType() {
+    static_assert(flatbuffers::is_scalar<T>::value, "Unrelated types");
+    return flatbuffers::is_floating_point<T>::value
+               ? FBT_FLOAT
+               : flatbuffers::is_same<T, bool>::value
+                     ? FBT_BOOL
+                     : (flatbuffers::is_unsigned<T>::value ? FBT_UINT
+                                                           : FBT_INT);
+  }
+
+ public:
+  // This was really intended to be private, except for LastValue/ReuseValue.
+  struct Value {
+    union {
+      int64_t i_;
+      uint64_t u_;
+      double f_;
+    };
+
+    Type type_;
+
+    // For scalars: of itself, for vector: of its elements, for string: length.
+    BitWidth min_bit_width_;
+
+    Value() : i_(0), type_(FBT_NULL), min_bit_width_(BIT_WIDTH_8) {}
+
+    Value(bool b)
+        : u_(static_cast<uint64_t>(b)),
+          type_(FBT_BOOL),
+          min_bit_width_(BIT_WIDTH_8) {}
+
+    Value(int64_t i, Type t, BitWidth bw)
+        : i_(i), type_(t), min_bit_width_(bw) {}
+    Value(uint64_t u, Type t, BitWidth bw)
+        : u_(u), type_(t), min_bit_width_(bw) {}
+
+    Value(float f)
+        : f_(static_cast<double>(f)),
+          type_(FBT_FLOAT),
+          min_bit_width_(BIT_WIDTH_32) {}
+    Value(double f) : f_(f), type_(FBT_FLOAT), min_bit_width_(WidthF(f)) {}
+
+    uint8_t StoredPackedType(BitWidth parent_bit_width_ = BIT_WIDTH_8) const {
+      return PackedType(StoredWidth(parent_bit_width_), type_);
+    }
+
+    BitWidth ElemWidth(size_t buf_size, size_t elem_index) const {
+      if (IsInline(type_)) {
+        return min_bit_width_;
+      } else {
+        // We have an absolute offset, but want to store a relative offset
+        // elem_index elements beyond the current buffer end. Since whether
+        // the relative offset fits in a certain byte_width depends on
+        // the size of the elements before it (and their alignment), we have
+        // to test for each size in turn.
+        for (size_t byte_width = 1;
+             byte_width <= sizeof(flatbuffers::largest_scalar_t);
+             byte_width *= 2) {
+          // Where are we going to write this offset?
+          auto offset_loc = buf_size +
+                            flatbuffers::PaddingBytes(buf_size, byte_width) +
+                            elem_index * byte_width;
+          // Compute relative offset.
+          auto offset = offset_loc - u_;
+          // Does it fit?
+          auto bit_width = WidthU(offset);
+          if (static_cast<size_t>(static_cast<size_t>(1U) << bit_width) ==
+              byte_width)
+            return bit_width;
+        }
+        FLATBUFFERS_ASSERT(false);  // Must match one of the sizes above.
+        return BIT_WIDTH_64;
+      }
+    }
+
+    BitWidth StoredWidth(BitWidth parent_bit_width_ = BIT_WIDTH_8) const {
+      if (IsInline(type_)) {
+        return (std::max)(min_bit_width_, parent_bit_width_);
+      } else {
+        return min_bit_width_;
+      }
+    }
+  };
+
+ private:
+  void WriteAny(const Value &val, uint8_t byte_width) {
+    switch (val.type_) {
+      case FBT_NULL:
+      case FBT_INT: Write(val.i_, byte_width); break;
+      case FBT_BOOL:
+      case FBT_UINT: Write(val.u_, byte_width); break;
+      case FBT_FLOAT: WriteDouble(val.f_, byte_width); break;
+      default: WriteOffset(val.u_, byte_width); break;
+    }
+  }
+
+  size_t CreateBlob(const void *data, size_t len, size_t trailing, Type type) {
+    auto bit_width = WidthU(len);
+    auto byte_width = Align(bit_width);
+    Write<uint64_t>(len, byte_width);
+    auto sloc = buf_.size();
+    WriteBytes(data, len + trailing);
+    stack_.push_back(Value(static_cast<uint64_t>(sloc), type, bit_width));
+    return sloc;
+  }
+
+  template<typename T>
+  size_t ScalarVector(const T *elems, size_t len, bool fixed) {
+    auto vector_type = GetScalarType<T>();
+    auto byte_width = sizeof(T);
+    auto bit_width = WidthB(byte_width);
+    // If you get this assert, you're trying to write a vector with a size
+    // field that is bigger than the scalars you're trying to write (e.g. a
+    // byte vector > 255 elements). For such types, write a "blob" instead.
+    // TODO: instead of asserting, could write vector with larger elements
+    // instead, though that would be wasteful.
+    FLATBUFFERS_ASSERT(WidthU(len) <= bit_width);
+    Align(bit_width);
+    if (!fixed) Write<uint64_t>(len, byte_width);
+    auto vloc = buf_.size();
+    for (size_t i = 0; i < len; i++) Write(elems[i], byte_width);
+    stack_.push_back(Value(static_cast<uint64_t>(vloc),
+                           ToTypedVector(vector_type, fixed ? len : 0),
+                           bit_width));
+    return vloc;
+  }
+
+  Value CreateVector(size_t start, size_t vec_len, size_t step, bool typed,
+                     bool fixed, const Value *keys = nullptr) {
+    FLATBUFFERS_ASSERT(
+        !fixed ||
+        typed);  // typed=false, fixed=true combination is not supported.
+    // Figure out smallest bit width we can store this vector with.
+    auto bit_width = (std::max)(force_min_bit_width_, WidthU(vec_len));
+    auto prefix_elems = 1;
+    if (keys) {
+      // If this vector is part of a map, we will pre-fix an offset to the keys
+      // to this vector.
+      bit_width = (std::max)(bit_width, keys->ElemWidth(buf_.size(), 0));
+      prefix_elems += 2;
+    }
+    Type vector_type = FBT_KEY;
+    // Check bit widths and types for all elements.
+    for (size_t i = start; i < stack_.size(); i += step) {
+      auto elem_width =
+          stack_[i].ElemWidth(buf_.size(), i - start + prefix_elems);
+      bit_width = (std::max)(bit_width, elem_width);
+      if (typed) {
+        if (i == start) {
+          vector_type = stack_[i].type_;
+        } else {
+          // If you get this assert, you are writing a typed vector with
+          // elements that are not all the same type.
+          FLATBUFFERS_ASSERT(vector_type == stack_[i].type_);
+        }
+      }
+    }
+    // If you get this assert, your typed types are not one of:
+    // Int / UInt / Float / Key.
+    FLATBUFFERS_ASSERT(!typed || IsTypedVectorElementType(vector_type));
+    auto byte_width = Align(bit_width);
+    // Write vector. First the keys width/offset if available, and size.
+    if (keys) {
+      WriteOffset(keys->u_, byte_width);
+      Write<uint64_t>(1ULL << keys->min_bit_width_, byte_width);
+    }
+    if (!fixed) Write<uint64_t>(vec_len, byte_width);
+    // Then the actual data.
+    auto vloc = buf_.size();
+    for (size_t i = start; i < stack_.size(); i += step) {
+      WriteAny(stack_[i], byte_width);
+    }
+    // Then the types.
+    if (!typed) {
+      for (size_t i = start; i < stack_.size(); i += step) {
+        buf_.push_back(stack_[i].StoredPackedType(bit_width));
+      }
+    }
+    return Value(static_cast<uint64_t>(vloc),
+                 keys ? FBT_MAP
+                      : (typed ? ToTypedVector(vector_type, fixed ? vec_len : 0)
+                               : FBT_VECTOR),
+                 bit_width);
+  }
+
+  // You shouldn't really be copying instances of this class.
+  Builder(const Builder &);
+  Builder &operator=(const Builder &);
+
+  std::vector<uint8_t> buf_;
+  std::vector<Value> stack_;
+
+  bool finished_;
+  bool has_duplicate_keys_;
+
+  BuilderFlag flags_;
+
+  BitWidth force_min_bit_width_;
+
+  struct KeyOffsetCompare {
+    explicit KeyOffsetCompare(const std::vector<uint8_t> &buf) : buf_(&buf) {}
+    bool operator()(size_t a, size_t b) const {
+      auto stra = reinterpret_cast<const char *>(buf_->data() + a);
+      auto strb = reinterpret_cast<const char *>(buf_->data() + b);
+      return strcmp(stra, strb) < 0;
+    }
+    const std::vector<uint8_t> *buf_;
+  };
+
+  typedef std::pair<size_t, size_t> StringOffset;
+  struct StringOffsetCompare {
+    explicit StringOffsetCompare(const std::vector<uint8_t> &buf)
+        : buf_(&buf) {}
+    bool operator()(const StringOffset &a, const StringOffset &b) const {
+      auto stra = buf_->data() + a.first;
+      auto strb = buf_->data() + b.first;
+      auto cr = memcmp(stra, strb, (std::min)(a.second, b.second) + 1);
+      return cr < 0 || (cr == 0 && a.second < b.second);
+    }
+    const std::vector<uint8_t> *buf_;
+  };
+
+  typedef std::set<size_t, KeyOffsetCompare> KeyOffsetMap;
+  typedef std::set<StringOffset, StringOffsetCompare> StringOffsetMap;
+
+  KeyOffsetMap key_pool;
+  StringOffsetMap string_pool;
+
+  friend class Verifier;
+};
+
+// Helper class to verify the integrity of a FlexBuffer
+class Verifier FLATBUFFERS_FINAL_CLASS {
+ public:
+  Verifier(const uint8_t *buf, size_t buf_len,
+           // Supplying this vector likely results in faster verification
+           // of larger buffers with many shared keys/strings, but
+           // comes at the cost of using additional memory the same size of
+           // the buffer being verified, so it is by default off.
+           std::vector<uint8_t> *reuse_tracker = nullptr,
+           bool _check_alignment = true, size_t max_depth = 64)
+      : buf_(buf),
+        size_(buf_len),
+        depth_(0),
+        max_depth_(max_depth),
+        num_vectors_(0),
+        max_vectors_(buf_len),
+        check_alignment_(_check_alignment),
+        reuse_tracker_(reuse_tracker) {
+    FLATBUFFERS_ASSERT(size_ < FLATBUFFERS_MAX_BUFFER_SIZE);
+    if (reuse_tracker_) {
+      reuse_tracker_->clear();
+      reuse_tracker_->resize(size_, PackedType(BIT_WIDTH_8, FBT_NULL));
+    }
+  }
+
+ private:
+  // Central location where any verification failures register.
+  bool Check(bool ok) const {
+    // clang-format off
+    #ifdef FLATBUFFERS_DEBUG_VERIFICATION_FAILURE
+      FLATBUFFERS_ASSERT(ok);
+    #endif
+    // clang-format on
+    return ok;
+  }
+
+  // Verify any range within the buffer.
+  bool VerifyFrom(size_t elem, size_t elem_len) const {
+    return Check(elem_len < size_ && elem <= size_ - elem_len);
+  }
+  bool VerifyBefore(size_t elem, size_t elem_len) const {
+    return Check(elem_len <= elem);
+  }
+
+  bool VerifyFromPointer(const uint8_t *p, size_t len) {
+    auto o = static_cast<size_t>(p - buf_);
+    return VerifyFrom(o, len);
+  }
+  bool VerifyBeforePointer(const uint8_t *p, size_t len) {
+    auto o = static_cast<size_t>(p - buf_);
+    return VerifyBefore(o, len);
+  }
+
+  bool VerifyByteWidth(size_t width) {
+    return Check(width == 1 || width == 2 || width == 4 || width == 8);
+  }
+
+  bool VerifyType(int type) { return Check(type >= 0 && type < FBT_MAX_TYPE); }
+
+  bool VerifyOffset(uint64_t off, const uint8_t *p) {
+    return Check(off <= static_cast<uint64_t>(size_)) &&
+           off <= static_cast<uint64_t>(p - buf_);
+  }
+
+  bool VerifyAlignment(const uint8_t *p, size_t size) const {
+    auto o = static_cast<size_t>(p - buf_);
+    return Check((o & (size - 1)) == 0 || !check_alignment_);
+  }
+
+// Macro, since we want to escape from parent function & use lazy args.
+#define FLEX_CHECK_VERIFIED(P, PACKED_TYPE)                     \
+  if (reuse_tracker_) {                                         \
+    auto packed_type = PACKED_TYPE;                             \
+    auto existing = (*reuse_tracker_)[P - buf_];                \
+    if (existing == packed_type) return true;                   \
+    /* Fail verification if already set with different type! */ \
+    if (!Check(existing == 0)) return false;                    \
+    (*reuse_tracker_)[P - buf_] = packed_type;                  \
+  }
+
+  bool VerifyVector(Reference r, const uint8_t *p, Type elem_type) {
+    // Any kind of nesting goes thru this function, so guard against that
+    // here, both with simple nesting checks, and the reuse tracker if on.
+    depth_++;
+    num_vectors_++;
+    if (!Check(depth_ <= max_depth_ && num_vectors_ <= max_vectors_))
+      return false;
+    auto size_byte_width = r.byte_width_;
+    if (!VerifyBeforePointer(p, size_byte_width)) return false;
+    FLEX_CHECK_VERIFIED(p - size_byte_width,
+                        PackedType(Builder::WidthB(size_byte_width), r.type_));
+    auto sized = Sized(p, size_byte_width);
+    auto num_elems = sized.size();
+    auto elem_byte_width = r.type_ == FBT_STRING || r.type_ == FBT_BLOB
+                               ? uint8_t(1)
+                               : r.byte_width_;
+    auto max_elems = SIZE_MAX / elem_byte_width;
+    if (!Check(num_elems < max_elems))
+      return false;  // Protect against byte_size overflowing.
+    auto byte_size = num_elems * elem_byte_width;
+    if (!VerifyFromPointer(p, byte_size)) return false;
+    if (elem_type == FBT_NULL) {
+      // Verify type bytes after the vector.
+      if (!VerifyFromPointer(p + byte_size, num_elems)) return false;
+      auto v = Vector(p, size_byte_width);
+      for (size_t i = 0; i < num_elems; i++)
+        if (!VerifyRef(v[i])) return false;
+    } else if (elem_type == FBT_KEY) {
+      auto v = TypedVector(p, elem_byte_width, FBT_KEY);
+      for (size_t i = 0; i < num_elems; i++)
+        if (!VerifyRef(v[i])) return false;
+    } else {
+      FLATBUFFERS_ASSERT(IsInline(elem_type));
+    }
+    depth_--;
+    return true;
+  }
+
+  bool VerifyKeys(const uint8_t *p, uint8_t byte_width) {
+    // The vector part of the map has already been verified.
+    const size_t num_prefixed_fields = 3;
+    if (!VerifyBeforePointer(p, byte_width * num_prefixed_fields)) return false;
+    p -= byte_width * num_prefixed_fields;
+    auto off = ReadUInt64(p, byte_width);
+    if (!VerifyOffset(off, p)) return false;
+    auto key_byte_with =
+        static_cast<uint8_t>(ReadUInt64(p + byte_width, byte_width));
+    if (!VerifyByteWidth(key_byte_with)) return false;
+    return VerifyVector(Reference(p, byte_width, key_byte_with, FBT_VECTOR_KEY),
+                        p - off, FBT_KEY);
+  }
+
+  bool VerifyKey(const uint8_t *p) {
+    FLEX_CHECK_VERIFIED(p, PackedType(BIT_WIDTH_8, FBT_KEY));
+    while (p < buf_ + size_)
+      if (*p++) return true;
+    return false;
+  }
+
+#undef FLEX_CHECK_VERIFIED
+
+  bool VerifyTerminator(const String &s) {
+    return VerifyFromPointer(reinterpret_cast<const uint8_t *>(s.c_str()),
+                             s.size() + 1);
+  }
+
+  bool VerifyRef(Reference r) {
+    // r.parent_width_ and r.data_ already verified.
+    if (!VerifyByteWidth(r.byte_width_) || !VerifyType(r.type_)) {
+      return false;
+    }
+    if (IsInline(r.type_)) {
+      // Inline scalars, don't require further verification.
+      return true;
+    }
+    // All remaining types are an offset.
+    auto off = ReadUInt64(r.data_, r.parent_width_);
+    if (!VerifyOffset(off, r.data_)) return false;
+    auto p = r.Indirect();
+    if (!VerifyAlignment(p, r.byte_width_)) return false;
+    switch (r.type_) {
+      case FBT_INDIRECT_INT:
+      case FBT_INDIRECT_UINT:
+      case FBT_INDIRECT_FLOAT: return VerifyFromPointer(p, r.byte_width_);
+      case FBT_KEY: return VerifyKey(p);
+      case FBT_MAP:
+        return VerifyVector(r, p, FBT_NULL) && VerifyKeys(p, r.byte_width_);
+      case FBT_VECTOR: return VerifyVector(r, p, FBT_NULL);
+      case FBT_VECTOR_INT: return VerifyVector(r, p, FBT_INT);
+      case FBT_VECTOR_BOOL:
+      case FBT_VECTOR_UINT: return VerifyVector(r, p, FBT_UINT);
+      case FBT_VECTOR_FLOAT: return VerifyVector(r, p, FBT_FLOAT);
+      case FBT_VECTOR_KEY: return VerifyVector(r, p, FBT_KEY);
+      case FBT_VECTOR_STRING_DEPRECATED:
+        // Use of FBT_KEY here intentional, see elsewhere.
+        return VerifyVector(r, p, FBT_KEY);
+      case FBT_BLOB: return VerifyVector(r, p, FBT_UINT);
+      case FBT_STRING:
+        return VerifyVector(r, p, FBT_UINT) &&
+               VerifyTerminator(String(p, r.byte_width_));
+      case FBT_VECTOR_INT2:
+      case FBT_VECTOR_UINT2:
+      case FBT_VECTOR_FLOAT2:
+      case FBT_VECTOR_INT3:
+      case FBT_VECTOR_UINT3:
+      case FBT_VECTOR_FLOAT3:
+      case FBT_VECTOR_INT4:
+      case FBT_VECTOR_UINT4:
+      case FBT_VECTOR_FLOAT4: {
+        uint8_t len = 0;
+        auto vtype = ToFixedTypedVectorElementType(r.type_, &len);
+        if (!VerifyType(vtype)) return false;
+        return VerifyFromPointer(p, r.byte_width_ * len);
+      }
+      default: return false;
+    }
+  }
+
+ public:
+  bool VerifyBuffer() {
+    if (!Check(size_ >= 3)) return false;
+    auto end = buf_ + size_;
+    auto byte_width = *--end;
+    auto packed_type = *--end;
+    return VerifyByteWidth(byte_width) && Check(end - buf_ >= byte_width) &&
+           VerifyRef(Reference(end - byte_width, byte_width, packed_type));
+  }
+
+ private:
+  const uint8_t *buf_;
+  size_t size_;
+  size_t depth_;
+  const size_t max_depth_;
+  size_t num_vectors_;
+  const size_t max_vectors_;
+  bool check_alignment_;
+  std::vector<uint8_t> *reuse_tracker_;
+};
+
+// Utility function that contructs the Verifier for you, see above for
+// parameters.
+inline bool VerifyBuffer(const uint8_t *buf, size_t buf_len,
+                         std::vector<uint8_t> *reuse_tracker = nullptr) {
+  Verifier verifier(buf, buf_len, reuse_tracker);
+  return verifier.VerifyBuffer();
+}
+
+}  // namespace flexbuffers
+
+#if defined(_MSC_VER)
+#  pragma warning(pop)
+#endif
+
+#endif  // FLATBUFFERS_FLEXBUFFERS_H_
diff --git a/third_party/tflite-micro/third_party/flatbuffers/include/flatbuffers/flexbuffers.h.rej b/third_party/tflite-micro/third_party/flatbuffers/include/flatbuffers/flexbuffers.h.rej
new file mode 100644
index 00000000..a0dba385
--- /dev/null
+++ b/third_party/tflite-micro/third_party/flatbuffers/include/flatbuffers/flexbuffers.h.rej
@@ -0,0 +1,1900 @@
+--- third_party/flatbuffers/include/flatbuffers/flexbuffers.h	2023-04-24 21:08:39.066741057 +0800
++++ third_party/flatbuffers/include/flatbuffers/flexbuffers.h	1970-01-01 08:00:00.000000000 +0800
+@@ -1,1897 +0,0 @@
+-/*
+- * Copyright 2017 Google Inc. All rights reserved.
+- *
+- * Licensed under the Apache License, Version 2.0 (the "License");
+- * you may not use this file except in compliance with the License.
+- * You may obtain a copy of the License at
+- *
+- *     http://www.apache.org/licenses/LICENSE-2.0
+- *
+- * Unless required by applicable law or agreed to in writing, software
+- * distributed under the License is distributed on an "AS IS" BASIS,
+- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+- * See the License for the specific language governing permissions and
+- * limitations under the License.
+- */
+-
+-#ifndef FLATBUFFERS_FLEXBUFFERS_H_
+-#define FLATBUFFERS_FLEXBUFFERS_H_
+-
+-#include <map>
+-// Used to select STL variant.
+-#include "flatbuffers/base.h"
+-// We use the basic binary writing functions from the regular FlatBuffers.
+-#include "flatbuffers/util.h"
+-
+-#ifdef _MSC_VER
+-#  include <intrin.h>
+-#endif
+-
+-#if defined(_MSC_VER)
+-#  pragma warning(push)
+-#  pragma warning(disable : 4127)  // C4127: conditional expression is constant
+-#endif
+-
+-namespace flexbuffers {
+-
+-class Reference;
+-class Map;
+-
+-// These are used in the lower 2 bits of a type field to determine the size of
+-// the elements (and or size field) of the item pointed to (e.g. vector).
+-enum BitWidth {
+-  BIT_WIDTH_8 = 0,
+-  BIT_WIDTH_16 = 1,
+-  BIT_WIDTH_32 = 2,
+-  BIT_WIDTH_64 = 3,
+-};
+-
+-// These are used as the upper 6 bits of a type field to indicate the actual
+-// type.
+-enum Type {
+-  FBT_NULL = 0,
+-  FBT_INT = 1,
+-  FBT_UINT = 2,
+-  FBT_FLOAT = 3,
+-  // Types above stored inline, types below (except FBT_BOOL) store an offset.
+-  FBT_KEY = 4,
+-  FBT_STRING = 5,
+-  FBT_INDIRECT_INT = 6,
+-  FBT_INDIRECT_UINT = 7,
+-  FBT_INDIRECT_FLOAT = 8,
+-  FBT_MAP = 9,
+-  FBT_VECTOR = 10,      // Untyped.
+-  FBT_VECTOR_INT = 11,  // Typed any size (stores no type table).
+-  FBT_VECTOR_UINT = 12,
+-  FBT_VECTOR_FLOAT = 13,
+-  FBT_VECTOR_KEY = 14,
+-  // DEPRECATED, use FBT_VECTOR or FBT_VECTOR_KEY instead.
+-  // Read test.cpp/FlexBuffersDeprecatedTest() for details on why.
+-  FBT_VECTOR_STRING_DEPRECATED = 15,
+-  FBT_VECTOR_INT2 = 16,  // Typed tuple (no type table, no size field).
+-  FBT_VECTOR_UINT2 = 17,
+-  FBT_VECTOR_FLOAT2 = 18,
+-  FBT_VECTOR_INT3 = 19,  // Typed triple (no type table, no size field).
+-  FBT_VECTOR_UINT3 = 20,
+-  FBT_VECTOR_FLOAT3 = 21,
+-  FBT_VECTOR_INT4 = 22,  // Typed quad (no type table, no size field).
+-  FBT_VECTOR_UINT4 = 23,
+-  FBT_VECTOR_FLOAT4 = 24,
+-  FBT_BLOB = 25,
+-  FBT_BOOL = 26,
+-  FBT_VECTOR_BOOL =
+-      36,  // To Allow the same type of conversion of type to vector type
+-
+-  FBT_MAX_TYPE = 37
+-};
+-
+-inline bool IsInline(Type t) { return t <= FBT_FLOAT || t == FBT_BOOL; }
+-
+-inline bool IsTypedVectorElementType(Type t) {
+-  return (t >= FBT_INT && t <= FBT_STRING) || t == FBT_BOOL;
+-}
+-
+-inline bool IsTypedVector(Type t) {
+-  return (t >= FBT_VECTOR_INT && t <= FBT_VECTOR_STRING_DEPRECATED) ||
+-         t == FBT_VECTOR_BOOL;
+-}
+-
+-inline bool IsFixedTypedVector(Type t) {
+-  return t >= FBT_VECTOR_INT2 && t <= FBT_VECTOR_FLOAT4;
+-}
+-
+-inline Type ToTypedVector(Type t, size_t fixed_len = 0) {
+-  FLATBUFFERS_ASSERT(IsTypedVectorElementType(t));
+-  switch (fixed_len) {
+-    case 0: return static_cast<Type>(t - FBT_INT + FBT_VECTOR_INT);
+-    case 2: return static_cast<Type>(t - FBT_INT + FBT_VECTOR_INT2);
+-    case 3: return static_cast<Type>(t - FBT_INT + FBT_VECTOR_INT3);
+-    case 4: return static_cast<Type>(t - FBT_INT + FBT_VECTOR_INT4);
+-    default: FLATBUFFERS_ASSERT(0); return FBT_NULL;
+-  }
+-}
+-
+-inline Type ToTypedVectorElementType(Type t) {
+-  FLATBUFFERS_ASSERT(IsTypedVector(t));
+-  return static_cast<Type>(t - FBT_VECTOR_INT + FBT_INT);
+-}
+-
+-inline Type ToFixedTypedVectorElementType(Type t, uint8_t *len) {
+-  FLATBUFFERS_ASSERT(IsFixedTypedVector(t));
+-  auto fixed_type = t - FBT_VECTOR_INT2;
+-  *len = static_cast<uint8_t>(fixed_type / 3 +
+-                              2);  // 3 types each, starting from length 2.
+-  return static_cast<Type>(fixed_type % 3 + FBT_INT);
+-}
+-
+-// TODO: implement proper support for 8/16bit floats, or decide not to
+-// support them.
+-typedef int16_t half;
+-typedef int8_t quarter;
+-
+-// TODO: can we do this without conditionals using intrinsics or inline asm
+-// on some platforms? Given branch prediction the method below should be
+-// decently quick, but it is the most frequently executed function.
+-// We could do an (unaligned) 64-bit read if we ifdef out the platforms for
+-// which that doesn't work (or where we'd read into un-owned memory).
+-template<typename R, typename T1, typename T2, typename T4, typename T8>
+-R ReadSizedScalar(const uint8_t *data, uint8_t byte_width) {
+-  return byte_width < 4
+-             ? (byte_width < 2
+-                    ? static_cast<R>(flatbuffers::ReadScalar<T1>(data))
+-                    : static_cast<R>(flatbuffers::ReadScalar<T2>(data)))
+-             : (byte_width < 8
+-                    ? static_cast<R>(flatbuffers::ReadScalar<T4>(data))
+-                    : static_cast<R>(flatbuffers::ReadScalar<T8>(data)));
+-}
+-
+-inline int64_t ReadInt64(const uint8_t *data, uint8_t byte_width) {
+-  return ReadSizedScalar<int64_t, int8_t, int16_t, int32_t, int64_t>(
+-      data, byte_width);
+-}
+-
+-inline uint64_t ReadUInt64(const uint8_t *data, uint8_t byte_width) {
+-  // This is the "hottest" function (all offset lookups use this), so worth
+-  // optimizing if possible.
+-  // TODO: GCC apparently replaces memcpy by a rep movsb, but only if count is a
+-  // constant, which here it isn't. Test if memcpy is still faster than
+-  // the conditionals in ReadSizedScalar. Can also use inline asm.
+-  // clang-format off
+-  #if defined(_MSC_VER) && defined(_M_X64) && !defined(_M_ARM64EC)
+-  // This is 64-bit Windows only, __movsb does not work on 32-bit Windows.
+-    uint64_t u = 0;
+-    __movsb(reinterpret_cast<uint8_t *>(&u),
+-            reinterpret_cast<const uint8_t *>(data), byte_width);
+-    return flatbuffers::EndianScalar(u);
+-  #else
+-    return ReadSizedScalar<uint64_t, uint8_t, uint16_t, uint32_t, uint64_t>(
+-             data, byte_width);
+-  #endif
+-  // clang-format on
+-}
+-
+-inline double ReadDouble(const uint8_t *data, uint8_t byte_width) {
+-  return ReadSizedScalar<double, quarter, half, float, double>(data,
+-                                                               byte_width);
+-}
+-
+-inline const uint8_t *Indirect(const uint8_t *offset, uint8_t byte_width) {
+-  return offset - ReadUInt64(offset, byte_width);
+-}
+-
+-template<typename T> const uint8_t *Indirect(const uint8_t *offset) {
+-  return offset - flatbuffers::ReadScalar<T>(offset);
+-}
+-
+-inline BitWidth WidthU(uint64_t u) {
+-#define FLATBUFFERS_GET_FIELD_BIT_WIDTH(value, width)                   \
+-  {                                                                     \
+-    if (!((u) & ~((1ULL << (width)) - 1ULL))) return BIT_WIDTH_##width; \
+-  }
+-  FLATBUFFERS_GET_FIELD_BIT_WIDTH(u, 8);
+-  FLATBUFFERS_GET_FIELD_BIT_WIDTH(u, 16);
+-  FLATBUFFERS_GET_FIELD_BIT_WIDTH(u, 32);
+-#undef FLATBUFFERS_GET_FIELD_BIT_WIDTH
+-  return BIT_WIDTH_64;
+-}
+-
+-inline BitWidth WidthI(int64_t i) {
+-  auto u = static_cast<uint64_t>(i) << 1;
+-  return WidthU(i >= 0 ? u : ~u);
+-}
+-
+-inline BitWidth WidthF(double f) {
+-  return static_cast<double>(static_cast<float>(f)) == f ? BIT_WIDTH_32
+-                                                         : BIT_WIDTH_64;
+-}
+-
+-// Base class of all types below.
+-// Points into the data buffer and allows access to one type.
+-class Object {
+- public:
+-  Object(const uint8_t *data, uint8_t byte_width)
+-      : data_(data), byte_width_(byte_width) {}
+-
+- protected:
+-  const uint8_t *data_;
+-  uint8_t byte_width_;
+-};
+-
+-// Object that has a size, obtained either from size prefix, or elsewhere.
+-class Sized : public Object {
+- public:
+-  // Size prefix.
+-  Sized(const uint8_t *data, uint8_t byte_width)
+-      : Object(data, byte_width), size_(read_size()) {}
+-  // Manual size.
+-  Sized(const uint8_t *data, uint8_t byte_width, size_t sz)
+-      : Object(data, byte_width), size_(sz) {}
+-  size_t size() const { return size_; }
+-  // Access size stored in `byte_width_` bytes before data_ pointer.
+-  size_t read_size() const {
+-    return static_cast<size_t>(ReadUInt64(data_ - byte_width_, byte_width_));
+-  }
+-
+- protected:
+-  size_t size_;
+-};
+-
+-class String : public Sized {
+- public:
+-  // Size prefix.
+-  String(const uint8_t *data, uint8_t byte_width) : Sized(data, byte_width) {}
+-  // Manual size.
+-  String(const uint8_t *data, uint8_t byte_width, size_t sz)
+-      : Sized(data, byte_width, sz) {}
+-
+-  size_t length() const { return size(); }
+-  const char *c_str() const { return reinterpret_cast<const char *>(data_); }
+-  std::string str() const { return std::string(c_str(), size()); }
+-
+-  static String EmptyString() {
+-    static const char *empty_string = "";
+-    return String(reinterpret_cast<const uint8_t *>(empty_string), 1, 0);
+-  }
+-  bool IsTheEmptyString() const { return data_ == EmptyString().data_; }
+-};
+-
+-class Blob : public Sized {
+- public:
+-  Blob(const uint8_t *data_buf, uint8_t byte_width)
+-      : Sized(data_buf, byte_width) {}
+-
+-  static Blob EmptyBlob() {
+-    static const uint8_t empty_blob[] = { 0 /*len*/ };
+-    return Blob(empty_blob + 1, 1);
+-  }
+-  bool IsTheEmptyBlob() const { return data_ == EmptyBlob().data_; }
+-  const uint8_t *data() const { return data_; }
+-};
+-
+-class Vector : public Sized {
+- public:
+-  Vector(const uint8_t *data, uint8_t byte_width) : Sized(data, byte_width) {}
+-
+-  Reference operator[](size_t i) const;
+-
+-  static Vector EmptyVector() {
+-    static const uint8_t empty_vector[] = { 0 /*len*/ };
+-    return Vector(empty_vector + 1, 1);
+-  }
+-  bool IsTheEmptyVector() const { return data_ == EmptyVector().data_; }
+-};
+-
+-class TypedVector : public Sized {
+- public:
+-  TypedVector(const uint8_t *data, uint8_t byte_width, Type element_type)
+-      : Sized(data, byte_width), type_(element_type) {}
+-
+-  Reference operator[](size_t i) const;
+-
+-  static TypedVector EmptyTypedVector() {
+-    static const uint8_t empty_typed_vector[] = { 0 /*len*/ };
+-    return TypedVector(empty_typed_vector + 1, 1, FBT_INT);
+-  }
+-  bool IsTheEmptyVector() const {
+-    return data_ == TypedVector::EmptyTypedVector().data_;
+-  }
+-
+-  Type ElementType() { return type_; }
+-
+-  friend Reference;
+-
+- private:
+-  Type type_;
+-
+-  friend Map;
+-};
+-
+-class FixedTypedVector : public Object {
+- public:
+-  FixedTypedVector(const uint8_t *data, uint8_t byte_width, Type element_type,
+-                   uint8_t len)
+-      : Object(data, byte_width), type_(element_type), len_(len) {}
+-
+-  Reference operator[](size_t i) const;
+-
+-  static FixedTypedVector EmptyFixedTypedVector() {
+-    static const uint8_t fixed_empty_vector[] = { 0 /* unused */ };
+-    return FixedTypedVector(fixed_empty_vector, 1, FBT_INT, 0);
+-  }
+-  bool IsTheEmptyFixedTypedVector() const {
+-    return data_ == FixedTypedVector::EmptyFixedTypedVector().data_;
+-  }
+-
+-  Type ElementType() const { return type_; }
+-  uint8_t size() const { return len_; }
+-
+- private:
+-  Type type_;
+-  uint8_t len_;
+-};
+-
+-class Map : public Vector {
+- public:
+-  Map(const uint8_t *data, uint8_t byte_width) : Vector(data, byte_width) {}
+-
+-  Reference operator[](const char *key) const;
+-  Reference operator[](const std::string &key) const;
+-
+-  Vector Values() const { return Vector(data_, byte_width_); }
+-
+-  TypedVector Keys() const {
+-    const size_t num_prefixed_fields = 3;
+-    auto keys_offset = data_ - byte_width_ * num_prefixed_fields;
+-    return TypedVector(Indirect(keys_offset, byte_width_),
+-                       static_cast<uint8_t>(
+-                           ReadUInt64(keys_offset + byte_width_, byte_width_)),
+-                       FBT_KEY);
+-  }
+-
+-  static Map EmptyMap() {
+-    static const uint8_t empty_map[] = {
+-      0 /*keys_len*/, 0 /*keys_offset*/, 1 /*keys_width*/, 0 /*len*/
+-    };
+-    return Map(empty_map + 4, 1);
+-  }
+-
+-  bool IsTheEmptyMap() const { return data_ == EmptyMap().data_; }
+-};
+-
+-template<typename T>
+-void AppendToString(std::string &s, T &&v, bool keys_quoted) {
+-  s += "[ ";
+-  for (size_t i = 0; i < v.size(); i++) {
+-    if (i) s += ", ";
+-    v[i].ToString(true, keys_quoted, s);
+-  }
+-  s += " ]";
+-}
+-
+-class Reference {
+- public:
+-  Reference()
+-      : data_(nullptr), parent_width_(0), byte_width_(0), type_(FBT_NULL) {}
+-
+-  Reference(const uint8_t *data, uint8_t parent_width, uint8_t byte_width,
+-            Type type)
+-      : data_(data),
+-        parent_width_(parent_width),
+-        byte_width_(byte_width),
+-        type_(type) {}
+-
+-  Reference(const uint8_t *data, uint8_t parent_width, uint8_t packed_type)
+-      : data_(data), parent_width_(parent_width) {
+-    byte_width_ = 1U << static_cast<BitWidth>(packed_type & 3);
+-    type_ = static_cast<Type>(packed_type >> 2);
+-  }
+-
+-  Type GetType() const { return type_; }
+-
+-  bool IsNull() const { return type_ == FBT_NULL; }
+-  bool IsBool() const { return type_ == FBT_BOOL; }
+-  bool IsInt() const { return type_ == FBT_INT || type_ == FBT_INDIRECT_INT; }
+-  bool IsUInt() const {
+-    return type_ == FBT_UINT || type_ == FBT_INDIRECT_UINT;
+-  }
+-  bool IsIntOrUint() const { return IsInt() || IsUInt(); }
+-  bool IsFloat() const {
+-    return type_ == FBT_FLOAT || type_ == FBT_INDIRECT_FLOAT;
+-  }
+-  bool IsNumeric() const { return IsIntOrUint() || IsFloat(); }
+-  bool IsString() const { return type_ == FBT_STRING; }
+-  bool IsKey() const { return type_ == FBT_KEY; }
+-  bool IsVector() const { return type_ == FBT_VECTOR || type_ == FBT_MAP; }
+-  bool IsUntypedVector() const { return type_ == FBT_VECTOR; }
+-  bool IsTypedVector() const { return flexbuffers::IsTypedVector(type_); }
+-  bool IsFixedTypedVector() const {
+-    return flexbuffers::IsFixedTypedVector(type_);
+-  }
+-  bool IsAnyVector() const {
+-    return (IsTypedVector() || IsFixedTypedVector() || IsVector());
+-  }
+-  bool IsMap() const { return type_ == FBT_MAP; }
+-  bool IsBlob() const { return type_ == FBT_BLOB; }
+-  bool AsBool() const {
+-    return (type_ == FBT_BOOL ? ReadUInt64(data_, parent_width_)
+-                              : AsUInt64()) != 0;
+-  }
+-
+-  // Reads any type as a int64_t. Never fails, does most sensible conversion.
+-  // Truncates floats, strings are attempted to be parsed for a number,
+-  // vectors/maps return their size. Returns 0 if all else fails.
+-  int64_t AsInt64() const {
+-    if (type_ == FBT_INT) {
+-      // A fast path for the common case.
+-      return ReadInt64(data_, parent_width_);
+-    } else
+-      switch (type_) {
+-        case FBT_INDIRECT_INT: return ReadInt64(Indirect(), byte_width_);
+-        case FBT_UINT: return ReadUInt64(data_, parent_width_);
+-        case FBT_INDIRECT_UINT: return ReadUInt64(Indirect(), byte_width_);
+-        case FBT_FLOAT:
+-          return static_cast<int64_t>(ReadDouble(data_, parent_width_));
+-        case FBT_INDIRECT_FLOAT:
+-          return static_cast<int64_t>(ReadDouble(Indirect(), byte_width_));
+-        case FBT_NULL: return 0;
+-        case FBT_STRING: return flatbuffers::StringToInt(AsString().c_str());
+-        case FBT_VECTOR: return static_cast<int64_t>(AsVector().size());
+-        case FBT_BOOL: return ReadInt64(data_, parent_width_);
+-        default:
+-          // Convert other things to int.
+-          return 0;
+-      }
+-  }
+-
+-  // TODO: could specialize these to not use AsInt64() if that saves
+-  // extension ops in generated code, and use a faster op than ReadInt64.
+-  int32_t AsInt32() const { return static_cast<int32_t>(AsInt64()); }
+-  int16_t AsInt16() const { return static_cast<int16_t>(AsInt64()); }
+-  int8_t AsInt8() const { return static_cast<int8_t>(AsInt64()); }
+-
+-  uint64_t AsUInt64() const {
+-    if (type_ == FBT_UINT) {
+-      // A fast path for the common case.
+-      return ReadUInt64(data_, parent_width_);
+-    } else
+-      switch (type_) {
+-        case FBT_INDIRECT_UINT: return ReadUInt64(Indirect(), byte_width_);
+-        case FBT_INT: return ReadInt64(data_, parent_width_);
+-        case FBT_INDIRECT_INT: return ReadInt64(Indirect(), byte_width_);
+-        case FBT_FLOAT:
+-          return static_cast<uint64_t>(ReadDouble(data_, parent_width_));
+-        case FBT_INDIRECT_FLOAT:
+-          return static_cast<uint64_t>(ReadDouble(Indirect(), byte_width_));
+-        case FBT_NULL: return 0;
+-        case FBT_STRING: return flatbuffers::StringToUInt(AsString().c_str());
+-        case FBT_VECTOR: return static_cast<uint64_t>(AsVector().size());
+-        case FBT_BOOL: return ReadUInt64(data_, parent_width_);
+-        default:
+-          // Convert other things to uint.
+-          return 0;
+-      }
+-  }
+-
+-  uint32_t AsUInt32() const { return static_cast<uint32_t>(AsUInt64()); }
+-  uint16_t AsUInt16() const { return static_cast<uint16_t>(AsUInt64()); }
+-  uint8_t AsUInt8() const { return static_cast<uint8_t>(AsUInt64()); }
+-
+-  double AsDouble() const {
+-    if (type_ == FBT_FLOAT) {
+-      // A fast path for the common case.
+-      return ReadDouble(data_, parent_width_);
+-    } else
+-      switch (type_) {
+-        case FBT_INDIRECT_FLOAT: return ReadDouble(Indirect(), byte_width_);
+-        case FBT_INT:
+-          return static_cast<double>(ReadInt64(data_, parent_width_));
+-        case FBT_UINT:
+-          return static_cast<double>(ReadUInt64(data_, parent_width_));
+-        case FBT_INDIRECT_INT:
+-          return static_cast<double>(ReadInt64(Indirect(), byte_width_));
+-        case FBT_INDIRECT_UINT:
+-          return static_cast<double>(ReadUInt64(Indirect(), byte_width_));
+-        case FBT_NULL: return 0.0;
+-        case FBT_STRING: {
+-          double d;
+-          flatbuffers::StringToNumber(AsString().c_str(), &d);
+-          return d;
+-        }
+-        case FBT_VECTOR: return static_cast<double>(AsVector().size());
+-        case FBT_BOOL:
+-          return static_cast<double>(ReadUInt64(data_, parent_width_));
+-        default:
+-          // Convert strings and other things to float.
+-          return 0;
+-      }
+-  }
+-
+-  float AsFloat() const { return static_cast<float>(AsDouble()); }
+-
+-  const char *AsKey() const {
+-    if (type_ == FBT_KEY || type_ == FBT_STRING) {
+-      return reinterpret_cast<const char *>(Indirect());
+-    } else {
+-      return "";
+-    }
+-  }
+-
+-  // This function returns the empty string if you try to read something that
+-  // is not a string or key.
+-  String AsString() const {
+-    if (type_ == FBT_STRING) {
+-      return String(Indirect(), byte_width_);
+-    } else if (type_ == FBT_KEY) {
+-      auto key = Indirect();
+-      return String(key, byte_width_,
+-                    strlen(reinterpret_cast<const char *>(key)));
+-    } else {
+-      return String::EmptyString();
+-    }
+-  }
+-
+-  // Unlike AsString(), this will convert any type to a std::string.
+-  std::string ToString() const {
+-    std::string s;
+-    ToString(false, false, s);
+-    return s;
+-  }
+-
+-  // Convert any type to a JSON-like string. strings_quoted determines if
+-  // string values at the top level receive "" quotes (inside other values
+-  // they always do). keys_quoted determines if keys are quoted, at any level.
+-  // TODO(wvo): add further options to have indentation/newlines.
+-  void ToString(bool strings_quoted, bool keys_quoted, std::string &s) const {
+-    if (type_ == FBT_STRING) {
+-      String str(Indirect(), byte_width_);
+-      if (strings_quoted) {
+-        flatbuffers::EscapeString(str.c_str(), str.length(), &s, true, false);
+-      } else {
+-        s.append(str.c_str(), str.length());
+-      }
+-    } else if (IsKey()) {
+-      auto str = AsKey();
+-      if (keys_quoted) {
+-        flatbuffers::EscapeString(str, strlen(str), &s, true, false);
+-      } else {
+-        s += str;
+-      }
+-    } else if (IsInt()) {
+-      s += flatbuffers::NumToString(AsInt64());
+-    } else if (IsUInt()) {
+-      s += flatbuffers::NumToString(AsUInt64());
+-    } else if (IsFloat()) {
+-      s += flatbuffers::NumToString(AsDouble());
+-    } else if (IsNull()) {
+-      s += "null";
+-    } else if (IsBool()) {
+-      s += AsBool() ? "true" : "false";
+-    } else if (IsMap()) {
+-      s += "{ ";
+-      auto m = AsMap();
+-      auto keys = m.Keys();
+-      auto vals = m.Values();
+-      for (size_t i = 0; i < keys.size(); i++) {
+-        bool kq = keys_quoted;
+-        if (!kq) {
+-          // FlexBuffers keys may contain arbitrary characters, only allow
+-          // unquoted if it looks like an "identifier":
+-          const char *p = keys[i].AsKey();
+-          if (!flatbuffers::is_alpha(*p) && *p != '_') {
+-              kq = true;
+-          } else {
+-            while (*++p) {
+-              if (!flatbuffers::is_alnum(*p) && *p != '_') {
+-                kq = true;
+-                break;
+-              }
+-            }
+-          }
+-        }
+-        keys[i].ToString(true, kq, s);
+-        s += ": ";
+-        vals[i].ToString(true, keys_quoted, s);
+-        if (i < keys.size() - 1) s += ", ";
+-      }
+-      s += " }";
+-    } else if (IsVector()) {
+-      AppendToString<Vector>(s, AsVector(), keys_quoted);
+-    } else if (IsTypedVector()) {
+-      AppendToString<TypedVector>(s, AsTypedVector(), keys_quoted);
+-    } else if (IsFixedTypedVector()) {
+-      AppendToString<FixedTypedVector>(s, AsFixedTypedVector(), keys_quoted);
+-    } else if (IsBlob()) {
+-      auto blob = AsBlob();
+-      flatbuffers::EscapeString(reinterpret_cast<const char *>(blob.data()),
+-                                blob.size(), &s, true, false);
+-    } else {
+-      s += "(?)";
+-    }
+-  }
+-
+-  // This function returns the empty blob if you try to read a not-blob.
+-  // Strings can be viewed as blobs too.
+-  Blob AsBlob() const {
+-    if (type_ == FBT_BLOB || type_ == FBT_STRING) {
+-      return Blob(Indirect(), byte_width_);
+-    } else {
+-      return Blob::EmptyBlob();
+-    }
+-  }
+-
+-  // This function returns the empty vector if you try to read a not-vector.
+-  // Maps can be viewed as vectors too.
+-  Vector AsVector() const {
+-    if (type_ == FBT_VECTOR || type_ == FBT_MAP) {
+-      return Vector(Indirect(), byte_width_);
+-    } else {
+-      return Vector::EmptyVector();
+-    }
+-  }
+-
+-  TypedVector AsTypedVector() const {
+-    if (IsTypedVector()) {
+-      auto tv =
+-          TypedVector(Indirect(), byte_width_, ToTypedVectorElementType(type_));
+-      if (tv.type_ == FBT_STRING) {
+-        // These can't be accessed as strings, since we don't know the bit-width
+-        // of the size field, see the declaration of
+-        // FBT_VECTOR_STRING_DEPRECATED above for details.
+-        // We change the type here to be keys, which are a subtype of strings,
+-        // and will ignore the size field. This will truncate strings with
+-        // embedded nulls.
+-        tv.type_ = FBT_KEY;
+-      }
+-      return tv;
+-    } else {
+-      return TypedVector::EmptyTypedVector();
+-    }
+-  }
+-
+-  FixedTypedVector AsFixedTypedVector() const {
+-    if (IsFixedTypedVector()) {
+-      uint8_t len = 0;
+-      auto vtype = ToFixedTypedVectorElementType(type_, &len);
+-      return FixedTypedVector(Indirect(), byte_width_, vtype, len);
+-    } else {
+-      return FixedTypedVector::EmptyFixedTypedVector();
+-    }
+-  }
+-
+-  Map AsMap() const {
+-    if (type_ == FBT_MAP) {
+-      return Map(Indirect(), byte_width_);
+-    } else {
+-      return Map::EmptyMap();
+-    }
+-  }
+-
+-  template<typename T> T As() const;
+-
+-  // Experimental: Mutation functions.
+-  // These allow scalars in an already created buffer to be updated in-place.
+-  // Since by default scalars are stored in the smallest possible space,
+-  // the new value may not fit, in which case these functions return false.
+-  // To avoid this, you can construct the values you intend to mutate using
+-  // Builder::ForceMinimumBitWidth.
+-  bool MutateInt(int64_t i) {
+-    if (type_ == FBT_INT) {
+-      return Mutate(data_, i, parent_width_, WidthI(i));
+-    } else if (type_ == FBT_INDIRECT_INT) {
+-      return Mutate(Indirect(), i, byte_width_, WidthI(i));
+-    } else if (type_ == FBT_UINT) {
+-      auto u = static_cast<uint64_t>(i);
+-      return Mutate(data_, u, parent_width_, WidthU(u));
+-    } else if (type_ == FBT_INDIRECT_UINT) {
+-      auto u = static_cast<uint64_t>(i);
+-      return Mutate(Indirect(), u, byte_width_, WidthU(u));
+-    } else {
+-      return false;
+-    }
+-  }
+-
+-  bool MutateBool(bool b) {
+-    return type_ == FBT_BOOL && Mutate(data_, b, parent_width_, BIT_WIDTH_8);
+-  }
+-
+-  bool MutateUInt(uint64_t u) {
+-    if (type_ == FBT_UINT) {
+-      return Mutate(data_, u, parent_width_, WidthU(u));
+-    } else if (type_ == FBT_INDIRECT_UINT) {
+-      return Mutate(Indirect(), u, byte_width_, WidthU(u));
+-    } else if (type_ == FBT_INT) {
+-      auto i = static_cast<int64_t>(u);
+-      return Mutate(data_, i, parent_width_, WidthI(i));
+-    } else if (type_ == FBT_INDIRECT_INT) {
+-      auto i = static_cast<int64_t>(u);
+-      return Mutate(Indirect(), i, byte_width_, WidthI(i));
+-    } else {
+-      return false;
+-    }
+-  }
+-
+-  bool MutateFloat(float f) {
+-    if (type_ == FBT_FLOAT) {
+-      return MutateF(data_, f, parent_width_, BIT_WIDTH_32);
+-    } else if (type_ == FBT_INDIRECT_FLOAT) {
+-      return MutateF(Indirect(), f, byte_width_, BIT_WIDTH_32);
+-    } else {
+-      return false;
+-    }
+-  }
+-
+-  bool MutateFloat(double d) {
+-    if (type_ == FBT_FLOAT) {
+-      return MutateF(data_, d, parent_width_, WidthF(d));
+-    } else if (type_ == FBT_INDIRECT_FLOAT) {
+-      return MutateF(Indirect(), d, byte_width_, WidthF(d));
+-    } else {
+-      return false;
+-    }
+-  }
+-
+-  bool MutateString(const char *str, size_t len) {
+-    auto s = AsString();
+-    if (s.IsTheEmptyString()) return false;
+-    // This is very strict, could allow shorter strings, but that creates
+-    // garbage.
+-    if (s.length() != len) return false;
+-    memcpy(const_cast<char *>(s.c_str()), str, len);
+-    return true;
+-  }
+-  bool MutateString(const char *str) { return MutateString(str, strlen(str)); }
+-  bool MutateString(const std::string &str) {
+-    return MutateString(str.data(), str.length());
+-  }
+-
+- private:
+-  const uint8_t *Indirect() const {
+-    return flexbuffers::Indirect(data_, parent_width_);
+-  }
+-
+-  template<typename T>
+-  bool Mutate(const uint8_t *dest, T t, size_t byte_width,
+-              BitWidth value_width) {
+-    auto fits = static_cast<size_t>(static_cast<size_t>(1U) << value_width) <=
+-                byte_width;
+-    if (fits) {
+-      t = flatbuffers::EndianScalar(t);
+-      memcpy(const_cast<uint8_t *>(dest), &t, byte_width);
+-    }
+-    return fits;
+-  }
+-
+-  template<typename T>
+-  bool MutateF(const uint8_t *dest, T t, size_t byte_width,
+-               BitWidth value_width) {
+-    if (byte_width == sizeof(double))
+-      return Mutate(dest, static_cast<double>(t), byte_width, value_width);
+-    if (byte_width == sizeof(float))
+-      return Mutate(dest, static_cast<float>(t), byte_width, value_width);
+-    FLATBUFFERS_ASSERT(false);
+-    return false;
+-  }
+-
+-  friend class Verifier;
+-
+-  const uint8_t *data_;
+-  uint8_t parent_width_;
+-  uint8_t byte_width_;
+-  Type type_;
+-};
+-
+-// Template specialization for As().
+-template<> inline bool Reference::As<bool>() const { return AsBool(); }
+-
+-template<> inline int8_t Reference::As<int8_t>() const { return AsInt8(); }
+-template<> inline int16_t Reference::As<int16_t>() const { return AsInt16(); }
+-template<> inline int32_t Reference::As<int32_t>() const { return AsInt32(); }
+-template<> inline int64_t Reference::As<int64_t>() const { return AsInt64(); }
+-
+-template<> inline uint8_t Reference::As<uint8_t>() const { return AsUInt8(); }
+-template<> inline uint16_t Reference::As<uint16_t>() const {
+-  return AsUInt16();
+-}
+-template<> inline uint32_t Reference::As<uint32_t>() const {
+-  return AsUInt32();
+-}
+-template<> inline uint64_t Reference::As<uint64_t>() const {
+-  return AsUInt64();
+-}
+-
+-template<> inline double Reference::As<double>() const { return AsDouble(); }
+-template<> inline float Reference::As<float>() const { return AsFloat(); }
+-
+-template<> inline String Reference::As<String>() const { return AsString(); }
+-template<> inline std::string Reference::As<std::string>() const {
+-  return AsString().str();
+-}
+-
+-template<> inline Blob Reference::As<Blob>() const { return AsBlob(); }
+-template<> inline Vector Reference::As<Vector>() const { return AsVector(); }
+-template<> inline TypedVector Reference::As<TypedVector>() const {
+-  return AsTypedVector();
+-}
+-template<> inline FixedTypedVector Reference::As<FixedTypedVector>() const {
+-  return AsFixedTypedVector();
+-}
+-template<> inline Map Reference::As<Map>() const { return AsMap(); }
+-
+-inline uint8_t PackedType(BitWidth bit_width, Type type) {
+-  return static_cast<uint8_t>(bit_width | (type << 2));
+-}
+-
+-inline uint8_t NullPackedType() { return PackedType(BIT_WIDTH_8, FBT_NULL); }
+-
+-// Vector accessors.
+-// Note: if you try to access outside of bounds, you get a Null value back
+-// instead. Normally this would be an assert, but since this is "dynamically
+-// typed" data, you may not want that (someone sends you a 2d vector and you
+-// wanted 3d).
+-// The Null converts seamlessly into a default value for any other type.
+-// TODO(wvo): Could introduce an #ifdef that makes this into an assert?
+-inline Reference Vector::operator[](size_t i) const {
+-  auto len = size();
+-  if (i >= len) return Reference(nullptr, 1, NullPackedType());
+-  auto packed_type = (data_ + len * byte_width_)[i];
+-  auto elem = data_ + i * byte_width_;
+-  return Reference(elem, byte_width_, packed_type);
+-}
+-
+-inline Reference TypedVector::operator[](size_t i) const {
+-  auto len = size();
+-  if (i >= len) return Reference(nullptr, 1, NullPackedType());
+-  auto elem = data_ + i * byte_width_;
+-  return Reference(elem, byte_width_, 1, type_);
+-}
+-
+-inline Reference FixedTypedVector::operator[](size_t i) const {
+-  if (i >= len_) return Reference(nullptr, 1, NullPackedType());
+-  auto elem = data_ + i * byte_width_;
+-  return Reference(elem, byte_width_, 1, type_);
+-}
+-
+-template<typename T> int KeyCompare(const void *key, const void *elem) {
+-  auto str_elem = reinterpret_cast<const char *>(
+-      Indirect<T>(reinterpret_cast<const uint8_t *>(elem)));
+-  auto skey = reinterpret_cast<const char *>(key);
+-  return strcmp(skey, str_elem);
+-}
+-
+-inline Reference Map::operator[](const char *key) const {
+-  auto keys = Keys();
+-  // We can't pass keys.byte_width_ to the comparison function, so we have
+-  // to pick the right one ahead of time.
+-  int (*comp)(const void *, const void *) = nullptr;
+-  switch (keys.byte_width_) {
+-    case 1: comp = KeyCompare<uint8_t>; break;
+-    case 2: comp = KeyCompare<uint16_t>; break;
+-    case 4: comp = KeyCompare<uint32_t>; break;
+-    case 8: comp = KeyCompare<uint64_t>; break;
+-    default: FLATBUFFERS_ASSERT(false); return Reference();
+-  }
+-  auto res = std::bsearch(key, keys.data_, keys.size(), keys.byte_width_, comp);
+-  if (!res) return Reference(nullptr, 1, NullPackedType());
+-  auto i = (reinterpret_cast<uint8_t *>(res) - keys.data_) / keys.byte_width_;
+-  return (*static_cast<const Vector *>(this))[i];
+-}
+-
+-inline Reference Map::operator[](const std::string &key) const {
+-  return (*this)[key.c_str()];
+-}
+-
+-inline Reference GetRoot(const uint8_t *buffer, size_t size) {
+-  // See Finish() below for the serialization counterpart of this.
+-  // The root starts at the end of the buffer, so we parse backwards from there.
+-  auto end = buffer + size;
+-  auto byte_width = *--end;
+-  auto packed_type = *--end;
+-  end -= byte_width;  // The root data item.
+-  return Reference(end, byte_width, packed_type);
+-}
+-
+-inline Reference GetRoot(const std::vector<uint8_t> &buffer) {
+-  return GetRoot(buffer.data(), buffer.size());
+-}
+-
+-// Flags that configure how the Builder behaves.
+-// The "Share" flags determine if the Builder automatically tries to pool
+-// this type. Pooling can reduce the size of serialized data if there are
+-// multiple maps of the same kind, at the expense of slightly slower
+-// serialization (the cost of lookups) and more memory use (std::set).
+-// By default this is on for keys, but off for strings.
+-// Turn keys off if you have e.g. only one map.
+-// Turn strings on if you expect many non-unique string values.
+-// Additionally, sharing key vectors can save space if you have maps with
+-// identical field populations.
+-enum BuilderFlag {
+-  BUILDER_FLAG_NONE = 0,
+-  BUILDER_FLAG_SHARE_KEYS = 1,
+-  BUILDER_FLAG_SHARE_STRINGS = 2,
+-  BUILDER_FLAG_SHARE_KEYS_AND_STRINGS = 3,
+-  BUILDER_FLAG_SHARE_KEY_VECTORS = 4,
+-  BUILDER_FLAG_SHARE_ALL = 7,
+-};
+-
+-class Builder FLATBUFFERS_FINAL_CLASS {
+- public:
+-  Builder(size_t initial_size = 256,
+-          BuilderFlag flags = BUILDER_FLAG_SHARE_KEYS)
+-      : buf_(initial_size),
+-        finished_(false),
+-        has_duplicate_keys_(false),
+-        flags_(flags),
+-        force_min_bit_width_(BIT_WIDTH_8),
+-        key_pool(KeyOffsetCompare(buf_)),
+-        string_pool(StringOffsetCompare(buf_)) {
+-    buf_.clear();
+-  }
+-
+-#ifdef FLATBUFFERS_DEFAULT_DECLARATION
+-  Builder(Builder &&) = default;
+-  Builder &operator=(Builder &&) = default;
+-#endif
+-
+-  /// @brief Get the serialized buffer (after you call `Finish()`).
+-  /// @return Returns a vector owned by this class.
+-  const std::vector<uint8_t> &GetBuffer() const {
+-    Finished();
+-    return buf_;
+-  }
+-
+-  // Size of the buffer. Does not include unfinished values.
+-  size_t GetSize() const { return buf_.size(); }
+-
+-  // Reset all state so we can re-use the buffer.
+-  void Clear() {
+-    buf_.clear();
+-    stack_.clear();
+-    finished_ = false;
+-    // flags_ remains as-is;
+-    force_min_bit_width_ = BIT_WIDTH_8;
+-    key_pool.clear();
+-    string_pool.clear();
+-  }
+-
+-  // All value constructing functions below have two versions: one that
+-  // takes a key (for placement inside a map) and one that doesn't (for inside
+-  // vectors and elsewhere).
+-
+-  void Null() { stack_.push_back(Value()); }
+-  void Null(const char *key) {
+-    Key(key);
+-    Null();
+-  }
+-
+-  void Int(int64_t i) { stack_.push_back(Value(i, FBT_INT, WidthI(i))); }
+-  void Int(const char *key, int64_t i) {
+-    Key(key);
+-    Int(i);
+-  }
+-
+-  void UInt(uint64_t u) { stack_.push_back(Value(u, FBT_UINT, WidthU(u))); }
+-  void UInt(const char *key, uint64_t u) {
+-    Key(key);
+-    UInt(u);
+-  }
+-
+-  void Float(float f) { stack_.push_back(Value(f)); }
+-  void Float(const char *key, float f) {
+-    Key(key);
+-    Float(f);
+-  }
+-
+-  void Double(double f) { stack_.push_back(Value(f)); }
+-  void Double(const char *key, double d) {
+-    Key(key);
+-    Double(d);
+-  }
+-
+-  void Bool(bool b) { stack_.push_back(Value(b)); }
+-  void Bool(const char *key, bool b) {
+-    Key(key);
+-    Bool(b);
+-  }
+-
+-  void IndirectInt(int64_t i) { PushIndirect(i, FBT_INDIRECT_INT, WidthI(i)); }
+-  void IndirectInt(const char *key, int64_t i) {
+-    Key(key);
+-    IndirectInt(i);
+-  }
+-
+-  void IndirectUInt(uint64_t u) {
+-    PushIndirect(u, FBT_INDIRECT_UINT, WidthU(u));
+-  }
+-  void IndirectUInt(const char *key, uint64_t u) {
+-    Key(key);
+-    IndirectUInt(u);
+-  }
+-
+-  void IndirectFloat(float f) {
+-    PushIndirect(f, FBT_INDIRECT_FLOAT, BIT_WIDTH_32);
+-  }
+-  void IndirectFloat(const char *key, float f) {
+-    Key(key);
+-    IndirectFloat(f);
+-  }
+-
+-  void IndirectDouble(double f) {
+-    PushIndirect(f, FBT_INDIRECT_FLOAT, WidthF(f));
+-  }
+-  void IndirectDouble(const char *key, double d) {
+-    Key(key);
+-    IndirectDouble(d);
+-  }
+-
+-  size_t Key(const char *str, size_t len) {
+-    auto sloc = buf_.size();
+-    WriteBytes(str, len + 1);
+-    if (flags_ & BUILDER_FLAG_SHARE_KEYS) {
+-      auto it = key_pool.find(sloc);
+-      if (it != key_pool.end()) {
+-        // Already in the buffer. Remove key we just serialized, and use
+-        // existing offset instead.
+-        buf_.resize(sloc);
+-        sloc = *it;
+-      } else {
+-        key_pool.insert(sloc);
+-      }
+-    }
+-    stack_.push_back(Value(static_cast<uint64_t>(sloc), FBT_KEY, BIT_WIDTH_8));
+-    return sloc;
+-  }
+-
+-  size_t Key(const char *str) { return Key(str, strlen(str)); }
+-  size_t Key(const std::string &str) { return Key(str.c_str(), str.size()); }
+-
+-  size_t String(const char *str, size_t len) {
+-    auto reset_to = buf_.size();
+-    auto sloc = CreateBlob(str, len, 1, FBT_STRING);
+-    if (flags_ & BUILDER_FLAG_SHARE_STRINGS) {
+-      StringOffset so(sloc, len);
+-      auto it = string_pool.find(so);
+-      if (it != string_pool.end()) {
+-        // Already in the buffer. Remove string we just serialized, and use
+-        // existing offset instead.
+-        buf_.resize(reset_to);
+-        sloc = it->first;
+-        stack_.back().u_ = sloc;
+-      } else {
+-        string_pool.insert(so);
+-      }
+-    }
+-    return sloc;
+-  }
+-  size_t String(const char *str) { return String(str, strlen(str)); }
+-  size_t String(const std::string &str) {
+-    return String(str.c_str(), str.size());
+-  }
+-  void String(const flexbuffers::String &str) {
+-    String(str.c_str(), str.length());
+-  }
+-
+-  void String(const char *key, const char *str) {
+-    Key(key);
+-    String(str);
+-  }
+-  void String(const char *key, const std::string &str) {
+-    Key(key);
+-    String(str);
+-  }
+-  void String(const char *key, const flexbuffers::String &str) {
+-    Key(key);
+-    String(str);
+-  }
+-
+-  size_t Blob(const void *data, size_t len) {
+-    return CreateBlob(data, len, 0, FBT_BLOB);
+-  }
+-  size_t Blob(const std::vector<uint8_t> &v) {
+-    return CreateBlob(v.data(), v.size(), 0, FBT_BLOB);
+-  }
+-
+-  void Blob(const char *key, const void *data, size_t len) {
+-    Key(key);
+-    Blob(data, len);
+-  }
+-  void Blob(const char *key, const std::vector<uint8_t> &v) {
+-    Key(key);
+-    Blob(v);
+-  }
+-
+-  // TODO(wvo): support all the FlexBuffer types (like flexbuffers::String),
+-  // e.g. Vector etc. Also in overloaded versions.
+-  // Also some FlatBuffers types?
+-
+-  size_t StartVector() { return stack_.size(); }
+-  size_t StartVector(const char *key) {
+-    Key(key);
+-    return stack_.size();
+-  }
+-  size_t StartMap() { return stack_.size(); }
+-  size_t StartMap(const char *key) {
+-    Key(key);
+-    return stack_.size();
+-  }
+-
+-  // TODO(wvo): allow this to specify an alignment greater than the natural
+-  // alignment.
+-  size_t EndVector(size_t start, bool typed, bool fixed) {
+-    auto vec = CreateVector(start, stack_.size() - start, 1, typed, fixed);
+-    // Remove temp elements and return vector.
+-    stack_.resize(start);
+-    stack_.push_back(vec);
+-    return static_cast<size_t>(vec.u_);
+-  }
+-
+-  size_t EndMap(size_t start) {
+-    // We should have interleaved keys and values on the stack.
+-    // Make sure it is an even number:
+-    auto len = stack_.size() - start;
+-    FLATBUFFERS_ASSERT(!(len & 1));
+-    len /= 2;
+-    // Make sure keys are all strings:
+-    for (auto key = start; key < stack_.size(); key += 2) {
+-      FLATBUFFERS_ASSERT(stack_[key].type_ == FBT_KEY);
+-    }
+-    // Now sort values, so later we can do a binary search lookup.
+-    // We want to sort 2 array elements at a time.
+-    struct TwoValue {
+-      Value key;
+-      Value val;
+-    };
+-    // TODO(wvo): strict aliasing?
+-    // TODO(wvo): allow the caller to indicate the data is already sorted
+-    // for maximum efficiency? With an assert to check sortedness to make sure
+-    // we're not breaking binary search.
+-    // Or, we can track if the map is sorted as keys are added which would be
+-    // be quite cheap (cheaper than checking it here), so we can skip this
+-    // step automatically when appliccable, and encourage people to write in
+-    // sorted fashion.
+-    // std::sort is typically already a lot faster on sorted data though.
+-    auto dict = reinterpret_cast<TwoValue *>(stack_.data() + start);
+-    std::sort(
+-        dict, dict + len, [&](const TwoValue &a, const TwoValue &b) -> bool {
+-          auto as = reinterpret_cast<const char *>(buf_.data() + a.key.u_);
+-          auto bs = reinterpret_cast<const char *>(buf_.data() + b.key.u_);
+-          auto comp = strcmp(as, bs);
+-          // We want to disallow duplicate keys, since this results in a
+-          // map where values cannot be found.
+-          // But we can't assert here (since we don't want to fail on
+-          // random JSON input) or have an error mechanism.
+-          // Instead, we set has_duplicate_keys_ in the builder to
+-          // signal this.
+-          // TODO: Have to check for pointer equality, as some sort
+-          // implementation apparently call this function with the same
+-          // element?? Why?
+-          if (!comp && &a != &b) has_duplicate_keys_ = true;
+-          return comp < 0;
+-        });
+-    // First create a vector out of all keys.
+-    // TODO(wvo): if kBuilderFlagShareKeyVectors is true, see if we can share
+-    // the first vector.
+-    auto keys = CreateVector(start, len, 2, true, false);
+-    auto vec = CreateVector(start + 1, len, 2, false, false, &keys);
+-    // Remove temp elements and return map.
+-    stack_.resize(start);
+-    stack_.push_back(vec);
+-    return static_cast<size_t>(vec.u_);
+-  }
+-
+-  // Call this after EndMap to see if the map had any duplicate keys.
+-  // Any map with such keys won't be able to retrieve all values.
+-  bool HasDuplicateKeys() const { return has_duplicate_keys_; }
+-
+-  template<typename F> size_t Vector(F f) {
+-    auto start = StartVector();
+-    f();
+-    return EndVector(start, false, false);
+-  }
+-  template<typename F, typename T> size_t Vector(F f, T &state) {
+-    auto start = StartVector();
+-    f(state);
+-    return EndVector(start, false, false);
+-  }
+-  template<typename F> size_t Vector(const char *key, F f) {
+-    auto start = StartVector(key);
+-    f();
+-    return EndVector(start, false, false);
+-  }
+-  template<typename F, typename T>
+-  size_t Vector(const char *key, F f, T &state) {
+-    auto start = StartVector(key);
+-    f(state);
+-    return EndVector(start, false, false);
+-  }
+-
+-  template<typename T> void Vector(const T *elems, size_t len) {
+-    if (flatbuffers::is_scalar<T>::value) {
+-      // This path should be a lot quicker and use less space.
+-      ScalarVector(elems, len, false);
+-    } else {
+-      auto start = StartVector();
+-      for (size_t i = 0; i < len; i++) Add(elems[i]);
+-      EndVector(start, false, false);
+-    }
+-  }
+-  template<typename T>
+-  void Vector(const char *key, const T *elems, size_t len) {
+-    Key(key);
+-    Vector(elems, len);
+-  }
+-  template<typename T> void Vector(const std::vector<T> &vec) {
+-    Vector(vec.data(), vec.size());
+-  }
+-
+-  template<typename F> size_t TypedVector(F f) {
+-    auto start = StartVector();
+-    f();
+-    return EndVector(start, true, false);
+-  }
+-  template<typename F, typename T> size_t TypedVector(F f, T &state) {
+-    auto start = StartVector();
+-    f(state);
+-    return EndVector(start, true, false);
+-  }
+-  template<typename F> size_t TypedVector(const char *key, F f) {
+-    auto start = StartVector(key);
+-    f();
+-    return EndVector(start, true, false);
+-  }
+-  template<typename F, typename T>
+-  size_t TypedVector(const char *key, F f, T &state) {
+-    auto start = StartVector(key);
+-    f(state);
+-    return EndVector(start, true, false);
+-  }
+-
+-  template<typename T> size_t FixedTypedVector(const T *elems, size_t len) {
+-    // We only support a few fixed vector lengths. Anything bigger use a
+-    // regular typed vector.
+-    FLATBUFFERS_ASSERT(len >= 2 && len <= 4);
+-    // And only scalar values.
+-    static_assert(flatbuffers::is_scalar<T>::value, "Unrelated types");
+-    return ScalarVector(elems, len, true);
+-  }
+-
+-  template<typename T>
+-  size_t FixedTypedVector(const char *key, const T *elems, size_t len) {
+-    Key(key);
+-    return FixedTypedVector(elems, len);
+-  }
+-
+-  template<typename F> size_t Map(F f) {
+-    auto start = StartMap();
+-    f();
+-    return EndMap(start);
+-  }
+-  template<typename F, typename T> size_t Map(F f, T &state) {
+-    auto start = StartMap();
+-    f(state);
+-    return EndMap(start);
+-  }
+-  template<typename F> size_t Map(const char *key, F f) {
+-    auto start = StartMap(key);
+-    f();
+-    return EndMap(start);
+-  }
+-  template<typename F, typename T> size_t Map(const char *key, F f, T &state) {
+-    auto start = StartMap(key);
+-    f(state);
+-    return EndMap(start);
+-  }
+-  template<typename T> void Map(const std::map<std::string, T> &map) {
+-    auto start = StartMap();
+-    for (auto it = map.begin(); it != map.end(); ++it)
+-      Add(it->first.c_str(), it->second);
+-    EndMap(start);
+-  }
+-
+-  // If you wish to share a value explicitly (a value not shared automatically
+-  // through one of the BUILDER_FLAG_SHARE_* flags) you can do so with these
+-  // functions. Or if you wish to turn those flags off for performance reasons
+-  // and still do some explicit sharing. For example:
+-  // builder.IndirectDouble(M_PI);
+-  // auto id = builder.LastValue();  // Remember where we stored it.
+-  // .. more code goes here ..
+-  // builder.ReuseValue(id);  // Refers to same double by offset.
+-  // LastValue works regardless of whether the value has a key or not.
+-  // Works on any data type.
+-  struct Value;
+-  Value LastValue() { return stack_.back(); }
+-  void ReuseValue(Value v) { stack_.push_back(v); }
+-  void ReuseValue(const char *key, Value v) {
+-    Key(key);
+-    ReuseValue(v);
+-  }
+-
+-  // Overloaded Add that tries to call the correct function above.
+-  void Add(int8_t i) { Int(i); }
+-  void Add(int16_t i) { Int(i); }
+-  void Add(int32_t i) { Int(i); }
+-  void Add(int64_t i) { Int(i); }
+-  void Add(uint8_t u) { UInt(u); }
+-  void Add(uint16_t u) { UInt(u); }
+-  void Add(uint32_t u) { UInt(u); }
+-  void Add(uint64_t u) { UInt(u); }
+-  void Add(float f) { Float(f); }
+-  void Add(double d) { Double(d); }
+-  void Add(bool b) { Bool(b); }
+-  void Add(const char *str) { String(str); }
+-  void Add(const std::string &str) { String(str); }
+-  void Add(const flexbuffers::String &str) { String(str); }
+-
+-  template<typename T> void Add(const std::vector<T> &vec) { Vector(vec); }
+-
+-  template<typename T> void Add(const char *key, const T &t) {
+-    Key(key);
+-    Add(t);
+-  }
+-
+-  template<typename T> void Add(const std::map<std::string, T> &map) {
+-    Map(map);
+-  }
+-
+-  template<typename T> void operator+=(const T &t) { Add(t); }
+-
+-  // This function is useful in combination with the Mutate* functions above.
+-  // It forces elements of vectors and maps to have a minimum size, such that
+-  // they can later be updated without failing.
+-  // Call with no arguments to reset.
+-  void ForceMinimumBitWidth(BitWidth bw = BIT_WIDTH_8) {
+-    force_min_bit_width_ = bw;
+-  }
+-
+-  void Finish() {
+-    // If you hit this assert, you likely have objects that were never included
+-    // in a parent. You need to have exactly one root to finish a buffer.
+-    // Check your Start/End calls are matched, and all objects are inside
+-    // some other object.
+-    FLATBUFFERS_ASSERT(stack_.size() == 1);
+-
+-    // Write root value.
+-    auto byte_width = Align(stack_[0].ElemWidth(buf_.size(), 0));
+-    WriteAny(stack_[0], byte_width);
+-    // Write root type.
+-    Write(stack_[0].StoredPackedType(), 1);
+-    // Write root size. Normally determined by parent, but root has no parent :)
+-    Write(byte_width, 1);
+-
+-    finished_ = true;
+-  }
+-
+- private:
+-  void Finished() const {
+-    // If you get this assert, you're attempting to get access a buffer
+-    // which hasn't been finished yet. Be sure to call
+-    // Builder::Finish with your root object.
+-    FLATBUFFERS_ASSERT(finished_);
+-  }
+-
+-  // Align to prepare for writing a scalar with a certain size.
+-  uint8_t Align(BitWidth alignment) {
+-    auto byte_width = 1U << alignment;
+-    buf_.insert(buf_.end(), flatbuffers::PaddingBytes(buf_.size(), byte_width),
+-                0);
+-    return static_cast<uint8_t>(byte_width);
+-  }
+-
+-  void WriteBytes(const void *val, size_t size) {
+-    buf_.insert(buf_.end(), reinterpret_cast<const uint8_t *>(val),
+-                reinterpret_cast<const uint8_t *>(val) + size);
+-  }
+-
+-  template<typename T> void Write(T val, size_t byte_width) {
+-    FLATBUFFERS_ASSERT(sizeof(T) >= byte_width);
+-    val = flatbuffers::EndianScalar(val);
+-    WriteBytes(&val, byte_width);
+-  }
+-
+-  void WriteDouble(double f, uint8_t byte_width) {
+-    switch (byte_width) {
+-      case 8: Write(f, byte_width); break;
+-      case 4: Write(static_cast<float>(f), byte_width); break;
+-      // case 2: Write(static_cast<half>(f), byte_width); break;
+-      // case 1: Write(static_cast<quarter>(f), byte_width); break;
+-      default: FLATBUFFERS_ASSERT(0);
+-    }
+-  }
+-
+-  void WriteOffset(uint64_t o, uint8_t byte_width) {
+-    auto reloff = buf_.size() - o;
+-    FLATBUFFERS_ASSERT(byte_width == 8 || reloff < 1ULL << (byte_width * 8));
+-    Write(reloff, byte_width);
+-  }
+-
+-  template<typename T> void PushIndirect(T val, Type type, BitWidth bit_width) {
+-    auto byte_width = Align(bit_width);
+-    auto iloc = buf_.size();
+-    Write(val, byte_width);
+-    stack_.push_back(Value(static_cast<uint64_t>(iloc), type, bit_width));
+-  }
+-
+-  static BitWidth WidthB(size_t byte_width) {
+-    switch (byte_width) {
+-      case 1: return BIT_WIDTH_8;
+-      case 2: return BIT_WIDTH_16;
+-      case 4: return BIT_WIDTH_32;
+-      case 8: return BIT_WIDTH_64;
+-      default: FLATBUFFERS_ASSERT(false); return BIT_WIDTH_64;
+-    }
+-  }
+-
+-  template<typename T> static Type GetScalarType() {
+-    static_assert(flatbuffers::is_scalar<T>::value, "Unrelated types");
+-    return flatbuffers::is_floating_point<T>::value ? FBT_FLOAT
+-           : flatbuffers::is_same<T, bool>::value
+-               ? FBT_BOOL
+-               : (flatbuffers::is_unsigned<T>::value ? FBT_UINT : FBT_INT);
+-  }
+-
+- public:
+-  // This was really intended to be private, except for LastValue/ReuseValue.
+-  struct Value {
+-    union {
+-      int64_t i_;
+-      uint64_t u_;
+-      double f_;
+-    };
+-
+-    Type type_;
+-
+-    // For scalars: of itself, for vector: of its elements, for string: length.
+-    BitWidth min_bit_width_;
+-
+-    Value() : i_(0), type_(FBT_NULL), min_bit_width_(BIT_WIDTH_8) {}
+-
+-    Value(bool b)
+-        : u_(static_cast<uint64_t>(b)),
+-          type_(FBT_BOOL),
+-          min_bit_width_(BIT_WIDTH_8) {}
+-
+-    Value(int64_t i, Type t, BitWidth bw)
+-        : i_(i), type_(t), min_bit_width_(bw) {}
+-    Value(uint64_t u, Type t, BitWidth bw)
+-        : u_(u), type_(t), min_bit_width_(bw) {}
+-
+-    Value(float f)
+-        : f_(static_cast<double>(f)),
+-          type_(FBT_FLOAT),
+-          min_bit_width_(BIT_WIDTH_32) {}
+-    Value(double f) : f_(f), type_(FBT_FLOAT), min_bit_width_(WidthF(f)) {}
+-
+-    uint8_t StoredPackedType(BitWidth parent_bit_width_ = BIT_WIDTH_8) const {
+-      return PackedType(StoredWidth(parent_bit_width_), type_);
+-    }
+-
+-    BitWidth ElemWidth(size_t buf_size, size_t elem_index) const {
+-      if (IsInline(type_)) {
+-        return min_bit_width_;
+-      } else {
+-        // We have an absolute offset, but want to store a relative offset
+-        // elem_index elements beyond the current buffer end. Since whether
+-        // the relative offset fits in a certain byte_width depends on
+-        // the size of the elements before it (and their alignment), we have
+-        // to test for each size in turn.
+-        for (size_t byte_width = 1;
+-             byte_width <= sizeof(flatbuffers::largest_scalar_t);
+-             byte_width *= 2) {
+-          // Where are we going to write this offset?
+-          auto offset_loc = buf_size +
+-                            flatbuffers::PaddingBytes(buf_size, byte_width) +
+-                            elem_index * byte_width;
+-          // Compute relative offset.
+-          auto offset = offset_loc - u_;
+-          // Does it fit?
+-          auto bit_width = WidthU(offset);
+-          if (static_cast<size_t>(static_cast<size_t>(1U) << bit_width) ==
+-              byte_width)
+-            return bit_width;
+-        }
+-        FLATBUFFERS_ASSERT(false);  // Must match one of the sizes above.
+-        return BIT_WIDTH_64;
+-      }
+-    }
+-
+-    BitWidth StoredWidth(BitWidth parent_bit_width_ = BIT_WIDTH_8) const {
+-      if (IsInline(type_)) {
+-        return (std::max)(min_bit_width_, parent_bit_width_);
+-      } else {
+-        return min_bit_width_;
+-      }
+-    }
+-  };
+-
+- private:
+-  void WriteAny(const Value &val, uint8_t byte_width) {
+-    switch (val.type_) {
+-      case FBT_NULL:
+-      case FBT_INT: Write(val.i_, byte_width); break;
+-      case FBT_BOOL:
+-      case FBT_UINT: Write(val.u_, byte_width); break;
+-      case FBT_FLOAT: WriteDouble(val.f_, byte_width); break;
+-      default: WriteOffset(val.u_, byte_width); break;
+-    }
+-  }
+-
+-  size_t CreateBlob(const void *data, size_t len, size_t trailing, Type type) {
+-    auto bit_width = WidthU(len);
+-    auto byte_width = Align(bit_width);
+-    Write<uint64_t>(len, byte_width);
+-    auto sloc = buf_.size();
+-    WriteBytes(data, len + trailing);
+-    stack_.push_back(Value(static_cast<uint64_t>(sloc), type, bit_width));
+-    return sloc;
+-  }
+-
+-  template<typename T>
+-  size_t ScalarVector(const T *elems, size_t len, bool fixed) {
+-    auto vector_type = GetScalarType<T>();
+-    auto byte_width = sizeof(T);
+-    auto bit_width = WidthB(byte_width);
+-    // If you get this assert, you're trying to write a vector with a size
+-    // field that is bigger than the scalars you're trying to write (e.g. a
+-    // byte vector > 255 elements). For such types, write a "blob" instead.
+-    // TODO: instead of asserting, could write vector with larger elements
+-    // instead, though that would be wasteful.
+-    FLATBUFFERS_ASSERT(WidthU(len) <= bit_width);
+-    Align(bit_width);
+-    if (!fixed) Write<uint64_t>(len, byte_width);
+-    auto vloc = buf_.size();
+-    for (size_t i = 0; i < len; i++) Write(elems[i], byte_width);
+-    stack_.push_back(Value(static_cast<uint64_t>(vloc),
+-                           ToTypedVector(vector_type, fixed ? len : 0),
+-                           bit_width));
+-    return vloc;
+-  }
+-
+-  Value CreateVector(size_t start, size_t vec_len, size_t step, bool typed,
+-                     bool fixed, const Value *keys = nullptr) {
+-    FLATBUFFERS_ASSERT(
+-        !fixed ||
+-        typed);  // typed=false, fixed=true combination is not supported.
+-    // Figure out smallest bit width we can store this vector with.
+-    auto bit_width = (std::max)(force_min_bit_width_, WidthU(vec_len));
+-    auto prefix_elems = 1;
+-    if (keys) {
+-      // If this vector is part of a map, we will pre-fix an offset to the keys
+-      // to this vector.
+-      bit_width = (std::max)(bit_width, keys->ElemWidth(buf_.size(), 0));
+-      prefix_elems += 2;
+-    }
+-    Type vector_type = FBT_KEY;
+-    // Check bit widths and types for all elements.
+-    for (size_t i = start; i < stack_.size(); i += step) {
+-      auto elem_width =
+-          stack_[i].ElemWidth(buf_.size(), i - start + prefix_elems);
+-      bit_width = (std::max)(bit_width, elem_width);
+-      if (typed) {
+-        if (i == start) {
+-          vector_type = stack_[i].type_;
+-        } else {
+-          // If you get this assert, you are writing a typed vector with
+-          // elements that are not all the same type.
+-          FLATBUFFERS_ASSERT(vector_type == stack_[i].type_);
+-        }
+-      }
+-    }
+-    // If you get this assert, your typed types are not one of:
+-    // Int / UInt / Float / Key.
+-    FLATBUFFERS_ASSERT(!typed || IsTypedVectorElementType(vector_type));
+-    auto byte_width = Align(bit_width);
+-    // Write vector. First the keys width/offset if available, and size.
+-    if (keys) {
+-      WriteOffset(keys->u_, byte_width);
+-      Write<uint64_t>(1ULL << keys->min_bit_width_, byte_width);
+-    }
+-    if (!fixed) Write<uint64_t>(vec_len, byte_width);
+-    // Then the actual data.
+-    auto vloc = buf_.size();
+-    for (size_t i = start; i < stack_.size(); i += step) {
+-      WriteAny(stack_[i], byte_width);
+-    }
+-    // Then the types.
+-    if (!typed) {
+-      for (size_t i = start; i < stack_.size(); i += step) {
+-        buf_.push_back(stack_[i].StoredPackedType(bit_width));
+-      }
+-    }
+-    return Value(static_cast<uint64_t>(vloc),
+-                 keys ? FBT_MAP
+-                      : (typed ? ToTypedVector(vector_type, fixed ? vec_len : 0)
+-                               : FBT_VECTOR),
+-                 bit_width);
+-  }
+-
+-  // You shouldn't really be copying instances of this class.
+-  Builder(const Builder &);
+-  Builder &operator=(const Builder &);
+-
+-  std::vector<uint8_t> buf_;
+-  std::vector<Value> stack_;
+-
+-  bool finished_;
+-  bool has_duplicate_keys_;
+-
+-  BuilderFlag flags_;
+-
+-  BitWidth force_min_bit_width_;
+-
+-  struct KeyOffsetCompare {
+-    explicit KeyOffsetCompare(const std::vector<uint8_t> &buf) : buf_(&buf) {}
+-    bool operator()(size_t a, size_t b) const {
+-      auto stra = reinterpret_cast<const char *>(buf_->data() + a);
+-      auto strb = reinterpret_cast<const char *>(buf_->data() + b);
+-      return strcmp(stra, strb) < 0;
+-    }
+-    const std::vector<uint8_t> *buf_;
+-  };
+-
+-  typedef std::pair<size_t, size_t> StringOffset;
+-  struct StringOffsetCompare {
+-    explicit StringOffsetCompare(const std::vector<uint8_t> &buf)
+-        : buf_(&buf) {}
+-    bool operator()(const StringOffset &a, const StringOffset &b) const {
+-      auto stra = buf_->data() + a.first;
+-      auto strb = buf_->data() + b.first;
+-      auto cr = memcmp(stra, strb, (std::min)(a.second, b.second) + 1);
+-      return cr < 0 || (cr == 0 && a.second < b.second);
+-    }
+-    const std::vector<uint8_t> *buf_;
+-  };
+-
+-  typedef std::set<size_t, KeyOffsetCompare> KeyOffsetMap;
+-  typedef std::set<StringOffset, StringOffsetCompare> StringOffsetMap;
+-
+-  KeyOffsetMap key_pool;
+-  StringOffsetMap string_pool;
+-
+-  friend class Verifier;
+-};
+-
+-// Helper class to verify the integrity of a FlexBuffer
+-class Verifier FLATBUFFERS_FINAL_CLASS {
+- public:
+-  Verifier(const uint8_t *buf, size_t buf_len,
+-           // Supplying this vector likely results in faster verification
+-           // of larger buffers with many shared keys/strings, but
+-           // comes at the cost of using additional memory the same size of
+-           // the buffer being verified, so it is by default off.
+-           std::vector<uint8_t> *reuse_tracker = nullptr,
+-           bool _check_alignment = true, size_t max_depth = 64)
+-      : buf_(buf),
+-        size_(buf_len),
+-        depth_(0),
+-        max_depth_(max_depth),
+-        num_vectors_(0),
+-        max_vectors_(buf_len),
+-        check_alignment_(_check_alignment),
+-        reuse_tracker_(reuse_tracker) {
+-    FLATBUFFERS_ASSERT(size_ < FLATBUFFERS_MAX_BUFFER_SIZE);
+-    if (reuse_tracker_) {
+-      reuse_tracker_->clear();
+-      reuse_tracker_->resize(size_, PackedType(BIT_WIDTH_8, FBT_NULL));
+-    }
+-  }
+-
+- private:
+-  // Central location where any verification failures register.
+-  bool Check(bool ok) const {
+-    // clang-format off
+-    #ifdef FLATBUFFERS_DEBUG_VERIFICATION_FAILURE
+-      FLATBUFFERS_ASSERT(ok);
+-    #endif
+-    // clang-format on
+-    return ok;
+-  }
+-
+-  // Verify any range within the buffer.
+-  bool VerifyFrom(size_t elem, size_t elem_len) const {
+-    return Check(elem_len < size_ && elem <= size_ - elem_len);
+-  }
+-  bool VerifyBefore(size_t elem, size_t elem_len) const {
+-    return Check(elem_len <= elem);
+-  }
+-
+-  bool VerifyFromPointer(const uint8_t *p, size_t len) {
+-    auto o = static_cast<size_t>(p - buf_);
+-    return VerifyFrom(o, len);
+-  }
+-  bool VerifyBeforePointer(const uint8_t *p, size_t len) {
+-    auto o = static_cast<size_t>(p - buf_);
+-    return VerifyBefore(o, len);
+-  }
+-
+-  bool VerifyByteWidth(size_t width) {
+-    return Check(width == 1 || width == 2 || width == 4 || width == 8);
+-  }
+-
+-  bool VerifyType(int type) { return Check(type >= 0 && type < FBT_MAX_TYPE); }
+-
+-  bool VerifyOffset(uint64_t off, const uint8_t *p) {
+-    return Check(off <= static_cast<uint64_t>(size_)) &&
+-           off <= static_cast<uint64_t>(p - buf_);
+-  }
+-
+-  bool VerifyAlignment(const uint8_t *p, size_t size) const {
+-    auto o = static_cast<size_t>(p - buf_);
+-    return Check((o & (size - 1)) == 0 || !check_alignment_);
+-  }
+-
+-// Macro, since we want to escape from parent function & use lazy args.
+-#define FLEX_CHECK_VERIFIED(P, PACKED_TYPE)                     \
+-  if (reuse_tracker_) {                                         \
+-    auto packed_type = PACKED_TYPE;                             \
+-    auto existing = (*reuse_tracker_)[P - buf_];                \
+-    if (existing == packed_type) return true;                   \
+-    /* Fail verification if already set with different type! */ \
+-    if (!Check(existing == 0)) return false;                    \
+-    (*reuse_tracker_)[P - buf_] = packed_type;                  \
+-  }
+-
+-  bool VerifyVector(Reference r, const uint8_t *p, Type elem_type) {
+-    // Any kind of nesting goes thru this function, so guard against that
+-    // here, both with simple nesting checks, and the reuse tracker if on.
+-    depth_++;
+-    num_vectors_++;
+-    if (!Check(depth_ <= max_depth_ && num_vectors_ <= max_vectors_))
+-      return false;
+-    auto size_byte_width = r.byte_width_;
+-    FLEX_CHECK_VERIFIED(p,
+-                        PackedType(Builder::WidthB(size_byte_width), r.type_));
+-    if (!VerifyBeforePointer(p, size_byte_width)) return false;
+-    auto sized = Sized(p, size_byte_width);
+-    auto num_elems = sized.size();
+-    auto elem_byte_width = r.type_ == FBT_STRING || r.type_ == FBT_BLOB
+-                               ? uint8_t(1)
+-                               : r.byte_width_;
+-    auto max_elems = SIZE_MAX / elem_byte_width;
+-    if (!Check(num_elems < max_elems))
+-      return false;  // Protect against byte_size overflowing.
+-    auto byte_size = num_elems * elem_byte_width;
+-    if (!VerifyFromPointer(p, byte_size)) return false;
+-    if (elem_type == FBT_NULL) {
+-      // Verify type bytes after the vector.
+-      if (!VerifyFromPointer(p + byte_size, num_elems)) return false;
+-      auto v = Vector(p, size_byte_width);
+-      for (size_t i = 0; i < num_elems; i++)
+-        if (!VerifyRef(v[i])) return false;
+-    } else if (elem_type == FBT_KEY) {
+-      auto v = TypedVector(p, elem_byte_width, FBT_KEY);
+-      for (size_t i = 0; i < num_elems; i++)
+-        if (!VerifyRef(v[i])) return false;
+-    } else {
+-      FLATBUFFERS_ASSERT(IsInline(elem_type));
+-    }
+-    depth_--;
+-    return true;
+-  }
+-
+-  bool VerifyKeys(const uint8_t *p, uint8_t byte_width) {
+-    // The vector part of the map has already been verified.
+-    const size_t num_prefixed_fields = 3;
+-    if (!VerifyBeforePointer(p, byte_width * num_prefixed_fields)) return false;
+-    p -= byte_width * num_prefixed_fields;
+-    auto off = ReadUInt64(p, byte_width);
+-    if (!VerifyOffset(off, p)) return false;
+-    auto key_byte_with =
+-        static_cast<uint8_t>(ReadUInt64(p + byte_width, byte_width));
+-    if (!VerifyByteWidth(key_byte_with)) return false;
+-    return VerifyVector(Reference(p, byte_width, key_byte_with, FBT_VECTOR_KEY),
+-                        p - off, FBT_KEY);
+-  }
+-
+-  bool VerifyKey(const uint8_t *p) {
+-    FLEX_CHECK_VERIFIED(p, PackedType(BIT_WIDTH_8, FBT_KEY));
+-    while (p < buf_ + size_)
+-      if (*p++) return true;
+-    return false;
+-  }
+-
+-#undef FLEX_CHECK_VERIFIED
+-
+-  bool VerifyTerminator(const String &s) {
+-    return VerifyFromPointer(reinterpret_cast<const uint8_t *>(s.c_str()),
+-                             s.size() + 1);
+-  }
+-
+-  bool VerifyRef(Reference r) {
+-    // r.parent_width_ and r.data_ already verified.
+-    if (!VerifyByteWidth(r.byte_width_) || !VerifyType(r.type_)) {
+-      return false;
+-    }
+-    if (IsInline(r.type_)) {
+-      // Inline scalars, don't require further verification.
+-      return true;
+-    }
+-    // All remaining types are an offset.
+-    auto off = ReadUInt64(r.data_, r.parent_width_);
+-    if (!VerifyOffset(off, r.data_)) return false;
+-    auto p = r.Indirect();
+-    if (!VerifyAlignment(p, r.byte_width_)) return false;
+-    switch (r.type_) {
+-      case FBT_INDIRECT_INT:
+-      case FBT_INDIRECT_UINT:
+-      case FBT_INDIRECT_FLOAT: return VerifyFromPointer(p, r.byte_width_);
+-      case FBT_KEY: return VerifyKey(p);
+-      case FBT_MAP:
+-        return VerifyVector(r, p, FBT_NULL) && VerifyKeys(p, r.byte_width_);
+-      case FBT_VECTOR: return VerifyVector(r, p, FBT_NULL);
+-      case FBT_VECTOR_INT: return VerifyVector(r, p, FBT_INT);
+-      case FBT_VECTOR_BOOL:
+-      case FBT_VECTOR_UINT: return VerifyVector(r, p, FBT_UINT);
+-      case FBT_VECTOR_FLOAT: return VerifyVector(r, p, FBT_FLOAT);
+-      case FBT_VECTOR_KEY: return VerifyVector(r, p, FBT_KEY);
+-      case FBT_VECTOR_STRING_DEPRECATED:
+-        // Use of FBT_KEY here intentional, see elsewhere.
+-        return VerifyVector(r, p, FBT_KEY);
+-      case FBT_BLOB: return VerifyVector(r, p, FBT_UINT);
+-      case FBT_STRING:
+-        return VerifyVector(r, p, FBT_UINT) &&
+-               VerifyTerminator(String(p, r.byte_width_));
+-      case FBT_VECTOR_INT2:
+-      case FBT_VECTOR_UINT2:
+-      case FBT_VECTOR_FLOAT2:
+-      case FBT_VECTOR_INT3:
+-      case FBT_VECTOR_UINT3:
+-      case FBT_VECTOR_FLOAT3:
+-      case FBT_VECTOR_INT4:
+-      case FBT_VECTOR_UINT4:
+-      case FBT_VECTOR_FLOAT4: {
+-        uint8_t len = 0;
+-        auto vtype = ToFixedTypedVectorElementType(r.type_, &len);
+-        if (!VerifyType(vtype)) return false;
+-        return VerifyFromPointer(p, r.byte_width_ * len);
+-      }
+-      default: return false;
+-    }
+-  }
+-
+- public:
+-  bool VerifyBuffer() {
+-    if (!Check(size_ >= 3)) return false;
+-    auto end = buf_ + size_;
+-    auto byte_width = *--end;
+-    auto packed_type = *--end;
+-    return VerifyByteWidth(byte_width) && Check(end - buf_ >= byte_width) &&
+-           VerifyRef(Reference(end - byte_width, byte_width, packed_type));
+-  }
+-
+- private:
+-  const uint8_t *buf_;
+-  size_t size_;
+-  size_t depth_;
+-  const size_t max_depth_;
+-  size_t num_vectors_;
+-  const size_t max_vectors_;
+-  bool check_alignment_;
+-  std::vector<uint8_t> *reuse_tracker_;
+-};
+-
+-// Utility function that contructs the Verifier for you, see above for
+-// parameters.
+-inline bool VerifyBuffer(const uint8_t *buf, size_t buf_len,
+-                         std::vector<uint8_t> *reuse_tracker = nullptr) {
+-  Verifier verifier(buf, buf_len, reuse_tracker);
+-  return verifier.VerifyBuffer();
+-}
+-
+-#ifdef FLATBUFFERS_H_
+-// This is a verifier utility function that works together with the
+-// FlatBuffers verifier, which should only be present if flatbuffer.h
+-// has been included (which it typically is in generated code).
+-inline bool VerifyNestedFlexBuffer(const flatbuffers::Vector<uint8_t> *nv,
+-                                   flatbuffers::Verifier &verifier) {
+-  if (!nv) return true;
+-  return verifier.Check(flexbuffers::VerifyBuffer(
+-      nv->data(), nv->size(), verifier.GetFlexReuseTracker()));
+-}
+-#endif
+-
+-}  // namespace flexbuffers
+-
+-#if defined(_MSC_VER)
+-#  pragma warning(pop)
+-#endif
+-
+-#endif  // FLATBUFFERS_FLEXBUFFERS_H_
diff --git a/third_party/tflite-micro/third_party/flatbuffers/include/flatbuffers/util.h b/third_party/tflite-micro/third_party/flatbuffers/include/flatbuffers/util.h
index 5a4bfe52..690e63d3 100644
--- a/third_party/tflite-micro/third_party/flatbuffers/include/flatbuffers/util.h
+++ b/third_party/tflite-micro/third_party/flatbuffers/include/flatbuffers/util.h
@@ -23,12 +23,6 @@
 #include "flatbuffers/base.h"
 #include "flatbuffers/stl_emulation.h"
 
-// For TFLM we always want to use FLATBUFFERS_PREFER_PRINTF=1. See
-// http://b/211811553 for more context.
-#ifndef FLATBUFFERS_PREFER_PRINTF
-#define FLATBUFFERS_PREFER_PRINTF 1
-#endif
-
 #ifndef FLATBUFFERS_PREFER_PRINTF
 #  include <iomanip>
 #  include <sstream>
@@ -454,9 +448,6 @@ std::string StripPath(const std::string &filepath);
 // Strip the last component of the path + separator.
 std::string StripFileName(const std::string &filepath);
 
-std::string StripPrefix(const std::string &filepath,
-                        const std::string &prefix_to_remove);
-
 // Concatenates a path with a filename, regardless of whether the path
 // ends in a separator or not.
 std::string ConCatPathFileName(const std::string &path,
@@ -694,32 +685,6 @@ bool ReadEnvironmentVariable(const char *var_name,
 // MSVC specific: Send all assert reports to STDOUT to prevent CI hangs.
 void SetupDefaultCRTReportMode();
 
-enum class Case {
-  kUnknown = 0,
-  // TheQuickBrownFox
-  kUpperCamel = 1,
-  // theQuickBrownFox
-  kLowerCamel = 2,
-  // the_quick_brown_fox
-  kSnake = 3,
-  // THE_QUICK_BROWN_FOX
-  kScreamingSnake = 4,
-  // THEQUICKBROWNFOX
-  kAllUpper = 5,
-  // thequickbrownfox
-  kAllLower = 6,
-  // the-quick-brown-fox
-  kDasher = 7,
-  // THEQuiCKBr_ownFox (or whatever you want, we won't change it)
-  kKeep = 8,
-  // the_quick_brown_fox123 (as opposed to the_quick_brown_fox_123)
-  kSnake2 = 9,
-};
-
-// Convert the `input` string of case `input_case` to the specified `output_case`.
-std::string ConvertCase(const std::string &input, Case output_case,
-                    Case input_case = Case::kSnake);
-
 }  // namespace flatbuffers
 
 #endif  // FLATBUFFERS_UTIL_H_
diff --git a/third_party/tflite-micro/third_party/flatbuffers/include/flatbuffers/util.h.orig b/third_party/tflite-micro/third_party/flatbuffers/include/flatbuffers/util.h.orig
new file mode 100644
index 00000000..5a4bfe52
--- /dev/null
+++ b/third_party/tflite-micro/third_party/flatbuffers/include/flatbuffers/util.h.orig
@@ -0,0 +1,725 @@
+/*
+ * Copyright 2014 Google Inc. All rights reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FLATBUFFERS_UTIL_H_
+#define FLATBUFFERS_UTIL_H_
+
+#include <ctype.h>
+#include <errno.h>
+
+#include "flatbuffers/base.h"
+#include "flatbuffers/stl_emulation.h"
+
+// For TFLM we always want to use FLATBUFFERS_PREFER_PRINTF=1. See
+// http://b/211811553 for more context.
+#ifndef FLATBUFFERS_PREFER_PRINTF
+#define FLATBUFFERS_PREFER_PRINTF 1
+#endif
+
+#ifndef FLATBUFFERS_PREFER_PRINTF
+#  include <iomanip>
+#  include <sstream>
+#else  // FLATBUFFERS_PREFER_PRINTF
+#  include <float.h>
+#  include <stdio.h>
+#endif  // FLATBUFFERS_PREFER_PRINTF
+
+#include <string>
+
+namespace flatbuffers {
+
+// @locale-independent functions for ASCII characters set.
+
+// Fast checking that character lies in closed range: [a <= x <= b]
+// using one compare (conditional branch) operator.
+inline bool check_ascii_range(char x, char a, char b) {
+  FLATBUFFERS_ASSERT(a <= b);
+  // (Hacker's Delight): `a <= x <= b` <=> `(x-a) <={u} (b-a)`.
+  // The x, a, b will be promoted to int and subtracted without overflow.
+  return static_cast<unsigned int>(x - a) <= static_cast<unsigned int>(b - a);
+}
+
+// Case-insensitive isalpha
+inline bool is_alpha(char c) {
+  // ASCII only: alpha to upper case => reset bit 0x20 (~0x20 = 0xDF).
+  return check_ascii_range(c & 0xDF, 'a' & 0xDF, 'z' & 0xDF);
+}
+
+// Check for uppercase alpha
+inline bool is_alpha_upper(char c) { return check_ascii_range(c, 'A', 'Z'); }
+
+// Check (case-insensitive) that `c` is equal to alpha.
+inline bool is_alpha_char(char c, char alpha) {
+  FLATBUFFERS_ASSERT(is_alpha(alpha));
+  // ASCII only: alpha to upper case => reset bit 0x20 (~0x20 = 0xDF).
+  return ((c & 0xDF) == (alpha & 0xDF));
+}
+
+// https://en.cppreference.com/w/cpp/string/byte/isxdigit
+// isdigit and isxdigit are the only standard narrow character classification
+// functions that are not affected by the currently installed C locale. although
+// some implementations (e.g. Microsoft in 1252 codepage) may classify
+// additional single-byte characters as digits.
+inline bool is_digit(char c) { return check_ascii_range(c, '0', '9'); }
+
+inline bool is_xdigit(char c) {
+  // Replace by look-up table.
+  return is_digit(c) || check_ascii_range(c & 0xDF, 'a' & 0xDF, 'f' & 0xDF);
+}
+
+// Case-insensitive isalnum
+inline bool is_alnum(char c) { return is_alpha(c) || is_digit(c); }
+
+inline char CharToUpper(char c) {
+  return static_cast<char>(::toupper(static_cast<unsigned char>(c)));
+}
+
+inline char CharToLower(char c) {
+  return static_cast<char>(::tolower(static_cast<unsigned char>(c)));
+}
+
+// @end-locale-independent functions for ASCII character set
+
+#ifdef FLATBUFFERS_PREFER_PRINTF
+template<typename T> size_t IntToDigitCount(T t) {
+  size_t digit_count = 0;
+  // Count the sign for negative numbers
+  if (t < 0) digit_count++;
+  // Count a single 0 left of the dot for fractional numbers
+  if (-1 < t && t < 1) digit_count++;
+  // Count digits until fractional part
+  T eps = std::numeric_limits<T>::epsilon();
+  while (t <= (-1 + eps) || (1 - eps) <= t) {
+    t /= 10;
+    digit_count++;
+  }
+  return digit_count;
+}
+
+template<typename T> size_t NumToStringWidth(T t, int precision = 0) {
+  size_t string_width = IntToDigitCount(t);
+  // Count the dot for floating point numbers
+  if (precision) string_width += (precision + 1);
+  return string_width;
+}
+
+template<typename T>
+std::string NumToStringImplWrapper(T t, const char *fmt, int precision = 0) {
+  size_t string_width = NumToStringWidth(t, precision);
+  std::string s(string_width, 0x00);
+  // Allow snprintf to use std::string trailing null to detect buffer overflow
+  snprintf(const_cast<char *>(s.data()), (s.size() + 1), fmt, string_width, t);
+  return s;
+}
+#endif  // FLATBUFFERS_PREFER_PRINTF
+
+// Convert an integer or floating point value to a string.
+// In contrast to std::stringstream, "char" values are
+// converted to a string of digits, and we don't use scientific notation.
+template<typename T> std::string NumToString(T t) {
+  // clang-format off
+
+  #ifndef FLATBUFFERS_PREFER_PRINTF
+    std::stringstream ss;
+    ss << t;
+    return ss.str();
+  #else // FLATBUFFERS_PREFER_PRINTF
+    auto v = static_cast<long long>(t);
+    return NumToStringImplWrapper(v, "%.*lld");
+  #endif // FLATBUFFERS_PREFER_PRINTF
+  // clang-format on
+}
+// Avoid char types used as character data.
+template<> inline std::string NumToString<signed char>(signed char t) {
+  return NumToString(static_cast<int>(t));
+}
+template<> inline std::string NumToString<unsigned char>(unsigned char t) {
+  return NumToString(static_cast<int>(t));
+}
+template<> inline std::string NumToString<char>(char t) {
+  return NumToString(static_cast<int>(t));
+}
+
+// Special versions for floats/doubles.
+template<typename T> std::string FloatToString(T t, int precision) {
+  // clang-format off
+
+  #ifndef FLATBUFFERS_PREFER_PRINTF
+    // to_string() prints different numbers of digits for floats depending on
+    // platform and isn't available on Android, so we use stringstream
+    std::stringstream ss;
+    // Use std::fixed to suppress scientific notation.
+    ss << std::fixed;
+    // Default precision is 6, we want that to be higher for doubles.
+    ss << std::setprecision(precision);
+    ss << t;
+    auto s = ss.str();
+  #else // FLATBUFFERS_PREFER_PRINTF
+    auto v = static_cast<double>(t);
+    auto s = NumToStringImplWrapper(v, "%0.*f", precision);
+  #endif // FLATBUFFERS_PREFER_PRINTF
+  // clang-format on
+  // Sadly, std::fixed turns "1" into "1.00000", so here we undo that.
+  auto p = s.find_last_not_of('0');
+  if (p != std::string::npos) {
+    // Strip trailing zeroes. If it is a whole number, keep one zero.
+    s.resize(p + (s[p] == '.' ? 2 : 1));
+  }
+  return s;
+}
+
+template<> inline std::string NumToString<double>(double t) {
+  return FloatToString(t, 12);
+}
+template<> inline std::string NumToString<float>(float t) {
+  return FloatToString(t, 6);
+}
+
+// Convert an integer value to a hexadecimal string.
+// The returned string length is always xdigits long, prefixed by 0 digits.
+// For example, IntToStringHex(0x23, 8) returns the string "00000023".
+inline std::string IntToStringHex(int i, int xdigits) {
+  FLATBUFFERS_ASSERT(i >= 0);
+  // clang-format off
+
+  #ifndef FLATBUFFERS_PREFER_PRINTF
+    std::stringstream ss;
+    ss << std::setw(xdigits) << std::setfill('0') << std::hex << std::uppercase
+       << i;
+    return ss.str();
+  #else // FLATBUFFERS_PREFER_PRINTF
+    return NumToStringImplWrapper(i, "%.*X", xdigits);
+  #endif // FLATBUFFERS_PREFER_PRINTF
+  // clang-format on
+}
+
+// clang-format off
+// Use locale independent functions {strtod_l, strtof_l, strtoll_l, strtoull_l}.
+#if defined(FLATBUFFERS_LOCALE_INDEPENDENT) && (FLATBUFFERS_LOCALE_INDEPENDENT > 0)
+  class ClassicLocale {
+    #ifdef _MSC_VER
+      typedef _locale_t locale_type;
+    #else
+      typedef locale_t locale_type;  // POSIX.1-2008 locale_t type
+    #endif
+    ClassicLocale();
+    ~ClassicLocale();
+    locale_type locale_;
+    static ClassicLocale instance_;
+  public:
+    static locale_type Get() { return instance_.locale_; }
+  };
+
+  #ifdef _MSC_VER
+    #define __strtoull_impl(s, pe, b) _strtoui64_l(s, pe, b, ClassicLocale::Get())
+    #define __strtoll_impl(s, pe, b) _strtoi64_l(s, pe, b, ClassicLocale::Get())
+    #define __strtod_impl(s, pe) _strtod_l(s, pe, ClassicLocale::Get())
+    #define __strtof_impl(s, pe) _strtof_l(s, pe, ClassicLocale::Get())
+  #else
+    #define __strtoull_impl(s, pe, b) strtoull_l(s, pe, b, ClassicLocale::Get())
+    #define __strtoll_impl(s, pe, b) strtoll_l(s, pe, b, ClassicLocale::Get())
+    #define __strtod_impl(s, pe) strtod_l(s, pe, ClassicLocale::Get())
+    #define __strtof_impl(s, pe) strtof_l(s, pe, ClassicLocale::Get())
+  #endif
+#else
+  #define __strtod_impl(s, pe) strtod(s, pe)
+  #define __strtof_impl(s, pe) static_cast<float>(strtod(s, pe))
+  #ifdef _MSC_VER
+    #define __strtoull_impl(s, pe, b) _strtoui64(s, pe, b)
+    #define __strtoll_impl(s, pe, b) _strtoi64(s, pe, b)
+  #else
+    #define __strtoull_impl(s, pe, b) strtoull(s, pe, b)
+    #define __strtoll_impl(s, pe, b) strtoll(s, pe, b)
+  #endif
+#endif
+
+inline void strtoval_impl(int64_t *val, const char *str, char **endptr,
+                                 int base) {
+    *val = __strtoll_impl(str, endptr, base);
+}
+
+inline void strtoval_impl(uint64_t *val, const char *str, char **endptr,
+                                 int base) {
+  *val = __strtoull_impl(str, endptr, base);
+}
+
+inline void strtoval_impl(double *val, const char *str, char **endptr) {
+  *val = __strtod_impl(str, endptr);
+}
+
+// UBSAN: double to float is safe if numeric_limits<float>::is_iec559 is true.
+__supress_ubsan__("float-cast-overflow")
+inline void strtoval_impl(float *val, const char *str, char **endptr) {
+  *val = __strtof_impl(str, endptr);
+}
+#undef __strtoull_impl
+#undef __strtoll_impl
+#undef __strtod_impl
+#undef __strtof_impl
+// clang-format on
+
+// Adaptor for strtoull()/strtoll().
+// Flatbuffers accepts numbers with any count of leading zeros (-009 is -9),
+// while strtoll with base=0 interprets first leading zero as octal prefix.
+// In future, it is possible to add prefixed 0b0101.
+// 1) Checks errno code for overflow condition (out of range).
+// 2) If base <= 0, function try to detect base of number by prefix.
+//
+// Return value (like strtoull and strtoll, but reject partial result):
+// - If successful, an integer value corresponding to the str is returned.
+// - If full string conversion can't be performed, 0 is returned.
+// - If the converted value falls out of range of corresponding return type, a
+// range error occurs. In this case value MAX(T)/MIN(T) is returned.
+template<typename T>
+inline bool StringToIntegerImpl(T *val, const char *const str,
+                                const int base = 0,
+                                const bool check_errno = true) {
+  // T is int64_t or uint64_T
+  FLATBUFFERS_ASSERT(str);
+  if (base <= 0) {
+    auto s = str;
+    while (*s && !is_digit(*s)) s++;
+    if (s[0] == '0' && is_alpha_char(s[1], 'X'))
+      return StringToIntegerImpl(val, str, 16, check_errno);
+    // if a prefix not match, try base=10
+    return StringToIntegerImpl(val, str, 10, check_errno);
+  } else {
+    if (check_errno) errno = 0;  // clear thread-local errno
+    auto endptr = str;
+    strtoval_impl(val, str, const_cast<char **>(&endptr), base);
+    if ((*endptr != '\0') || (endptr == str)) {
+      *val = 0;      // erase partial result
+      return false;  // invalid string
+    }
+    // errno is out-of-range, return MAX/MIN
+    if (check_errno && errno) return false;
+    return true;
+  }
+}
+
+template<typename T>
+inline bool StringToFloatImpl(T *val, const char *const str) {
+  // Type T must be either float or double.
+  FLATBUFFERS_ASSERT(str && val);
+  auto end = str;
+  strtoval_impl(val, str, const_cast<char **>(&end));
+  auto done = (end != str) && (*end == '\0');
+  if (!done) *val = 0;  // erase partial result
+  return done;
+}
+
+// Convert a string to an instance of T.
+// Return value (matched with StringToInteger64Impl and strtod):
+// - If successful, a numeric value corresponding to the str is returned.
+// - If full string conversion can't be performed, 0 is returned.
+// - If the converted value falls out of range of corresponding return type, a
+// range error occurs. In this case value MAX(T)/MIN(T) is returned.
+template<typename T> inline bool StringToNumber(const char *s, T *val) {
+  // Assert on `unsigned long` and `signed long` on LP64.
+  // If it is necessary, it could be solved with flatbuffers::enable_if<B,T>.
+  static_assert(sizeof(T) < sizeof(int64_t), "unexpected type T");
+  FLATBUFFERS_ASSERT(s && val);
+  int64_t i64;
+  // The errno check isn't needed, will return MAX/MIN on overflow.
+  if (StringToIntegerImpl(&i64, s, 0, false)) {
+    const int64_t max = (flatbuffers::numeric_limits<T>::max)();
+    const int64_t min = flatbuffers::numeric_limits<T>::lowest();
+    if (i64 > max) {
+      *val = static_cast<T>(max);
+      return false;
+    }
+    if (i64 < min) {
+      // For unsigned types return max to distinguish from
+      // "no conversion can be performed" when 0 is returned.
+      *val = static_cast<T>(flatbuffers::is_unsigned<T>::value ? max : min);
+      return false;
+    }
+    *val = static_cast<T>(i64);
+    return true;
+  }
+  *val = 0;
+  return false;
+}
+
+template<> inline bool StringToNumber<int64_t>(const char *str, int64_t *val) {
+  return StringToIntegerImpl(val, str);
+}
+
+template<>
+inline bool StringToNumber<uint64_t>(const char *str, uint64_t *val) {
+  if (!StringToIntegerImpl(val, str)) return false;
+  // The strtoull accepts negative numbers:
+  // If the minus sign was part of the input sequence, the numeric value
+  // calculated from the sequence of digits is negated as if by unary minus
+  // in the result type, which applies unsigned integer wraparound rules.
+  // Fix this behaviour (except -0).
+  if (*val) {
+    auto s = str;
+    while (*s && !is_digit(*s)) s++;
+    s = (s > str) ? (s - 1) : s;  // step back to one symbol
+    if (*s == '-') {
+      // For unsigned types return the max to distinguish from
+      // "no conversion can be performed".
+      *val = (flatbuffers::numeric_limits<uint64_t>::max)();
+      return false;
+    }
+  }
+  return true;
+}
+
+template<> inline bool StringToNumber(const char *s, float *val) {
+  return StringToFloatImpl(val, s);
+}
+
+template<> inline bool StringToNumber(const char *s, double *val) {
+  return StringToFloatImpl(val, s);
+}
+
+inline int64_t StringToInt(const char *s, int base = 10) {
+  int64_t val;
+  return StringToIntegerImpl(&val, s, base) ? val : 0;
+}
+
+inline uint64_t StringToUInt(const char *s, int base = 10) {
+  uint64_t val;
+  return StringToIntegerImpl(&val, s, base) ? val : 0;
+}
+
+typedef bool (*LoadFileFunction)(const char *filename, bool binary,
+                                 std::string *dest);
+typedef bool (*FileExistsFunction)(const char *filename);
+
+LoadFileFunction SetLoadFileFunction(LoadFileFunction load_file_function);
+
+FileExistsFunction SetFileExistsFunction(
+    FileExistsFunction file_exists_function);
+
+// Check if file "name" exists.
+bool FileExists(const char *name);
+
+// Check if "name" exists and it is also a directory.
+bool DirExists(const char *name);
+
+// Load file "name" into "buf" returning true if successful
+// false otherwise.  If "binary" is false data is read
+// using ifstream's text mode, otherwise data is read with
+// no transcoding.
+bool LoadFile(const char *name, bool binary, std::string *buf);
+
+// Save data "buf" of length "len" bytes into a file
+// "name" returning true if successful, false otherwise.
+// If "binary" is false data is written using ifstream's
+// text mode, otherwise data is written with no
+// transcoding.
+bool SaveFile(const char *name, const char *buf, size_t len, bool binary);
+
+// Save data "buf" into file "name" returning true if
+// successful, false otherwise.  If "binary" is false
+// data is written using ifstream's text mode, otherwise
+// data is written with no transcoding.
+inline bool SaveFile(const char *name, const std::string &buf, bool binary) {
+  return SaveFile(name, buf.c_str(), buf.size(), binary);
+}
+
+// Functionality for minimalistic portable path handling.
+
+// The functions below behave correctly regardless of whether posix ('/') or
+// Windows ('/' or '\\') separators are used.
+
+// Any new separators inserted are always posix.
+FLATBUFFERS_CONSTEXPR char kPathSeparator = '/';
+
+// Returns the path with the extension, if any, removed.
+std::string StripExtension(const std::string &filepath);
+
+// Returns the extension, if any.
+std::string GetExtension(const std::string &filepath);
+
+// Return the last component of the path, after the last separator.
+std::string StripPath(const std::string &filepath);
+
+// Strip the last component of the path + separator.
+std::string StripFileName(const std::string &filepath);
+
+std::string StripPrefix(const std::string &filepath,
+                        const std::string &prefix_to_remove);
+
+// Concatenates a path with a filename, regardless of whether the path
+// ends in a separator or not.
+std::string ConCatPathFileName(const std::string &path,
+                               const std::string &filename);
+
+// Replaces any '\\' separators with '/'
+std::string PosixPath(const char *path);
+std::string PosixPath(const std::string &path);
+
+// This function ensure a directory exists, by recursively
+// creating dirs for any parts of the path that don't exist yet.
+void EnsureDirExists(const std::string &filepath);
+
+// Obtains the absolute path from any other path.
+// Returns the input path if the absolute path couldn't be resolved.
+std::string AbsolutePath(const std::string &filepath);
+
+// Returns files relative to the --project_root path, prefixed with `//`.
+std::string RelativeToRootPath(const std::string &project,
+                               const std::string &filepath);
+
+// To and from UTF-8 unicode conversion functions
+
+// Convert a unicode code point into a UTF-8 representation by appending it
+// to a string. Returns the number of bytes generated.
+inline int ToUTF8(uint32_t ucc, std::string *out) {
+  FLATBUFFERS_ASSERT(!(ucc & 0x80000000));  // Top bit can't be set.
+  // 6 possible encodings: http://en.wikipedia.org/wiki/UTF-8
+  for (int i = 0; i < 6; i++) {
+    // Max bits this encoding can represent.
+    uint32_t max_bits = 6 + i * 5 + static_cast<int>(!i);
+    if (ucc < (1u << max_bits)) {  // does it fit?
+      // Remaining bits not encoded in the first byte, store 6 bits each
+      uint32_t remain_bits = i * 6;
+      // Store first byte:
+      (*out) += static_cast<char>((0xFE << (max_bits - remain_bits)) |
+                                  (ucc >> remain_bits));
+      // Store remaining bytes:
+      for (int j = i - 1; j >= 0; j--) {
+        (*out) += static_cast<char>(((ucc >> (j * 6)) & 0x3F) | 0x80);
+      }
+      return i + 1;  // Return the number of bytes added.
+    }
+  }
+  FLATBUFFERS_ASSERT(0);  // Impossible to arrive here.
+  return -1;
+}
+
+// Converts whatever prefix of the incoming string corresponds to a valid
+// UTF-8 sequence into a unicode code. The incoming pointer will have been
+// advanced past all bytes parsed.
+// returns -1 upon corrupt UTF-8 encoding (ignore the incoming pointer in
+// this case).
+inline int FromUTF8(const char **in) {
+  int len = 0;
+  // Count leading 1 bits.
+  for (int mask = 0x80; mask >= 0x04; mask >>= 1) {
+    if (**in & mask) {
+      len++;
+    } else {
+      break;
+    }
+  }
+  if ((static_cast<unsigned char>(**in) << len) & 0x80)
+    return -1;  // Bit after leading 1's must be 0.
+  if (!len) return *(*in)++;
+  // UTF-8 encoded values with a length are between 2 and 4 bytes.
+  if (len < 2 || len > 4) { return -1; }
+  // Grab initial bits of the code.
+  int ucc = *(*in)++ & ((1 << (7 - len)) - 1);
+  for (int i = 0; i < len - 1; i++) {
+    if ((**in & 0xC0) != 0x80) return -1;  // Upper bits must 1 0.
+    ucc <<= 6;
+    ucc |= *(*in)++ & 0x3F;  // Grab 6 more bits of the code.
+  }
+  // UTF-8 cannot encode values between 0xD800 and 0xDFFF (reserved for
+  // UTF-16 surrogate pairs).
+  if (ucc >= 0xD800 && ucc <= 0xDFFF) { return -1; }
+  // UTF-8 must represent code points in their shortest possible encoding.
+  switch (len) {
+    case 2:
+      // Two bytes of UTF-8 can represent code points from U+0080 to U+07FF.
+      if (ucc < 0x0080 || ucc > 0x07FF) { return -1; }
+      break;
+    case 3:
+      // Three bytes of UTF-8 can represent code points from U+0800 to U+FFFF.
+      if (ucc < 0x0800 || ucc > 0xFFFF) { return -1; }
+      break;
+    case 4:
+      // Four bytes of UTF-8 can represent code points from U+10000 to U+10FFFF.
+      if (ucc < 0x10000 || ucc > 0x10FFFF) { return -1; }
+      break;
+  }
+  return ucc;
+}
+
+#ifndef FLATBUFFERS_PREFER_PRINTF
+// Wraps a string to a maximum length, inserting new lines where necessary. Any
+// existing whitespace will be collapsed down to a single space. A prefix or
+// suffix can be provided, which will be inserted before or after a wrapped
+// line, respectively.
+inline std::string WordWrap(const std::string in, size_t max_length,
+                            const std::string wrapped_line_prefix,
+                            const std::string wrapped_line_suffix) {
+  std::istringstream in_stream(in);
+  std::string wrapped, line, word;
+
+  in_stream >> word;
+  line = word;
+
+  while (in_stream >> word) {
+    if ((line.length() + 1 + word.length() + wrapped_line_suffix.length()) <
+        max_length) {
+      line += " " + word;
+    } else {
+      wrapped += line + wrapped_line_suffix + "\n";
+      line = wrapped_line_prefix + word;
+    }
+  }
+  wrapped += line;
+
+  return wrapped;
+}
+#endif  // !FLATBUFFERS_PREFER_PRINTF
+
+inline bool EscapeString(const char *s, size_t length, std::string *_text,
+                         bool allow_non_utf8, bool natural_utf8) {
+  std::string &text = *_text;
+  text += "\"";
+  for (uoffset_t i = 0; i < length; i++) {
+    char c = s[i];
+    switch (c) {
+      case '\n': text += "\\n"; break;
+      case '\t': text += "\\t"; break;
+      case '\r': text += "\\r"; break;
+      case '\b': text += "\\b"; break;
+      case '\f': text += "\\f"; break;
+      case '\"': text += "\\\""; break;
+      case '\\': text += "\\\\"; break;
+      default:
+        if (c >= ' ' && c <= '~') {
+          text += c;
+        } else {
+          // Not printable ASCII data. Let's see if it's valid UTF-8 first:
+          const char *utf8 = s + i;
+          int ucc = FromUTF8(&utf8);
+          if (ucc < 0) {
+            if (allow_non_utf8) {
+              text += "\\x";
+              text += IntToStringHex(static_cast<uint8_t>(c), 2);
+            } else {
+              // There are two cases here:
+              //
+              // 1) We reached here by parsing an IDL file. In that case,
+              // we previously checked for non-UTF-8, so we shouldn't reach
+              // here.
+              //
+              // 2) We reached here by someone calling GenerateText()
+              // on a previously-serialized flatbuffer. The data might have
+              // non-UTF-8 Strings, or might be corrupt.
+              //
+              // In both cases, we have to give up and inform the caller
+              // they have no JSON.
+              return false;
+            }
+          } else {
+            if (natural_utf8) {
+              // utf8 points to past all utf-8 bytes parsed
+              text.append(s + i, static_cast<size_t>(utf8 - s - i));
+            } else if (ucc <= 0xFFFF) {
+              // Parses as Unicode within JSON's \uXXXX range, so use that.
+              text += "\\u";
+              text += IntToStringHex(ucc, 4);
+            } else if (ucc <= 0x10FFFF) {
+              // Encode Unicode SMP values to a surrogate pair using two \u
+              // escapes.
+              uint32_t base = ucc - 0x10000;
+              auto high_surrogate = (base >> 10) + 0xD800;
+              auto low_surrogate = (base & 0x03FF) + 0xDC00;
+              text += "\\u";
+              text += IntToStringHex(high_surrogate, 4);
+              text += "\\u";
+              text += IntToStringHex(low_surrogate, 4);
+            }
+            // Skip past characters recognized.
+            i = static_cast<uoffset_t>(utf8 - s - 1);
+          }
+        }
+        break;
+    }
+  }
+  text += "\"";
+  return true;
+}
+
+inline std::string BufferToHexText(const void *buffer, size_t buffer_size,
+                                   size_t max_length,
+                                   const std::string &wrapped_line_prefix,
+                                   const std::string &wrapped_line_suffix) {
+  std::string text = wrapped_line_prefix;
+  size_t start_offset = 0;
+  const char *s = reinterpret_cast<const char *>(buffer);
+  for (size_t i = 0; s && i < buffer_size; i++) {
+    // Last iteration or do we have more?
+    bool have_more = i + 1 < buffer_size;
+    text += "0x";
+    text += IntToStringHex(static_cast<uint8_t>(s[i]), 2);
+    if (have_more) { text += ','; }
+    // If we have more to process and we reached max_length
+    if (have_more &&
+        text.size() + wrapped_line_suffix.size() >= start_offset + max_length) {
+      text += wrapped_line_suffix;
+      text += '\n';
+      start_offset = text.size();
+      text += wrapped_line_prefix;
+    }
+  }
+  text += wrapped_line_suffix;
+  return text;
+}
+
+// Remove paired quotes in a string: "text"|'text' -> text.
+std::string RemoveStringQuotes(const std::string &s);
+
+// Change th global C-locale to locale with name <locale_name>.
+// Returns an actual locale name in <_value>, useful if locale_name is "" or
+// null.
+bool SetGlobalTestLocale(const char *locale_name,
+                         std::string *_value = nullptr);
+
+// Read (or test) a value of environment variable.
+bool ReadEnvironmentVariable(const char *var_name,
+                             std::string *_value = nullptr);
+
+// MSVC specific: Send all assert reports to STDOUT to prevent CI hangs.
+void SetupDefaultCRTReportMode();
+
+enum class Case {
+  kUnknown = 0,
+  // TheQuickBrownFox
+  kUpperCamel = 1,
+  // theQuickBrownFox
+  kLowerCamel = 2,
+  // the_quick_brown_fox
+  kSnake = 3,
+  // THE_QUICK_BROWN_FOX
+  kScreamingSnake = 4,
+  // THEQUICKBROWNFOX
+  kAllUpper = 5,
+  // thequickbrownfox
+  kAllLower = 6,
+  // the-quick-brown-fox
+  kDasher = 7,
+  // THEQuiCKBr_ownFox (or whatever you want, we won't change it)
+  kKeep = 8,
+  // the_quick_brown_fox123 (as opposed to the_quick_brown_fox_123)
+  kSnake2 = 9,
+};
+
+// Convert the `input` string of case `input_case` to the specified `output_case`.
+std::string ConvertCase(const std::string &input, Case output_case,
+                    Case input_case = Case::kSnake);
+
+}  // namespace flatbuffers
+
+#endif  // FLATBUFFERS_UTIL_H_
diff --git a/third_party/tflite-micro/third_party/flatbuffers/include/flatbuffers/util.h.rej b/third_party/tflite-micro/third_party/flatbuffers/include/flatbuffers/util.h.rej
new file mode 100644
index 00000000..562398a4
--- /dev/null
+++ b/third_party/tflite-micro/third_party/flatbuffers/include/flatbuffers/util.h.rej
@@ -0,0 +1,693 @@
+--- third_party/flatbuffers/include/flatbuffers/util.h	2023-04-24 21:08:39.066741057 +0800
++++ third_party/flatbuffers/include/flatbuffers/util.h	1970-01-01 08:00:00.000000000 +0800
+@@ -1,690 +0,0 @@
+-/*
+- * Copyright 2014 Google Inc. All rights reserved.
+- *
+- * Licensed under the Apache License, Version 2.0 (the "License");
+- * you may not use this file except in compliance with the License.
+- * You may obtain a copy of the License at
+- *
+- *     http://www.apache.org/licenses/LICENSE-2.0
+- *
+- * Unless required by applicable law or agreed to in writing, software
+- * distributed under the License is distributed on an "AS IS" BASIS,
+- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+- * See the License for the specific language governing permissions and
+- * limitations under the License.
+- */
+-
+-#ifndef FLATBUFFERS_UTIL_H_
+-#define FLATBUFFERS_UTIL_H_
+-
+-#include <ctype.h>
+-#include <errno.h>
+-
+-#include "flatbuffers/base.h"
+-#include "flatbuffers/stl_emulation.h"
+-
+-#ifndef FLATBUFFERS_PREFER_PRINTF
+-#  include <iomanip>
+-#  include <sstream>
+-#else  // FLATBUFFERS_PREFER_PRINTF
+-#  include <float.h>
+-#  include <stdio.h>
+-#endif  // FLATBUFFERS_PREFER_PRINTF
+-
+-#include <string>
+-
+-namespace flatbuffers {
+-
+-// @locale-independent functions for ASCII characters set.
+-
+-// Fast checking that character lies in closed range: [a <= x <= b]
+-// using one compare (conditional branch) operator.
+-inline bool check_ascii_range(char x, char a, char b) {
+-  FLATBUFFERS_ASSERT(a <= b);
+-  // (Hacker's Delight): `a <= x <= b` <=> `(x-a) <={u} (b-a)`.
+-  // The x, a, b will be promoted to int and subtracted without overflow.
+-  return static_cast<unsigned int>(x - a) <= static_cast<unsigned int>(b - a);
+-}
+-
+-// Case-insensitive isalpha
+-inline bool is_alpha(char c) {
+-  // ASCII only: alpha to upper case => reset bit 0x20 (~0x20 = 0xDF).
+-  return check_ascii_range(c & 0xDF, 'a' & 0xDF, 'z' & 0xDF);
+-}
+-
+-// Check for uppercase alpha
+-inline bool is_alpha_upper(char c) { return check_ascii_range(c, 'A', 'Z'); }
+-
+-// Check (case-insensitive) that `c` is equal to alpha.
+-inline bool is_alpha_char(char c, char alpha) {
+-  FLATBUFFERS_ASSERT(is_alpha(alpha));
+-  // ASCII only: alpha to upper case => reset bit 0x20 (~0x20 = 0xDF).
+-  return ((c & 0xDF) == (alpha & 0xDF));
+-}
+-
+-// https://en.cppreference.com/w/cpp/string/byte/isxdigit
+-// isdigit and isxdigit are the only standard narrow character classification
+-// functions that are not affected by the currently installed C locale. although
+-// some implementations (e.g. Microsoft in 1252 codepage) may classify
+-// additional single-byte characters as digits.
+-inline bool is_digit(char c) { return check_ascii_range(c, '0', '9'); }
+-
+-inline bool is_xdigit(char c) {
+-  // Replace by look-up table.
+-  return is_digit(c) || check_ascii_range(c & 0xDF, 'a' & 0xDF, 'f' & 0xDF);
+-}
+-
+-// Case-insensitive isalnum
+-inline bool is_alnum(char c) { return is_alpha(c) || is_digit(c); }
+-
+-inline char CharToUpper(char c) {
+-  return static_cast<char>(::toupper(static_cast<unsigned char>(c)));
+-}
+-
+-inline char CharToLower(char c) {
+-  return static_cast<char>(::tolower(static_cast<unsigned char>(c)));
+-}
+-
+-// @end-locale-independent functions for ASCII character set
+-
+-#ifdef FLATBUFFERS_PREFER_PRINTF
+-template<typename T> size_t IntToDigitCount(T t) {
+-  size_t digit_count = 0;
+-  // Count the sign for negative numbers
+-  if (t < 0) digit_count++;
+-  // Count a single 0 left of the dot for fractional numbers
+-  if (-1 < t && t < 1) digit_count++;
+-  // Count digits until fractional part
+-  T eps = std::numeric_limits<T>::epsilon();
+-  while (t <= (-1 + eps) || (1 - eps) <= t) {
+-    t /= 10;
+-    digit_count++;
+-  }
+-  return digit_count;
+-}
+-
+-template<typename T> size_t NumToStringWidth(T t, int precision = 0) {
+-  size_t string_width = IntToDigitCount(t);
+-  // Count the dot for floating point numbers
+-  if (precision) string_width += (precision + 1);
+-  return string_width;
+-}
+-
+-template<typename T>
+-std::string NumToStringImplWrapper(T t, const char *fmt, int precision = 0) {
+-  size_t string_width = NumToStringWidth(t, precision);
+-  std::string s(string_width, 0x00);
+-  // Allow snprintf to use std::string trailing null to detect buffer overflow
+-  snprintf(const_cast<char *>(s.data()), (s.size() + 1), fmt, string_width, t);
+-  return s;
+-}
+-#endif  // FLATBUFFERS_PREFER_PRINTF
+-
+-// Convert an integer or floating point value to a string.
+-// In contrast to std::stringstream, "char" values are
+-// converted to a string of digits, and we don't use scientific notation.
+-template<typename T> std::string NumToString(T t) {
+-  // clang-format off
+-
+-  #ifndef FLATBUFFERS_PREFER_PRINTF
+-    std::stringstream ss;
+-    ss << t;
+-    return ss.str();
+-  #else // FLATBUFFERS_PREFER_PRINTF
+-    auto v = static_cast<long long>(t);
+-    return NumToStringImplWrapper(v, "%.*lld");
+-  #endif // FLATBUFFERS_PREFER_PRINTF
+-  // clang-format on
+-}
+-// Avoid char types used as character data.
+-template<> inline std::string NumToString<signed char>(signed char t) {
+-  return NumToString(static_cast<int>(t));
+-}
+-template<> inline std::string NumToString<unsigned char>(unsigned char t) {
+-  return NumToString(static_cast<int>(t));
+-}
+-template<> inline std::string NumToString<char>(char t) {
+-  return NumToString(static_cast<int>(t));
+-}
+-
+-// Special versions for floats/doubles.
+-template<typename T> std::string FloatToString(T t, int precision) {
+-  // clang-format off
+-
+-  #ifndef FLATBUFFERS_PREFER_PRINTF
+-    // to_string() prints different numbers of digits for floats depending on
+-    // platform and isn't available on Android, so we use stringstream
+-    std::stringstream ss;
+-    // Use std::fixed to suppress scientific notation.
+-    ss << std::fixed;
+-    // Default precision is 6, we want that to be higher for doubles.
+-    ss << std::setprecision(precision);
+-    ss << t;
+-    auto s = ss.str();
+-  #else // FLATBUFFERS_PREFER_PRINTF
+-    auto v = static_cast<double>(t);
+-    auto s = NumToStringImplWrapper(v, "%0.*f", precision);
+-  #endif // FLATBUFFERS_PREFER_PRINTF
+-  // clang-format on
+-  // Sadly, std::fixed turns "1" into "1.00000", so here we undo that.
+-  auto p = s.find_last_not_of('0');
+-  if (p != std::string::npos) {
+-    // Strip trailing zeroes. If it is a whole number, keep one zero.
+-    s.resize(p + (s[p] == '.' ? 2 : 1));
+-  }
+-  return s;
+-}
+-
+-template<> inline std::string NumToString<double>(double t) {
+-  return FloatToString(t, 12);
+-}
+-template<> inline std::string NumToString<float>(float t) {
+-  return FloatToString(t, 6);
+-}
+-
+-// Convert an integer value to a hexadecimal string.
+-// The returned string length is always xdigits long, prefixed by 0 digits.
+-// For example, IntToStringHex(0x23, 8) returns the string "00000023".
+-inline std::string IntToStringHex(int i, int xdigits) {
+-  FLATBUFFERS_ASSERT(i >= 0);
+-  // clang-format off
+-
+-  #ifndef FLATBUFFERS_PREFER_PRINTF
+-    std::stringstream ss;
+-    ss << std::setw(xdigits) << std::setfill('0') << std::hex << std::uppercase
+-       << i;
+-    return ss.str();
+-  #else // FLATBUFFERS_PREFER_PRINTF
+-    return NumToStringImplWrapper(i, "%.*X", xdigits);
+-  #endif // FLATBUFFERS_PREFER_PRINTF
+-  // clang-format on
+-}
+-
+-// clang-format off
+-// Use locale independent functions {strtod_l, strtof_l, strtoll_l, strtoull_l}.
+-#if defined(FLATBUFFERS_LOCALE_INDEPENDENT) && (FLATBUFFERS_LOCALE_INDEPENDENT > 0)
+-  class ClassicLocale {
+-    #ifdef _MSC_VER
+-      typedef _locale_t locale_type;
+-    #else
+-      typedef locale_t locale_type;  // POSIX.1-2008 locale_t type
+-    #endif
+-    ClassicLocale();
+-    ~ClassicLocale();
+-    locale_type locale_;
+-    static ClassicLocale instance_;
+-  public:
+-    static locale_type Get() { return instance_.locale_; }
+-  };
+-
+-  #ifdef _MSC_VER
+-    #define __strtoull_impl(s, pe, b) _strtoui64_l(s, pe, b, ClassicLocale::Get())
+-    #define __strtoll_impl(s, pe, b) _strtoi64_l(s, pe, b, ClassicLocale::Get())
+-    #define __strtod_impl(s, pe) _strtod_l(s, pe, ClassicLocale::Get())
+-    #define __strtof_impl(s, pe) _strtof_l(s, pe, ClassicLocale::Get())
+-  #else
+-    #define __strtoull_impl(s, pe, b) strtoull_l(s, pe, b, ClassicLocale::Get())
+-    #define __strtoll_impl(s, pe, b) strtoll_l(s, pe, b, ClassicLocale::Get())
+-    #define __strtod_impl(s, pe) strtod_l(s, pe, ClassicLocale::Get())
+-    #define __strtof_impl(s, pe) strtof_l(s, pe, ClassicLocale::Get())
+-  #endif
+-#else
+-  #define __strtod_impl(s, pe) strtod(s, pe)
+-  #define __strtof_impl(s, pe) static_cast<float>(strtod(s, pe))
+-  #ifdef _MSC_VER
+-    #define __strtoull_impl(s, pe, b) _strtoui64(s, pe, b)
+-    #define __strtoll_impl(s, pe, b) _strtoi64(s, pe, b)
+-  #else
+-    #define __strtoull_impl(s, pe, b) strtoull(s, pe, b)
+-    #define __strtoll_impl(s, pe, b) strtoll(s, pe, b)
+-  #endif
+-#endif
+-
+-inline void strtoval_impl(int64_t *val, const char *str, char **endptr,
+-                                 int base) {
+-    *val = __strtoll_impl(str, endptr, base);
+-}
+-
+-inline void strtoval_impl(uint64_t *val, const char *str, char **endptr,
+-                                 int base) {
+-  *val = __strtoull_impl(str, endptr, base);
+-}
+-
+-inline void strtoval_impl(double *val, const char *str, char **endptr) {
+-  *val = __strtod_impl(str, endptr);
+-}
+-
+-// UBSAN: double to float is safe if numeric_limits<float>::is_iec559 is true.
+-__supress_ubsan__("float-cast-overflow")
+-inline void strtoval_impl(float *val, const char *str, char **endptr) {
+-  *val = __strtof_impl(str, endptr);
+-}
+-#undef __strtoull_impl
+-#undef __strtoll_impl
+-#undef __strtod_impl
+-#undef __strtof_impl
+-// clang-format on
+-
+-// Adaptor for strtoull()/strtoll().
+-// Flatbuffers accepts numbers with any count of leading zeros (-009 is -9),
+-// while strtoll with base=0 interprets first leading zero as octal prefix.
+-// In future, it is possible to add prefixed 0b0101.
+-// 1) Checks errno code for overflow condition (out of range).
+-// 2) If base <= 0, function try to detect base of number by prefix.
+-//
+-// Return value (like strtoull and strtoll, but reject partial result):
+-// - If successful, an integer value corresponding to the str is returned.
+-// - If full string conversion can't be performed, 0 is returned.
+-// - If the converted value falls out of range of corresponding return type, a
+-// range error occurs. In this case value MAX(T)/MIN(T) is returned.
+-template<typename T>
+-inline bool StringToIntegerImpl(T *val, const char *const str,
+-                                const int base = 0,
+-                                const bool check_errno = true) {
+-  // T is int64_t or uint64_T
+-  FLATBUFFERS_ASSERT(str);
+-  if (base <= 0) {
+-    auto s = str;
+-    while (*s && !is_digit(*s)) s++;
+-    if (s[0] == '0' && is_alpha_char(s[1], 'X'))
+-      return StringToIntegerImpl(val, str, 16, check_errno);
+-    // if a prefix not match, try base=10
+-    return StringToIntegerImpl(val, str, 10, check_errno);
+-  } else {
+-    if (check_errno) errno = 0;  // clear thread-local errno
+-    auto endptr = str;
+-    strtoval_impl(val, str, const_cast<char **>(&endptr), base);
+-    if ((*endptr != '\0') || (endptr == str)) {
+-      *val = 0;      // erase partial result
+-      return false;  // invalid string
+-    }
+-    // errno is out-of-range, return MAX/MIN
+-    if (check_errno && errno) return false;
+-    return true;
+-  }
+-}
+-
+-template<typename T>
+-inline bool StringToFloatImpl(T *val, const char *const str) {
+-  // Type T must be either float or double.
+-  FLATBUFFERS_ASSERT(str && val);
+-  auto end = str;
+-  strtoval_impl(val, str, const_cast<char **>(&end));
+-  auto done = (end != str) && (*end == '\0');
+-  if (!done) *val = 0;  // erase partial result
+-  return done;
+-}
+-
+-// Convert a string to an instance of T.
+-// Return value (matched with StringToInteger64Impl and strtod):
+-// - If successful, a numeric value corresponding to the str is returned.
+-// - If full string conversion can't be performed, 0 is returned.
+-// - If the converted value falls out of range of corresponding return type, a
+-// range error occurs. In this case value MAX(T)/MIN(T) is returned.
+-template<typename T> inline bool StringToNumber(const char *s, T *val) {
+-  // Assert on `unsigned long` and `signed long` on LP64.
+-  // If it is necessary, it could be solved with flatbuffers::enable_if<B,T>.
+-  static_assert(sizeof(T) < sizeof(int64_t), "unexpected type T");
+-  FLATBUFFERS_ASSERT(s && val);
+-  int64_t i64;
+-  // The errno check isn't needed, will return MAX/MIN on overflow.
+-  if (StringToIntegerImpl(&i64, s, 0, false)) {
+-    const int64_t max = (flatbuffers::numeric_limits<T>::max)();
+-    const int64_t min = flatbuffers::numeric_limits<T>::lowest();
+-    if (i64 > max) {
+-      *val = static_cast<T>(max);
+-      return false;
+-    }
+-    if (i64 < min) {
+-      // For unsigned types return max to distinguish from
+-      // "no conversion can be performed" when 0 is returned.
+-      *val = static_cast<T>(flatbuffers::is_unsigned<T>::value ? max : min);
+-      return false;
+-    }
+-    *val = static_cast<T>(i64);
+-    return true;
+-  }
+-  *val = 0;
+-  return false;
+-}
+-
+-template<> inline bool StringToNumber<int64_t>(const char *str, int64_t *val) {
+-  return StringToIntegerImpl(val, str);
+-}
+-
+-template<>
+-inline bool StringToNumber<uint64_t>(const char *str, uint64_t *val) {
+-  if (!StringToIntegerImpl(val, str)) return false;
+-  // The strtoull accepts negative numbers:
+-  // If the minus sign was part of the input sequence, the numeric value
+-  // calculated from the sequence of digits is negated as if by unary minus
+-  // in the result type, which applies unsigned integer wraparound rules.
+-  // Fix this behaviour (except -0).
+-  if (*val) {
+-    auto s = str;
+-    while (*s && !is_digit(*s)) s++;
+-    s = (s > str) ? (s - 1) : s;  // step back to one symbol
+-    if (*s == '-') {
+-      // For unsigned types return the max to distinguish from
+-      // "no conversion can be performed".
+-      *val = (flatbuffers::numeric_limits<uint64_t>::max)();
+-      return false;
+-    }
+-  }
+-  return true;
+-}
+-
+-template<> inline bool StringToNumber(const char *s, float *val) {
+-  return StringToFloatImpl(val, s);
+-}
+-
+-template<> inline bool StringToNumber(const char *s, double *val) {
+-  return StringToFloatImpl(val, s);
+-}
+-
+-inline int64_t StringToInt(const char *s, int base = 10) {
+-  int64_t val;
+-  return StringToIntegerImpl(&val, s, base) ? val : 0;
+-}
+-
+-inline uint64_t StringToUInt(const char *s, int base = 10) {
+-  uint64_t val;
+-  return StringToIntegerImpl(&val, s, base) ? val : 0;
+-}
+-
+-typedef bool (*LoadFileFunction)(const char *filename, bool binary,
+-                                 std::string *dest);
+-typedef bool (*FileExistsFunction)(const char *filename);
+-
+-LoadFileFunction SetLoadFileFunction(LoadFileFunction load_file_function);
+-
+-FileExistsFunction SetFileExistsFunction(
+-    FileExistsFunction file_exists_function);
+-
+-// Check if file "name" exists.
+-bool FileExists(const char *name);
+-
+-// Check if "name" exists and it is also a directory.
+-bool DirExists(const char *name);
+-
+-// Load file "name" into "buf" returning true if successful
+-// false otherwise.  If "binary" is false data is read
+-// using ifstream's text mode, otherwise data is read with
+-// no transcoding.
+-bool LoadFile(const char *name, bool binary, std::string *buf);
+-
+-// Save data "buf" of length "len" bytes into a file
+-// "name" returning true if successful, false otherwise.
+-// If "binary" is false data is written using ifstream's
+-// text mode, otherwise data is written with no
+-// transcoding.
+-bool SaveFile(const char *name, const char *buf, size_t len, bool binary);
+-
+-// Save data "buf" into file "name" returning true if
+-// successful, false otherwise.  If "binary" is false
+-// data is written using ifstream's text mode, otherwise
+-// data is written with no transcoding.
+-inline bool SaveFile(const char *name, const std::string &buf, bool binary) {
+-  return SaveFile(name, buf.c_str(), buf.size(), binary);
+-}
+-
+-// Functionality for minimalistic portable path handling.
+-
+-// The functions below behave correctly regardless of whether posix ('/') or
+-// Windows ('/' or '\\') separators are used.
+-
+-// Any new separators inserted are always posix.
+-FLATBUFFERS_CONSTEXPR char kPathSeparator = '/';
+-
+-// Returns the path with the extension, if any, removed.
+-std::string StripExtension(const std::string &filepath);
+-
+-// Returns the extension, if any.
+-std::string GetExtension(const std::string &filepath);
+-
+-// Return the last component of the path, after the last separator.
+-std::string StripPath(const std::string &filepath);
+-
+-// Strip the last component of the path + separator.
+-std::string StripFileName(const std::string &filepath);
+-
+-// Concatenates a path with a filename, regardless of whether the path
+-// ends in a separator or not.
+-std::string ConCatPathFileName(const std::string &path,
+-                               const std::string &filename);
+-
+-// Replaces any '\\' separators with '/'
+-std::string PosixPath(const char *path);
+-std::string PosixPath(const std::string &path);
+-
+-// This function ensure a directory exists, by recursively
+-// creating dirs for any parts of the path that don't exist yet.
+-void EnsureDirExists(const std::string &filepath);
+-
+-// Obtains the absolute path from any other path.
+-// Returns the input path if the absolute path couldn't be resolved.
+-std::string AbsolutePath(const std::string &filepath);
+-
+-// Returns files relative to the --project_root path, prefixed with `//`.
+-std::string RelativeToRootPath(const std::string &project,
+-                               const std::string &filepath);
+-
+-// To and from UTF-8 unicode conversion functions
+-
+-// Convert a unicode code point into a UTF-8 representation by appending it
+-// to a string. Returns the number of bytes generated.
+-inline int ToUTF8(uint32_t ucc, std::string *out) {
+-  FLATBUFFERS_ASSERT(!(ucc & 0x80000000));  // Top bit can't be set.
+-  // 6 possible encodings: http://en.wikipedia.org/wiki/UTF-8
+-  for (int i = 0; i < 6; i++) {
+-    // Max bits this encoding can represent.
+-    uint32_t max_bits = 6 + i * 5 + static_cast<int>(!i);
+-    if (ucc < (1u << max_bits)) {  // does it fit?
+-      // Remaining bits not encoded in the first byte, store 6 bits each
+-      uint32_t remain_bits = i * 6;
+-      // Store first byte:
+-      (*out) += static_cast<char>((0xFE << (max_bits - remain_bits)) |
+-                                  (ucc >> remain_bits));
+-      // Store remaining bytes:
+-      for (int j = i - 1; j >= 0; j--) {
+-        (*out) += static_cast<char>(((ucc >> (j * 6)) & 0x3F) | 0x80);
+-      }
+-      return i + 1;  // Return the number of bytes added.
+-    }
+-  }
+-  FLATBUFFERS_ASSERT(0);  // Impossible to arrive here.
+-  return -1;
+-}
+-
+-// Converts whatever prefix of the incoming string corresponds to a valid
+-// UTF-8 sequence into a unicode code. The incoming pointer will have been
+-// advanced past all bytes parsed.
+-// returns -1 upon corrupt UTF-8 encoding (ignore the incoming pointer in
+-// this case).
+-inline int FromUTF8(const char **in) {
+-  int len = 0;
+-  // Count leading 1 bits.
+-  for (int mask = 0x80; mask >= 0x04; mask >>= 1) {
+-    if (**in & mask) {
+-      len++;
+-    } else {
+-      break;
+-    }
+-  }
+-  if ((static_cast<unsigned char>(**in) << len) & 0x80)
+-    return -1;  // Bit after leading 1's must be 0.
+-  if (!len) return *(*in)++;
+-  // UTF-8 encoded values with a length are between 2 and 4 bytes.
+-  if (len < 2 || len > 4) { return -1; }
+-  // Grab initial bits of the code.
+-  int ucc = *(*in)++ & ((1 << (7 - len)) - 1);
+-  for (int i = 0; i < len - 1; i++) {
+-    if ((**in & 0xC0) != 0x80) return -1;  // Upper bits must 1 0.
+-    ucc <<= 6;
+-    ucc |= *(*in)++ & 0x3F;  // Grab 6 more bits of the code.
+-  }
+-  // UTF-8 cannot encode values between 0xD800 and 0xDFFF (reserved for
+-  // UTF-16 surrogate pairs).
+-  if (ucc >= 0xD800 && ucc <= 0xDFFF) { return -1; }
+-  // UTF-8 must represent code points in their shortest possible encoding.
+-  switch (len) {
+-    case 2:
+-      // Two bytes of UTF-8 can represent code points from U+0080 to U+07FF.
+-      if (ucc < 0x0080 || ucc > 0x07FF) { return -1; }
+-      break;
+-    case 3:
+-      // Three bytes of UTF-8 can represent code points from U+0800 to U+FFFF.
+-      if (ucc < 0x0800 || ucc > 0xFFFF) { return -1; }
+-      break;
+-    case 4:
+-      // Four bytes of UTF-8 can represent code points from U+10000 to U+10FFFF.
+-      if (ucc < 0x10000 || ucc > 0x10FFFF) { return -1; }
+-      break;
+-  }
+-  return ucc;
+-}
+-
+-#ifndef FLATBUFFERS_PREFER_PRINTF
+-// Wraps a string to a maximum length, inserting new lines where necessary. Any
+-// existing whitespace will be collapsed down to a single space. A prefix or
+-// suffix can be provided, which will be inserted before or after a wrapped
+-// line, respectively.
+-inline std::string WordWrap(const std::string in, size_t max_length,
+-                            const std::string wrapped_line_prefix,
+-                            const std::string wrapped_line_suffix) {
+-  std::istringstream in_stream(in);
+-  std::string wrapped, line, word;
+-
+-  in_stream >> word;
+-  line = word;
+-
+-  while (in_stream >> word) {
+-    if ((line.length() + 1 + word.length() + wrapped_line_suffix.length()) <
+-        max_length) {
+-      line += " " + word;
+-    } else {
+-      wrapped += line + wrapped_line_suffix + "\n";
+-      line = wrapped_line_prefix + word;
+-    }
+-  }
+-  wrapped += line;
+-
+-  return wrapped;
+-}
+-#endif  // !FLATBUFFERS_PREFER_PRINTF
+-
+-inline bool EscapeString(const char *s, size_t length, std::string *_text,
+-                         bool allow_non_utf8, bool natural_utf8) {
+-  std::string &text = *_text;
+-  text += "\"";
+-  for (uoffset_t i = 0; i < length; i++) {
+-    char c = s[i];
+-    switch (c) {
+-      case '\n': text += "\\n"; break;
+-      case '\t': text += "\\t"; break;
+-      case '\r': text += "\\r"; break;
+-      case '\b': text += "\\b"; break;
+-      case '\f': text += "\\f"; break;
+-      case '\"': text += "\\\""; break;
+-      case '\\': text += "\\\\"; break;
+-      default:
+-        if (c >= ' ' && c <= '~') {
+-          text += c;
+-        } else {
+-          // Not printable ASCII data. Let's see if it's valid UTF-8 first:
+-          const char *utf8 = s + i;
+-          int ucc = FromUTF8(&utf8);
+-          if (ucc < 0) {
+-            if (allow_non_utf8) {
+-              text += "\\x";
+-              text += IntToStringHex(static_cast<uint8_t>(c), 2);
+-            } else {
+-              // There are two cases here:
+-              //
+-              // 1) We reached here by parsing an IDL file. In that case,
+-              // we previously checked for non-UTF-8, so we shouldn't reach
+-              // here.
+-              //
+-              // 2) We reached here by someone calling GenerateText()
+-              // on a previously-serialized flatbuffer. The data might have
+-              // non-UTF-8 Strings, or might be corrupt.
+-              //
+-              // In both cases, we have to give up and inform the caller
+-              // they have no JSON.
+-              return false;
+-            }
+-          } else {
+-            if (natural_utf8) {
+-              // utf8 points to past all utf-8 bytes parsed
+-              text.append(s + i, static_cast<size_t>(utf8 - s - i));
+-            } else if (ucc <= 0xFFFF) {
+-              // Parses as Unicode within JSON's \uXXXX range, so use that.
+-              text += "\\u";
+-              text += IntToStringHex(ucc, 4);
+-            } else if (ucc <= 0x10FFFF) {
+-              // Encode Unicode SMP values to a surrogate pair using two \u
+-              // escapes.
+-              uint32_t base = ucc - 0x10000;
+-              auto high_surrogate = (base >> 10) + 0xD800;
+-              auto low_surrogate = (base & 0x03FF) + 0xDC00;
+-              text += "\\u";
+-              text += IntToStringHex(high_surrogate, 4);
+-              text += "\\u";
+-              text += IntToStringHex(low_surrogate, 4);
+-            }
+-            // Skip past characters recognized.
+-            i = static_cast<uoffset_t>(utf8 - s - 1);
+-          }
+-        }
+-        break;
+-    }
+-  }
+-  text += "\"";
+-  return true;
+-}
+-
+-inline std::string BufferToHexText(const void *buffer, size_t buffer_size,
+-                                   size_t max_length,
+-                                   const std::string &wrapped_line_prefix,
+-                                   const std::string &wrapped_line_suffix) {
+-  std::string text = wrapped_line_prefix;
+-  size_t start_offset = 0;
+-  const char *s = reinterpret_cast<const char *>(buffer);
+-  for (size_t i = 0; s && i < buffer_size; i++) {
+-    // Last iteration or do we have more?
+-    bool have_more = i + 1 < buffer_size;
+-    text += "0x";
+-    text += IntToStringHex(static_cast<uint8_t>(s[i]), 2);
+-    if (have_more) { text += ','; }
+-    // If we have more to process and we reached max_length
+-    if (have_more &&
+-        text.size() + wrapped_line_suffix.size() >= start_offset + max_length) {
+-      text += wrapped_line_suffix;
+-      text += '\n';
+-      start_offset = text.size();
+-      text += wrapped_line_prefix;
+-    }
+-  }
+-  text += wrapped_line_suffix;
+-  return text;
+-}
+-
+-// Remove paired quotes in a string: "text"|'text' -> text.
+-std::string RemoveStringQuotes(const std::string &s);
+-
+-// Change th global C-locale to locale with name <locale_name>.
+-// Returns an actual locale name in <_value>, useful if locale_name is "" or
+-// null.
+-bool SetGlobalTestLocale(const char *locale_name,
+-                         std::string *_value = nullptr);
+-
+-// Read (or test) a value of environment variable.
+-bool ReadEnvironmentVariable(const char *var_name,
+-                             std::string *_value = nullptr);
+-
+-// MSVC specific: Send all assert reports to STDOUT to prevent CI hangs.
+-void SetupDefaultCRTReportMode();
+-
+-}  // namespace flatbuffers
+-
+-#endif  // FLATBUFFERS_UTIL_H_
